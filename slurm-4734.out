ai-gpgpu14
Visible GPUs (physical): 0,1,2,3,4,5,6,7
Using devices (logical): cuda:3 cuda:4 cuda:5
Backend: cuda
Session log: output/imputation/cuda/session_timellm_20250823T101457.log
🔥 RUN: timellm | physionet_2012 | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=35)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 10:15:36 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 10:15:36 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-23 10:15:36 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-23 10:15:36 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-23 10:15:36 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-23 10:15:42 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-23 10:15:42 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-23 10:15:42 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-23 10:15:42 [INFO]: Total sample number: 3997
2025-08-23 10:15:42 [INFO]: Training set size: 2557 (63.97%)
2025-08-23 10:15:42 [INFO]: Validation set size: 640 (16.01%)
2025-08-23 10:15:42 [INFO]: Test set size: 800 (20.02%)
2025-08-23 10:15:42 [INFO]: Number of steps: 48
2025-08-23 10:15:42 [INFO]: Number of features: 37
2025-08-23 10:15:42 [INFO]: Train set missing rate: 79.68%
2025-08-23 10:15:42 [INFO]: Validating set missing rate: 81.65%
2025-08-23 10:15:42 [INFO]: Test set missing rate: 81.96%
✅ Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-23 10:15:42 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 10:15:42 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 10:15:42 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 10:15:42 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 10:15:42 [INFO]: Using customized MAE as the training loss function.
2025-08-23 10:15:42 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 10:15:44 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 10:15:44 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 10:21:51 [INFO]: Epoch 001 - training loss (MAE): 1.7846, validation MSE: 1.9032
2025-08-23 10:28:03 [INFO]: Epoch 002 - training loss (MAE): 1.6723, validation MSE: 1.7839
2025-08-23 10:34:34 [INFO]: Epoch 003 - training loss (MAE): 1.6177, validation MSE: 1.7846
2025-08-23 10:41:08 [INFO]: Epoch 004 - training loss (MAE): 1.5976, validation MSE: 1.7905
2025-08-23 10:47:50 [INFO]: Epoch 005 - training loss (MAE): 1.5984, validation MSE: 1.7635
2025-08-23 10:54:09 [INFO]: Epoch 006 - training loss (MAE): 1.5921, validation MSE: 1.7503
2025-08-23 11:00:17 [INFO]: Epoch 007 - training loss (MAE): 1.5926, validation MSE: 1.7688
2025-08-23 11:06:13 [INFO]: Epoch 008 - training loss (MAE): 1.5971, validation MSE: 1.7503
2025-08-23 11:11:47 [INFO]: Epoch 009 - training loss (MAE): 1.6034, validation MSE: 1.7621
2025-08-23 11:17:38 [INFO]: Epoch 010 - training loss (MAE): 1.6339, validation MSE: 1.8210
2025-08-23 11:23:51 [INFO]: Epoch 011 - training loss (MAE): 1.6299, validation MSE: 1.7779
2025-08-23 11:29:48 [INFO]: Epoch 012 - training loss (MAE): 1.6081, validation MSE: 1.7409
2025-08-23 11:35:42 [INFO]: Epoch 013 - training loss (MAE): 1.5996, validation MSE: 1.7522
2025-08-23 11:41:39 [INFO]: Epoch 014 - training loss (MAE): 1.5956, validation MSE: 1.7411
2025-08-23 11:47:27 [INFO]: Epoch 015 - training loss (MAE): 1.5984, validation MSE: 1.7529
2025-08-23 11:47:27 [INFO]: Finished training. The best model is from epoch#12.
2025-08-23 11:47:29 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.4657| MSE: 0.5438| RMSE: 0.7374| MRE: 0.6540| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: physionet_2012 | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | physionet_2012 | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=35)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 11:49:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 11:49:18 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-23 11:49:18 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-23 11:49:18 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-23 11:49:18 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-23 11:49:24 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-23 11:49:24 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-23 11:49:24 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-23 11:49:24 [INFO]: Total sample number: 3997
2025-08-23 11:49:24 [INFO]: Training set size: 2557 (63.97%)
2025-08-23 11:49:24 [INFO]: Validation set size: 640 (16.01%)
2025-08-23 11:49:24 [INFO]: Test set size: 800 (20.02%)
2025-08-23 11:49:24 [INFO]: Number of steps: 48
2025-08-23 11:49:24 [INFO]: Number of features: 37
2025-08-23 11:49:24 [INFO]: Train set missing rate: 79.68%
2025-08-23 11:49:24 [INFO]: Validating set missing rate: 83.68%
2025-08-23 11:49:24 [INFO]: Test set missing rate: 83.99%
✅ Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-23 11:49:24 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 11:49:24 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 11:49:25 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 11:49:25 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 11:49:25 [INFO]: Using customized MAE as the training loss function.
2025-08-23 11:49:25 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 11:49:26 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 11:49:26 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 11:55:28 [INFO]: Epoch 001 - training loss (MAE): 1.7846, validation MSE: 1.9794
2025-08-23 12:01:26 [INFO]: Epoch 002 - training loss (MAE): 1.6723, validation MSE: 1.8422
2025-08-23 12:07:30 [INFO]: Epoch 003 - training loss (MAE): 1.6177, validation MSE: 1.8358
2025-08-23 12:13:25 [INFO]: Epoch 004 - training loss (MAE): 1.5976, validation MSE: 1.8363
2025-08-23 12:19:17 [INFO]: Epoch 005 - training loss (MAE): 1.5984, validation MSE: 1.8151
2025-08-23 12:25:11 [INFO]: Epoch 006 - training loss (MAE): 1.5921, validation MSE: 1.8100
2025-08-23 12:31:05 [INFO]: Epoch 007 - training loss (MAE): 1.5926, validation MSE: 1.8265
2025-08-23 12:36:53 [INFO]: Epoch 008 - training loss (MAE): 1.5971, validation MSE: 1.8133
2025-08-23 12:42:43 [INFO]: Epoch 009 - training loss (MAE): 1.6034, validation MSE: 1.8176
2025-08-23 12:48:43 [INFO]: Epoch 010 - training loss (MAE): 1.6339, validation MSE: 1.8779
2025-08-23 12:54:37 [INFO]: Epoch 011 - training loss (MAE): 1.6299, validation MSE: 1.8310
2025-08-23 12:54:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-23 12:54:37 [INFO]: Finished training. The best model is from epoch#6.
2025-08-23 12:54:38 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.4850| MSE: 0.5894| RMSE: 0.7677| MRE: 0.6847| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: physionet_2012 | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | physionet_2012 | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=35)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 12:56:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 12:56:12 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-23 12:56:12 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-23 12:56:12 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-23 12:56:12 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-23 12:56:18 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-23 12:56:18 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-23 12:56:18 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-23 12:56:18 [INFO]: Total sample number: 3997
2025-08-23 12:56:18 [INFO]: Training set size: 2557 (63.97%)
2025-08-23 12:56:18 [INFO]: Validation set size: 640 (16.01%)
2025-08-23 12:56:18 [INFO]: Test set size: 800 (20.02%)
2025-08-23 12:56:18 [INFO]: Number of steps: 48
2025-08-23 12:56:18 [INFO]: Number of features: 37
2025-08-23 12:56:18 [INFO]: Train set missing rate: 79.68%
2025-08-23 12:56:18 [INFO]: Validating set missing rate: 85.72%
2025-08-23 12:56:18 [INFO]: Test set missing rate: 85.98%
✅ Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-23 12:56:18 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 12:56:18 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 12:56:18 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 12:56:18 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 12:56:18 [INFO]: Using customized MAE as the training loss function.
2025-08-23 12:56:18 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 12:56:20 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 12:56:20 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 13:02:13 [INFO]: Epoch 001 - training loss (MAE): 1.7846, validation MSE: 2.0051
2025-08-23 13:08:16 [INFO]: Epoch 002 - training loss (MAE): 1.6723, validation MSE: 1.8712
2025-08-23 13:14:01 [INFO]: Epoch 003 - training loss (MAE): 1.6177, validation MSE: 1.8550
2025-08-23 13:19:45 [INFO]: Epoch 004 - training loss (MAE): 1.5976, validation MSE: 1.8455
2025-08-23 13:25:26 [INFO]: Epoch 005 - training loss (MAE): 1.5984, validation MSE: 1.8373
2025-08-23 13:31:11 [INFO]: Epoch 006 - training loss (MAE): 1.5921, validation MSE: 1.8345
2025-08-23 13:36:55 [INFO]: Epoch 007 - training loss (MAE): 1.5926, validation MSE: 1.8351
2025-08-23 13:42:32 [INFO]: Epoch 008 - training loss (MAE): 1.5971, validation MSE: 1.8263
2025-08-23 13:48:17 [INFO]: Epoch 009 - training loss (MAE): 1.6034, validation MSE: 1.8396
2025-08-23 13:53:59 [INFO]: Epoch 010 - training loss (MAE): 1.6339, validation MSE: 1.9039
2025-08-23 13:59:38 [INFO]: Epoch 011 - training loss (MAE): 1.6299, validation MSE: 1.8563
2025-08-23 14:05:19 [INFO]: Epoch 012 - training loss (MAE): 1.6081, validation MSE: 1.8194
2025-08-23 14:11:07 [INFO]: Epoch 013 - training loss (MAE): 1.5996, validation MSE: 1.8255
2025-08-23 14:16:55 [INFO]: Epoch 014 - training loss (MAE): 1.5956, validation MSE: 1.8200
2025-08-23 14:22:37 [INFO]: Epoch 015 - training loss (MAE): 1.5984, validation MSE: 1.8205
2025-08-23 14:22:37 [INFO]: Finished training. The best model is from epoch#12.
2025-08-23 14:22:39 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.5019| MSE: 0.5991| RMSE: 0.7740| MRE: 0.7103| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: physionet_2012 | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | physionet_2012 | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=35)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 14:24:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 14:24:34 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-23 14:24:34 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-23 14:24:34 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-23 14:24:34 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-23 14:24:40 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-23 14:24:40 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-23 14:24:40 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-23 14:24:40 [INFO]: Total sample number: 3997
2025-08-23 14:24:40 [INFO]: Training set size: 2557 (63.97%)
2025-08-23 14:24:40 [INFO]: Validation set size: 640 (16.01%)
2025-08-23 14:24:40 [INFO]: Test set size: 800 (20.02%)
2025-08-23 14:24:40 [INFO]: Number of steps: 48
2025-08-23 14:24:40 [INFO]: Number of features: 37
2025-08-23 14:24:40 [INFO]: Train set missing rate: 79.68%
2025-08-23 14:24:40 [INFO]: Validating set missing rate: 87.77%
2025-08-23 14:24:40 [INFO]: Test set missing rate: 87.98%
✅ Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-23 14:24:40 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 14:24:40 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 14:24:40 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 14:24:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 14:24:40 [INFO]: Using customized MAE as the training loss function.
2025-08-23 14:24:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 14:24:42 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 14:24:42 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 14:30:40 [INFO]: Epoch 001 - training loss (MAE): 1.7846, validation MSE: 2.0556
2025-08-23 14:36:32 [INFO]: Epoch 002 - training loss (MAE): 1.6723, validation MSE: 1.9418
2025-08-23 14:42:27 [INFO]: Epoch 003 - training loss (MAE): 1.6177, validation MSE: 1.9063
2025-08-23 14:48:16 [INFO]: Epoch 004 - training loss (MAE): 1.5976, validation MSE: 1.8963
2025-08-23 14:54:08 [INFO]: Epoch 005 - training loss (MAE): 1.5984, validation MSE: 1.9016
2025-08-23 15:00:00 [INFO]: Epoch 006 - training loss (MAE): 1.5921, validation MSE: 1.8927
2025-08-23 15:05:48 [INFO]: Epoch 007 - training loss (MAE): 1.5926, validation MSE: 1.8866
2025-08-23 15:11:46 [INFO]: Epoch 008 - training loss (MAE): 1.5971, validation MSE: 1.8885
2025-08-23 15:17:32 [INFO]: Epoch 009 - training loss (MAE): 1.6034, validation MSE: 1.8965
2025-08-23 15:23:28 [INFO]: Epoch 010 - training loss (MAE): 1.6339, validation MSE: 1.9627
2025-08-23 15:29:30 [INFO]: Epoch 011 - training loss (MAE): 1.6299, validation MSE: 1.9174
2025-08-23 15:35:41 [INFO]: Epoch 012 - training loss (MAE): 1.6081, validation MSE: 1.8834
2025-08-23 15:41:46 [INFO]: Epoch 013 - training loss (MAE): 1.5996, validation MSE: 1.8950
2025-08-23 15:47:43 [INFO]: Epoch 014 - training loss (MAE): 1.5956, validation MSE: 1.8873
2025-08-23 15:53:46 [INFO]: Epoch 015 - training loss (MAE): 1.5984, validation MSE: 1.8879
2025-08-23 15:53:46 [INFO]: Finished training. The best model is from epoch#12.
2025-08-23 15:53:48 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.5231| MSE: 0.6394| RMSE: 0.7996| MRE: 0.7413| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: physionet_2012 | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | physionet_2012 | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=35)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 15:55:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 15:55:26 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-23 15:55:26 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-23 15:55:26 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-23 15:55:27 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-23 15:55:32 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-23 15:55:32 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-23 15:55:32 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-23 15:55:32 [INFO]: Total sample number: 3997
2025-08-23 15:55:32 [INFO]: Training set size: 2557 (63.97%)
2025-08-23 15:55:32 [INFO]: Validation set size: 640 (16.01%)
2025-08-23 15:55:32 [INFO]: Test set size: 800 (20.02%)
2025-08-23 15:55:32 [INFO]: Number of steps: 48
2025-08-23 15:55:32 [INFO]: Number of features: 37
2025-08-23 15:55:32 [INFO]: Train set missing rate: 79.68%
2025-08-23 15:55:32 [INFO]: Validating set missing rate: 89.79%
2025-08-23 15:55:32 [INFO]: Test set missing rate: 89.99%
✅ Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-23 15:55:32 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 15:55:32 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 15:55:33 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 15:55:33 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 15:55:33 [INFO]: Using customized MAE as the training loss function.
2025-08-23 15:55:33 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 15:55:34 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 15:55:34 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 16:01:34 [INFO]: Epoch 001 - training loss (MAE): 1.7846, validation MSE: 2.1629
2025-08-23 16:07:21 [INFO]: Epoch 002 - training loss (MAE): 1.6723, validation MSE: 2.0500
2025-08-23 16:13:11 [INFO]: Epoch 003 - training loss (MAE): 1.6177, validation MSE: 2.0298
2025-08-23 16:19:09 [INFO]: Epoch 004 - training loss (MAE): 1.5976, validation MSE: 2.0041
2025-08-23 16:24:56 [INFO]: Epoch 005 - training loss (MAE): 1.5984, validation MSE: 2.0099
2025-08-23 16:30:45 [INFO]: Epoch 006 - training loss (MAE): 1.5921, validation MSE: 1.9966
2025-08-23 16:36:30 [INFO]: Epoch 007 - training loss (MAE): 1.5926, validation MSE: 2.0033
2025-08-23 16:42:07 [INFO]: Epoch 008 - training loss (MAE): 1.5971, validation MSE: 2.0096
2025-08-23 16:48:01 [INFO]: Epoch 009 - training loss (MAE): 1.6034, validation MSE: 2.0180
2025-08-23 16:53:40 [INFO]: Epoch 010 - training loss (MAE): 1.6339, validation MSE: 2.0717
2025-08-23 16:59:33 [INFO]: Epoch 011 - training loss (MAE): 1.6299, validation MSE: 2.0312
2025-08-23 16:59:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-23 16:59:33 [INFO]: Finished training. The best model is from epoch#6.
2025-08-23 16:59:35 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.5485| MSE: 0.6963| RMSE: 0.8345| MRE: 0.7774| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: physionet_2012 | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:01:07 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:01:07 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-23 17:01:07 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-23 17:01:07 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:01:07 [INFO]: Loaded successfully!
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:07 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:08 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:08 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:08 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:08 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:08 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-23 17:01:08 [INFO]: Original df missing rate: 0.016
2025-08-23 17:01:08 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-23 17:01:08 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-23 17:01:08 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-23 17:01:08 [INFO]: Total sample number: 730
2025-08-23 17:01:08 [INFO]: Training set size: 426 (58.36%)
2025-08-23 17:01:08 [INFO]: Validation set size: 152 (20.82%)
2025-08-23 17:01:08 [INFO]: Test set size: 152 (20.82%)
2025-08-23 17:01:08 [INFO]: Number of steps: 48
2025-08-23 17:01:08 [INFO]: Number of features: 132
2025-08-23 17:01:08 [INFO]: Train set missing rate: 11.67%
2025-08-23 17:01:08 [INFO]: Validating set missing rate: 10.80%
2025-08-23 17:01:08 [INFO]: Test set missing rate: 11.13%
✅ Dataset 'beijing_multisite_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-23 17:01:08 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:01:08 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:01:08 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:01:08 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:01:08 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:01:08 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:01:10 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:01:10 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
2025-08-23 17:01:26 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:01:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:01:34 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-23 17:01:34 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-23 17:01:34 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:01:34 [INFO]: Loaded successfully!
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:01:34 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-23 17:01:34 [INFO]: Original df missing rate: 0.016
2025-08-23 17:01:34 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-23 17:01:34 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-23 17:01:34 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-23 17:01:34 [INFO]: Total sample number: 730
2025-08-23 17:01:34 [INFO]: Training set size: 426 (58.36%)
2025-08-23 17:01:34 [INFO]: Validation set size: 152 (20.82%)
2025-08-23 17:01:34 [INFO]: Test set size: 152 (20.82%)
2025-08-23 17:01:34 [INFO]: Number of steps: 48
2025-08-23 17:01:34 [INFO]: Number of features: 132
2025-08-23 17:01:34 [INFO]: Train set missing rate: 21.51%
2025-08-23 17:01:34 [INFO]: Validating set missing rate: 20.70%
2025-08-23 17:01:34 [INFO]: Test set missing rate: 20.98%
✅ Dataset 'beijing_multisite_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-23 17:01:34 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:01:34 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:01:35 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:01:35 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:01:35 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:01:35 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:01:36 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:01:36 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
2025-08-23 17:01:51 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.07 GiB. GPU 3 has a total capacity of 23.58 GiB of which 487.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 740.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.07 GiB. GPU 3 has a total capacity of 23.58 GiB of which 487.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 740.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:01:59 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:01:59 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-23 17:01:59 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-23 17:01:59 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:01:59 [INFO]: Loaded successfully!
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:00 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-23 17:02:00 [INFO]: Original df missing rate: 0.016
2025-08-23 17:02:00 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-23 17:02:00 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-23 17:02:00 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-23 17:02:00 [INFO]: Total sample number: 730
2025-08-23 17:02:00 [INFO]: Training set size: 426 (58.36%)
2025-08-23 17:02:00 [INFO]: Validation set size: 152 (20.82%)
2025-08-23 17:02:00 [INFO]: Test set size: 152 (20.82%)
2025-08-23 17:02:00 [INFO]: Number of steps: 48
2025-08-23 17:02:00 [INFO]: Number of features: 132
2025-08-23 17:02:00 [INFO]: Train set missing rate: 31.34%
2025-08-23 17:02:00 [INFO]: Validating set missing rate: 30.61%
2025-08-23 17:02:00 [INFO]: Test set missing rate: 30.86%
✅ Dataset 'beijing_multisite_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-23 17:02:00 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:02:00 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:02:00 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:02:00 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:02:00 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:02:00 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:02:02 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:02:02 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
2025-08-23 17:02:17 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:02:25 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:02:25 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-23 17:02:25 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-23 17:02:25 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:02:25 [INFO]: Loaded successfully!
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:25 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-23 17:02:25 [INFO]: Original df missing rate: 0.016
2025-08-23 17:02:26 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-23 17:02:26 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-23 17:02:26 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-23 17:02:26 [INFO]: Total sample number: 730
2025-08-23 17:02:26 [INFO]: Training set size: 426 (58.36%)
2025-08-23 17:02:26 [INFO]: Validation set size: 152 (20.82%)
2025-08-23 17:02:26 [INFO]: Test set size: 152 (20.82%)
2025-08-23 17:02:26 [INFO]: Number of steps: 48
2025-08-23 17:02:26 [INFO]: Number of features: 132
2025-08-23 17:02:26 [INFO]: Train set missing rate: 41.13%
2025-08-23 17:02:26 [INFO]: Validating set missing rate: 40.51%
2025-08-23 17:02:26 [INFO]: Test set missing rate: 40.77%
✅ Dataset 'beijing_multisite_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-23 17:02:26 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:02:26 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:02:26 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:02:26 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:02:26 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:02:26 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:02:28 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:02:28 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
2025-08-23 17:02:41 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 3 has a total capacity of 23.58 GiB of which 345.06 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 756.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:02:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:02:49 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-23 17:02:49 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-23 17:02:49 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:02:49 [INFO]: Loaded successfully!
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-23 17:02:49 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-23 17:02:49 [INFO]: Original df missing rate: 0.016
2025-08-23 17:02:49 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-23 17:02:49 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-23 17:02:49 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-23 17:02:49 [INFO]: Total sample number: 730
2025-08-23 17:02:49 [INFO]: Training set size: 426 (58.36%)
2025-08-23 17:02:49 [INFO]: Validation set size: 152 (20.82%)
2025-08-23 17:02:49 [INFO]: Test set size: 152 (20.82%)
2025-08-23 17:02:49 [INFO]: Number of steps: 48
2025-08-23 17:02:49 [INFO]: Number of features: 132
2025-08-23 17:02:49 [INFO]: Train set missing rate: 50.93%
2025-08-23 17:02:49 [INFO]: Validating set missing rate: 50.44%
2025-08-23 17:02:49 [INFO]: Test set missing rate: 50.71%
✅ Dataset 'beijing_multisite_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-23 17:02:49 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:02:49 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:02:50 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:02:50 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:02:50 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:02:50 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:02:51 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:02:51 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
2025-08-23 17:03:05 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.07 GiB. GPU 3 has a total capacity of 23.58 GiB of which 487.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 740.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 440, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 365, in forward
    hidden_states = self.c_fc(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/pytorch_utils.py", line 116, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.07 GiB. GPU 3 has a total capacity of 23.58 GiB of which 487.06 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 21.98 GiB is allocated by PyTorch, and 740.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | italy_air_quality | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:03:13 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:03:13 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-23 17:03:13 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-23 17:03:13 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:03:13 [INFO]: Loaded successfully!
2025-08-23 17:03:13 [INFO]: Total sample number: 192
2025-08-23 17:03:13 [INFO]: Training set size: 116 (60.42%)
2025-08-23 17:03:13 [INFO]: Validation set size: 38 (19.79%)
2025-08-23 17:03:13 [INFO]: Test set size: 38 (19.79%)
2025-08-23 17:03:13 [INFO]: Number of steps: 48
2025-08-23 17:03:13 [INFO]: Number of features: 13
2025-08-23 17:03:13 [INFO]: Train set missing rate: 9.87%
2025-08-23 17:03:13 [INFO]: Validating set missing rate: 9.86%
2025-08-23 17:03:13 [INFO]: Test set missing rate: 9.87%
✅ Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-23 17:03:13 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:03:13 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:03:13 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:03:13 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:03:13 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:03:13 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:03:15 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:03:15 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 17:03:26 [INFO]: Epoch 001 - training loss (MAE): 2.4759, validation MSE: 3.3383
2025-08-23 17:03:35 [INFO]: Epoch 002 - training loss (MAE): 1.6267, validation MSE: 2.5857
2025-08-23 17:03:45 [INFO]: Epoch 003 - training loss (MAE): 1.5695, validation MSE: 2.1807
2025-08-23 17:03:55 [INFO]: Epoch 004 - training loss (MAE): 1.5447, validation MSE: 2.2255
2025-08-23 17:04:04 [INFO]: Epoch 005 - training loss (MAE): 1.5379, validation MSE: 2.3801
2025-08-23 17:04:14 [INFO]: Epoch 006 - training loss (MAE): 1.5166, validation MSE: 2.2442
2025-08-23 17:04:24 [INFO]: Epoch 007 - training loss (MAE): 1.5310, validation MSE: 2.2230
2025-08-23 17:04:33 [INFO]: Epoch 008 - training loss (MAE): 1.4501, validation MSE: 2.0720
2025-08-23 17:04:43 [INFO]: Epoch 009 - training loss (MAE): 1.4064, validation MSE: 2.0082
2025-08-23 17:04:53 [INFO]: Epoch 010 - training loss (MAE): 1.3812, validation MSE: 1.8055
2025-08-23 17:05:02 [INFO]: Epoch 011 - training loss (MAE): 1.3968, validation MSE: 1.7217
2025-08-23 17:05:13 [INFO]: Epoch 012 - training loss (MAE): 1.3551, validation MSE: 1.7913
2025-08-23 17:05:23 [INFO]: Epoch 013 - training loss (MAE): 1.2708, validation MSE: 1.7110
2025-08-23 17:05:33 [INFO]: Epoch 014 - training loss (MAE): 1.2583, validation MSE: 1.7816
2025-08-23 17:05:43 [INFO]: Epoch 015 - training loss (MAE): 1.2559, validation MSE: 1.7058
2025-08-23 17:05:43 [INFO]: Finished training. The best model is from epoch#15.
2025-08-23 17:05:44 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.3928| MSE: 0.4143| RMSE: 0.6437| MRE: 0.5112| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: italy_air_quality | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | italy_air_quality | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:05:59 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:05:59 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-23 17:05:59 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-23 17:05:59 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:05:59 [INFO]: Loaded successfully!
2025-08-23 17:05:59 [INFO]: Total sample number: 192
2025-08-23 17:05:59 [INFO]: Training set size: 116 (60.42%)
2025-08-23 17:05:59 [INFO]: Validation set size: 38 (19.79%)
2025-08-23 17:05:59 [INFO]: Test set size: 38 (19.79%)
2025-08-23 17:05:59 [INFO]: Number of steps: 48
2025-08-23 17:05:59 [INFO]: Number of features: 13
2025-08-23 17:05:59 [INFO]: Train set missing rate: 19.79%
2025-08-23 17:05:59 [INFO]: Validating set missing rate: 19.86%
2025-08-23 17:05:59 [INFO]: Test set missing rate: 19.88%
✅ Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-23 17:05:59 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:05:59 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:06:00 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:06:00 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:06:00 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:06:00 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:06:01 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:06:01 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 17:06:12 [INFO]: Epoch 001 - training loss (MAE): 2.5213, validation MSE: 4.3451
2025-08-23 17:06:21 [INFO]: Epoch 002 - training loss (MAE): 1.6950, validation MSE: 3.3367
2025-08-23 17:06:30 [INFO]: Epoch 003 - training loss (MAE): 1.6151, validation MSE: 2.7148
2025-08-23 17:06:40 [INFO]: Epoch 004 - training loss (MAE): 1.5742, validation MSE: 2.6153
2025-08-23 17:06:50 [INFO]: Epoch 005 - training loss (MAE): 1.5928, validation MSE: 2.6955
2025-08-23 17:07:00 [INFO]: Epoch 006 - training loss (MAE): 1.5512, validation MSE: 2.7259
2025-08-23 17:07:09 [INFO]: Epoch 007 - training loss (MAE): 1.5831, validation MSE: 2.6096
2025-08-23 17:07:19 [INFO]: Epoch 008 - training loss (MAE): 1.4948, validation MSE: 2.4317
2025-08-23 17:07:28 [INFO]: Epoch 009 - training loss (MAE): 1.5271, validation MSE: 2.7274
2025-08-23 17:07:38 [INFO]: Epoch 010 - training loss (MAE): 1.5238, validation MSE: 2.5708
2025-08-23 17:07:47 [INFO]: Epoch 011 - training loss (MAE): 1.5392, validation MSE: 2.5648
2025-08-23 17:07:57 [INFO]: Epoch 012 - training loss (MAE): 1.4944, validation MSE: 2.6449
2025-08-23 17:08:07 [INFO]: Epoch 013 - training loss (MAE): 1.4769, validation MSE: 2.4267
2025-08-23 17:08:16 [INFO]: Epoch 014 - training loss (MAE): 1.4337, validation MSE: 2.4750
2025-08-23 17:08:25 [INFO]: Epoch 015 - training loss (MAE): 1.4914, validation MSE: 2.5605
2025-08-23 17:08:25 [INFO]: Finished training. The best model is from epoch#13.
2025-08-23 17:08:26 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.4469| MSE: 0.5701| RMSE: 0.7551| MRE: 0.5766| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: italy_air_quality | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | italy_air_quality | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:08:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:08:41 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-23 17:08:41 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-23 17:08:41 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:08:41 [INFO]: Loaded successfully!
2025-08-23 17:08:41 [INFO]: Total sample number: 192
2025-08-23 17:08:41 [INFO]: Training set size: 116 (60.42%)
2025-08-23 17:08:41 [INFO]: Validation set size: 38 (19.79%)
2025-08-23 17:08:41 [INFO]: Test set size: 38 (19.79%)
2025-08-23 17:08:41 [INFO]: Number of steps: 48
2025-08-23 17:08:41 [INFO]: Number of features: 13
2025-08-23 17:08:41 [INFO]: Train set missing rate: 29.80%
2025-08-23 17:08:41 [INFO]: Validating set missing rate: 29.90%
2025-08-23 17:08:41 [INFO]: Test set missing rate: 30.05%
✅ Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-23 17:08:41 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:08:41 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:08:42 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:08:42 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:08:42 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:08:42 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:08:43 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:08:43 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 17:08:53 [INFO]: Epoch 001 - training loss (MAE): 2.4284, validation MSE: 3.9648
2025-08-23 17:09:02 [INFO]: Epoch 002 - training loss (MAE): 1.6973, validation MSE: 3.3305
2025-08-23 17:09:10 [INFO]: Epoch 003 - training loss (MAE): 1.6379, validation MSE: 2.9681
2025-08-23 17:09:19 [INFO]: Epoch 004 - training loss (MAE): 1.6205, validation MSE: 2.8941
2025-08-23 17:09:28 [INFO]: Epoch 005 - training loss (MAE): 1.6276, validation MSE: 3.1463
2025-08-23 17:09:37 [INFO]: Epoch 006 - training loss (MAE): 1.6053, validation MSE: 3.1147
2025-08-23 17:09:46 [INFO]: Epoch 007 - training loss (MAE): 1.6286, validation MSE: 3.0486
2025-08-23 17:09:54 [INFO]: Epoch 008 - training loss (MAE): 1.5321, validation MSE: 2.7879
2025-08-23 17:10:03 [INFO]: Epoch 009 - training loss (MAE): 1.4995, validation MSE: 2.6536
2025-08-23 17:10:12 [INFO]: Epoch 010 - training loss (MAE): 1.4691, validation MSE: 2.6611
2025-08-23 17:10:21 [INFO]: Epoch 011 - training loss (MAE): 1.4422, validation MSE: 2.5205
2025-08-23 17:10:30 [INFO]: Epoch 012 - training loss (MAE): 1.4291, validation MSE: 2.4527
2025-08-23 17:10:39 [INFO]: Epoch 013 - training loss (MAE): 1.3626, validation MSE: 2.1719
2025-08-23 17:10:47 [INFO]: Epoch 014 - training loss (MAE): 1.3300, validation MSE: 2.1988
2025-08-23 17:10:56 [INFO]: Epoch 015 - training loss (MAE): 1.3556, validation MSE: 2.6163
2025-08-23 17:10:56 [INFO]: Finished training. The best model is from epoch#13.
2025-08-23 17:10:57 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.4637| MSE: 0.5768| RMSE: 0.7595| MRE: 0.5933| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: italy_air_quality | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | italy_air_quality | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
🚀 Running imputation pipeline for model: timellm
2025-08-23 17:11:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:11:12 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-23 17:11:12 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-23 17:11:12 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:11:12 [INFO]: Loaded successfully!
✅ Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-23 17:11:12 [INFO]: Total sample number: 192
2025-08-23 17:11:12 [INFO]: Training set size: 116 (60.42%)
2025-08-23 17:11:12 [INFO]: Validation set size: 38 (19.79%)
2025-08-23 17:11:12 [INFO]: Test set size: 38 (19.79%)
2025-08-23 17:11:12 [INFO]: Number of steps: 48
2025-08-23 17:11:12 [INFO]: Number of features: 13
2025-08-23 17:11:12 [INFO]: Train set missing rate: 39.84%
2025-08-23 17:11:12 [INFO]: Validating set missing rate: 39.87%
2025-08-23 17:11:12 [INFO]: Test set missing rate: 40.52%
2025-08-23 17:11:12 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:11:12 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:11:12 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:11:12 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:11:12 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:11:12 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:11:14 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:11:14 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 17:11:24 [INFO]: Epoch 001 - training loss (MAE): 2.4395, validation MSE: 4.9405
2025-08-23 17:11:33 [INFO]: Epoch 002 - training loss (MAE): 1.7497, validation MSE: 4.3753
2025-08-23 17:11:42 [INFO]: Epoch 003 - training loss (MAE): 1.6988, validation MSE: 3.7921
2025-08-23 17:11:52 [INFO]: Epoch 004 - training loss (MAE): 1.6633, validation MSE: 3.6623
2025-08-23 17:12:01 [INFO]: Epoch 005 - training loss (MAE): 1.6794, validation MSE: 3.8502
2025-08-23 17:12:10 [INFO]: Epoch 006 - training loss (MAE): 1.6421, validation MSE: 3.8602
2025-08-23 17:12:19 [INFO]: Epoch 007 - training loss (MAE): 1.6743, validation MSE: 3.8144
2025-08-23 17:12:28 [INFO]: Epoch 008 - training loss (MAE): 1.5995, validation MSE: 3.7001
2025-08-23 17:12:37 [INFO]: Epoch 009 - training loss (MAE): 1.5909, validation MSE: 3.6178
2025-08-23 17:12:47 [INFO]: Epoch 010 - training loss (MAE): 1.5890, validation MSE: 3.5958
2025-08-23 17:12:56 [INFO]: Epoch 011 - training loss (MAE): 1.5927, validation MSE: 3.6555
2025-08-23 17:13:05 [INFO]: Epoch 012 - training loss (MAE): 1.6196, validation MSE: 3.6859
2025-08-23 17:13:14 [INFO]: Epoch 013 - training loss (MAE): 1.5835, validation MSE: 3.5471
2025-08-23 17:13:23 [INFO]: Epoch 014 - training loss (MAE): 1.5029, validation MSE: 3.4257
2025-08-23 17:13:32 [INFO]: Epoch 015 - training loss (MAE): 1.5725, validation MSE: 3.3649
2025-08-23 17:13:32 [INFO]: Finished training. The best model is from epoch#15.
2025-08-23 17:13:33 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.5176| MSE: 0.7260| RMSE: 0.8520| MRE: 0.6644| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: italy_air_quality | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | italy_air_quality | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=48, N_FEATURES=36)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:13:48 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:13:48 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-23 17:13:48 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-23 17:13:48 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-23 17:13:48 [INFO]: Loaded successfully!
2025-08-23 17:13:48 [INFO]: Total sample number: 192
2025-08-23 17:13:48 [INFO]: Training set size: 116 (60.42%)
2025-08-23 17:13:48 [INFO]: Validation set size: 38 (19.79%)
2025-08-23 17:13:48 [INFO]: Test set size: 38 (19.79%)
2025-08-23 17:13:48 [INFO]: Number of steps: 48
2025-08-23 17:13:48 [INFO]: Number of features: 13
2025-08-23 17:13:48 [INFO]: Train set missing rate: 50.11%
2025-08-23 17:13:48 [INFO]: Validating set missing rate: 50.03%
2025-08-23 17:13:48 [INFO]: Test set missing rate: 50.12%
✅ Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-23 17:13:48 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:13:48 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:13:48 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:13:48 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:13:48 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:13:48 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:13:50 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:13:50 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,909,120
 ├─ Trainable parameters: 50,435,904
 └─ Trainable ratio: 52.04%
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
2025-08-23 17:14:00 [INFO]: Epoch 001 - training loss (MAE): 2.3198, validation MSE: 5.4815
2025-08-23 17:14:08 [INFO]: Epoch 002 - training loss (MAE): 1.7769, validation MSE: 4.6834
2025-08-23 17:14:17 [INFO]: Epoch 003 - training loss (MAE): 1.7199, validation MSE: 4.6004
2025-08-23 17:14:26 [INFO]: Epoch 004 - training loss (MAE): 1.7207, validation MSE: 4.4538
2025-08-23 17:14:35 [INFO]: Epoch 005 - training loss (MAE): 1.7384, validation MSE: 4.3645
2025-08-23 17:14:43 [INFO]: Epoch 006 - training loss (MAE): 1.6874, validation MSE: 4.3836
2025-08-23 17:14:53 [INFO]: Epoch 007 - training loss (MAE): 1.7186, validation MSE: 4.4468
2025-08-23 17:15:01 [INFO]: Epoch 008 - training loss (MAE): 1.6244, validation MSE: 4.2668
2025-08-23 17:15:10 [INFO]: Epoch 009 - training loss (MAE): 1.5973, validation MSE: 4.1550
2025-08-23 17:15:19 [INFO]: Epoch 010 - training loss (MAE): 1.5885, validation MSE: 4.7372
2025-08-23 17:15:28 [INFO]: Epoch 011 - training loss (MAE): 1.6012, validation MSE: 4.3565
2025-08-23 17:15:37 [INFO]: Epoch 012 - training loss (MAE): 1.6231, validation MSE: 3.7720
2025-08-23 17:15:46 [INFO]: Epoch 013 - training loss (MAE): 1.5450, validation MSE: 3.9826
2025-08-23 17:15:54 [INFO]: Epoch 014 - training loss (MAE): 1.4827, validation MSE: 3.9538
2025-08-23 17:16:04 [INFO]: Epoch 015 - training loss (MAE): 1.5653, validation MSE: 4.0517
2025-08-23 17:16:04 [INFO]: Finished training. The best model is from epoch#12.
2025-08-23 17:16:05 [INFO]: Saved the model to output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/TimeLLM.pypots
[LLM4IMP] Testing —— MAE: 0.5497| MSE: 0.8255| RMSE: 0.9086| MRE: 0.7080| 
📊 Metrics saved to: output/imputation/cuda/timellm/BERT/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/metrics/timellm_metrics.json
✅ DONE: italy_air_quality | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | pems_traffic | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=228)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:16:21 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:16:21 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-23 17:16:21 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-23 17:16:21 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-23 17:16:21 [INFO]: Loaded successfully!
2025-08-23 17:16:21 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-23 17:16:21 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-23 17:16:21 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-23 17:16:21 [INFO]: Total sample number: 182
2025-08-23 17:16:21 [INFO]: Training set size: 114 (62.64%)
2025-08-23 17:16:21 [INFO]: Validation set size: 30 (16.48%)
2025-08-23 17:16:21 [INFO]: Test set size: 38 (20.88%)
2025-08-23 17:16:21 [INFO]: Number of steps: 96
2025-08-23 17:16:21 [INFO]: Number of features: 862
2025-08-23 17:16:21 [INFO]: Train set missing rate: 9.98%
2025-08-23 17:16:21 [INFO]: Validating set missing rate: 10.03%
2025-08-23 17:16:21 [INFO]: Test set missing rate: 10.00%
✅ Dataset 'pems_traffic' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-23 17:16:22 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:16:22 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:16:22 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:16:22 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:16:22 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:16:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:16:23 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:16:23 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:18:41 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): pems_traffic | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | pems_traffic | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=228)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:18:53 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:18:53 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-23 17:18:53 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-23 17:18:53 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-23 17:18:53 [INFO]: Loaded successfully!
2025-08-23 17:18:53 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-23 17:18:53 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-23 17:18:53 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-23 17:18:54 [INFO]: Total sample number: 182
2025-08-23 17:18:54 [INFO]: Training set size: 114 (62.64%)
2025-08-23 17:18:54 [INFO]: Validation set size: 30 (16.48%)
2025-08-23 17:18:54 [INFO]: Test set size: 38 (20.88%)
2025-08-23 17:18:54 [INFO]: Number of steps: 96
2025-08-23 17:18:54 [INFO]: Number of features: 862
2025-08-23 17:18:54 [INFO]: Train set missing rate: 20.00%
2025-08-23 17:18:54 [INFO]: Validating set missing rate: 20.00%
2025-08-23 17:18:54 [INFO]: Test set missing rate: 20.01%
✅ Dataset 'pems_traffic' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-23 17:18:54 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:18:54 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:18:54 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:18:54 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:18:54 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:18:54 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:18:56 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:18:56 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:20:59 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.17 GiB. GPU 3 has a total capacity of 23.58 GiB of which 367.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 672.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.17 GiB. GPU 3 has a total capacity of 23.58 GiB of which 367.06 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 22.17 GiB is allocated by PyTorch, and 672.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): pems_traffic | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | pems_traffic | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=228)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:21:11 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:21:11 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-23 17:21:11 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-23 17:21:11 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-23 17:21:11 [INFO]: Loaded successfully!
2025-08-23 17:21:11 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-23 17:21:11 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-23 17:21:11 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-23 17:21:12 [INFO]: Total sample number: 182
2025-08-23 17:21:12 [INFO]: Training set size: 114 (62.64%)
2025-08-23 17:21:12 [INFO]: Validation set size: 30 (16.48%)
2025-08-23 17:21:12 [INFO]: Test set size: 38 (20.88%)
2025-08-23 17:21:12 [INFO]: Number of steps: 96
2025-08-23 17:21:12 [INFO]: Number of features: 862
2025-08-23 17:21:12 [INFO]: Train set missing rate: 30.01%
2025-08-23 17:21:12 [INFO]: Validating set missing rate: 29.97%
2025-08-23 17:21:12 [INFO]: Test set missing rate: 30.02%
✅ Dataset 'pems_traffic' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-23 17:21:12 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:21:12 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:21:12 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:21:12 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:21:12 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:21:12 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:21:14 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:21:14 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:23:09 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): pems_traffic | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | pems_traffic | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=228)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
🚀 Running imputation pipeline for model: timellm
2025-08-23 17:23:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:23:22 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-23 17:23:22 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-23 17:23:22 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-23 17:23:22 [INFO]: Loaded successfully!
2025-08-23 17:23:22 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-23 17:23:22 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-23 17:23:22 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-23 17:23:23 [INFO]: Total sample number: 182
2025-08-23 17:23:23 [INFO]: Training set size: 114 (62.64%)
2025-08-23 17:23:23 [INFO]: Validation set size: 30 (16.48%)
2025-08-23 17:23:23 [INFO]: Test set size: 38 (20.88%)
2025-08-23 17:23:23 [INFO]: Number of steps: 96
2025-08-23 17:23:23 [INFO]: Number of features: 862
2025-08-23 17:23:23 [INFO]: Train set missing rate: 40.01%
2025-08-23 17:23:23 [INFO]: Validating set missing rate: 39.94%
2025-08-23 17:23:23 [INFO]: Test set missing rate: 40.04%
✅ Dataset 'pems_traffic' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-23 17:23:23 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:23:23 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:23:23 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:23:23 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:23:23 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:23:23 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:23:25 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:23:25 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:25:26 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): pems_traffic | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | pems_traffic | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=228)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:25:37 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:25:37 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-23 17:25:37 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-23 17:25:37 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-23 17:25:38 [INFO]: Loaded successfully!
2025-08-23 17:25:38 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-23 17:25:38 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-23 17:25:38 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-23 17:25:38 [INFO]: Total sample number: 182
2025-08-23 17:25:38 [INFO]: Training set size: 114 (62.64%)
2025-08-23 17:25:38 [INFO]: Validation set size: 30 (16.48%)
2025-08-23 17:25:38 [INFO]: Test set size: 38 (20.88%)
2025-08-23 17:25:38 [INFO]: Number of steps: 96
2025-08-23 17:25:38 [INFO]: Number of features: 862
2025-08-23 17:25:38 [INFO]: Train set missing rate: 50.01%
2025-08-23 17:25:38 [INFO]: Validating set missing rate: 49.93%
2025-08-23 17:25:38 [INFO]: Test set missing rate: 50.04%
✅ Dataset 'pems_traffic' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-23 17:25:38 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:25:38 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:25:38 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:25:38 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:25:38 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:25:38 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:25:40 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:25:40 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:27:28 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 861, in forward
    hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)
                    ~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.15 GiB. GPU 3 has a total capacity of 23.58 GiB of which 423.06 MiB is free. Including non-PyTorch memory, this process has 23.16 GiB memory in use. Of the allocated memory 22.11 GiB is allocated by PyTorch, and 671.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): pems_traffic | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | solar_alabama | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=137)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
🚀 Running imputation pipeline for model: timellm
2025-08-23 17:27:40 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:27:40 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-23 17:27:40 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-23 17:27:40 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-23 17:27:40 [INFO]: Loaded successfully!
2025-08-23 17:27:40 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-23 17:27:40 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-23 17:27:40 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-23 17:27:40 [INFO]: Total sample number: 547
2025-08-23 17:27:40 [INFO]: Training set size: 271 (49.54%)
2025-08-23 17:27:40 [INFO]: Validation set size: 138 (25.23%)
2025-08-23 17:27:40 [INFO]: Test set size: 138 (25.23%)
2025-08-23 17:27:40 [INFO]: Number of steps: 96
2025-08-23 17:27:40 [INFO]: Number of features: 137
2025-08-23 17:27:40 [INFO]: Train set missing rate: 9.98%
2025-08-23 17:27:40 [INFO]: Validating set missing rate: 9.94%
2025-08-23 17:27:40 [INFO]: Test set missing rate: 9.99%
✅ Dataset 'solar_alabama' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-23 17:27:40 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:27:40 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:27:40 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:27:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:27:40 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:27:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:27:42 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:27:42 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:28:02 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): solar_alabama | mr0.1_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | solar_alabama | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=137)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:28:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:28:10 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-23 17:28:10 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-23 17:28:10 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-23 17:28:10 [INFO]: Loaded successfully!
2025-08-23 17:28:10 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-23 17:28:10 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-23 17:28:10 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-23 17:28:10 [INFO]: Total sample number: 547
2025-08-23 17:28:10 [INFO]: Training set size: 271 (49.54%)
2025-08-23 17:28:10 [INFO]: Validation set size: 138 (25.23%)
2025-08-23 17:28:10 [INFO]: Test set size: 138 (25.23%)
2025-08-23 17:28:10 [INFO]: Number of steps: 96
2025-08-23 17:28:10 [INFO]: Number of features: 137
2025-08-23 17:28:10 [INFO]: Train set missing rate: 20.00%
2025-08-23 17:28:10 [INFO]: Validating set missing rate: 19.95%
2025-08-23 17:28:10 [INFO]: Test set missing rate: 20.01%
✅ Dataset 'solar_alabama' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-23 17:28:10 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:28:10 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:28:10 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:28:10 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:28:10 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:28:10 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:28:12 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:28:12 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:28:31 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.47 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.51 GiB is free. Including non-PyTorch memory, this process has 16.06 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 102.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.47 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.51 GiB is free. Including non-PyTorch memory, this process has 16.06 GiB memory in use. Of the allocated memory 15.57 GiB is allocated by PyTorch, and 102.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): solar_alabama | mr0.2_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | solar_alabama | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=137)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:28:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:28:39 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-23 17:28:39 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-23 17:28:39 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-23 17:28:39 [INFO]: Loaded successfully!
2025-08-23 17:28:39 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-23 17:28:39 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-23 17:28:39 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-23 17:28:39 [INFO]: Total sample number: 547
2025-08-23 17:28:39 [INFO]: Training set size: 271 (49.54%)
2025-08-23 17:28:39 [INFO]: Validation set size: 138 (25.23%)
2025-08-23 17:28:39 [INFO]: Test set size: 138 (25.23%)
2025-08-23 17:28:39 [INFO]: Number of steps: 96
2025-08-23 17:28:39 [INFO]: Number of features: 137
2025-08-23 17:28:39 [INFO]: Train set missing rate: 30.02%
2025-08-23 17:28:39 [INFO]: Validating set missing rate: 29.94%
2025-08-23 17:28:39 [INFO]: Test set missing rate: 30.02%
✅ Dataset 'solar_alabama' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-23 17:28:39 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:28:39 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:28:40 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:28:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:28:40 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:28:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:28:41 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:28:41 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:29:01 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): solar_alabama | mr0.3_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | solar_alabama | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=137)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
🚀 Running imputation pipeline for model: timellm
2025-08-23 17:29:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:29:09 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-23 17:29:09 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-23 17:29:09 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-23 17:29:09 [INFO]: Loaded successfully!
2025-08-23 17:29:09 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-23 17:29:09 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-23 17:29:09 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-23 17:29:09 [INFO]: Total sample number: 547
2025-08-23 17:29:09 [INFO]: Training set size: 271 (49.54%)
2025-08-23 17:29:09 [INFO]: Validation set size: 138 (25.23%)
2025-08-23 17:29:09 [INFO]: Test set size: 138 (25.23%)
2025-08-23 17:29:09 [INFO]: Number of steps: 96
2025-08-23 17:29:09 [INFO]: Number of features: 137
2025-08-23 17:29:09 [INFO]: Train set missing rate: 40.00%
2025-08-23 17:29:09 [INFO]: Validating set missing rate: 39.97%
2025-08-23 17:29:09 [INFO]: Test set missing rate: 40.00%
✅ Dataset 'solar_alabama' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-23 17:29:09 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:29:09 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:29:09 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:29:09 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:29:09 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:29:09 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:29:11 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:29:11 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:29:31 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): solar_alabama | mr0.4_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
🔥 RUN: timellm | solar_alabama | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse (DEVICE=cuda:3 cuda:4 cuda:5, N_STEPS=96, N_FEATURES=137)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-23 17:29:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
🚀 Running imputation pipeline for model: timellm
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-23 17:29:39 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-23 17:29:39 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-23 17:29:39 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-23 17:29:39 [INFO]: Loaded successfully!
2025-08-23 17:29:39 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-23 17:29:39 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-23 17:29:39 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-23 17:29:39 [INFO]: Total sample number: 547
2025-08-23 17:29:39 [INFO]: Training set size: 271 (49.54%)
2025-08-23 17:29:39 [INFO]: Validation set size: 138 (25.23%)
2025-08-23 17:29:39 [INFO]: Test set size: 138 (25.23%)
2025-08-23 17:29:39 [INFO]: Number of steps: 96
2025-08-23 17:29:39 [INFO]: Number of features: 137
2025-08-23 17:29:39 [INFO]: Train set missing rate: 49.99%
2025-08-23 17:29:39 [INFO]: Validating set missing rate: 50.03%
2025-08-23 17:29:39 [INFO]: Test set missing rate: 50.02%
✅ Dataset 'solar_alabama' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-23 17:29:39 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using device list: ['cuda:3', 'cuda:4', 'cuda:5']
2025-08-23 17:29:39 [INFO]: 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥Using the given device: [device(type='cuda', index=3), device(type='cuda', index=4), device(type='cuda', index=5)]
2025-08-23 17:29:40 [INFO]: Model files will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
2025-08-23 17:29:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/timellm/BERT/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse/tensorboard
2025-08-23 17:29:40 [INFO]: Using customized MAE as the training loss function.
2025-08-23 17:29:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-23 17:29:41 [INFO]: Model placed on CUDA devices [3, 4, 5]
2025-08-23 17:29:41 [INFO]: TimeLLM initialized with the given hyperparameters.
 ├─ Total parameters: 96,982,896
 ├─ Trainable parameters: 50,509,680
 └─ Trainable ratio: 52.08%
2025-08-23 17:30:02 [ERROR]: ❌ Exception: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 126, in parallel_apply
    output.reraise()
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/_utils.py", line 715, in reraise
    raise exception
torch.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 3.
Original Traceback (most recent call last):
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 96, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/core.py", line 73, in forward
    reconstruction = self.backbone(X, missing_mask)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/timellm/backbone.py", line 219, in forward
    dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 917, in forward
    outputs = block(
              ^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_output, self_attn_weights = self.attn(
                                     ^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 336, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 114, in eager_attention_forward
    attn_weights = torch.matmul(query, key.transpose(-1, -2))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.37 GiB. GPU 3 has a total capacity of 23.58 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 15.99 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 108.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/timellm.py", line 65, in train_and_evaluate_timellm
    timellm.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/timellm/model.py", line 287, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
❌ FAIL(1): solar_alabama | mr0.5_bs32_dm64_ffn64_h1_ly1_dllm768_ps12_st6_dp0.1_mit1.0_ort0.1_proffalse
✅ All timellm runs completed at 2025-08-23T17:30:04+09:00.
