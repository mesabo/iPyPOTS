nohup: ignoring input
ai-gpgpu14
🚀 SAITS | dataset=physionet_2012 | mr=0.1 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:20:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:20:14 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-15 03:20:14 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-15 03:20:14 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-15 03:20:15 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-15 03:20:40 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-15 03:20:40 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-15 03:20:40 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-15 03:20:40 [INFO]: Total sample number: 3997
2025-08-15 03:20:40 [INFO]: Training set size: 2557 (63.97%)
2025-08-15 03:20:40 [INFO]: Validation set size: 640 (16.01%)
2025-08-15 03:20:40 [INFO]: Test set size: 800 (20.02%)
2025-08-15 03:20:40 [INFO]: Number of steps: 48
2025-08-15 03:20:40 [INFO]: Number of features: 37
2025-08-15 03:20:40 [INFO]: Train set missing rate: 79.68%
2025-08-15 03:20:41 [INFO]: Validating set missing rate: 81.65%
2025-08-15 03:20:41 [INFO]: Test set missing rate: 81.96%
2025-08-15 03:20:41 [INFO]: Using the given device: cpu
2025-08-15 03:20:41 [INFO]: Model files will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:20:41 [INFO]: Tensorboard file will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:20:41 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:20:41 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:20:41 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:20:41 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=physionet_2012 | mr=0.2 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:21:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:21:09 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-15 03:21:09 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-15 03:21:09 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-15 03:21:10 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-15 03:21:38 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-15 03:21:39 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-15 03:21:39 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-15 03:21:39 [INFO]: Total sample number: 3997
2025-08-15 03:21:39 [INFO]: Training set size: 2557 (63.97%)
2025-08-15 03:21:39 [INFO]: Validation set size: 640 (16.01%)
2025-08-15 03:21:39 [INFO]: Test set size: 800 (20.02%)
2025-08-15 03:21:39 [INFO]: Number of steps: 48
2025-08-15 03:21:39 [INFO]: Number of features: 37
2025-08-15 03:21:39 [INFO]: Train set missing rate: 79.68%
2025-08-15 03:21:39 [INFO]: Validating set missing rate: 83.68%
2025-08-15 03:21:39 [INFO]: Test set missing rate: 83.99%
2025-08-15 03:21:39 [INFO]: Using the given device: cpu
2025-08-15 03:21:39 [INFO]: Model files will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:21:39 [INFO]: Tensorboard file will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:21:39 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:21:39 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:21:39 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:21:39 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=physionet_2012 | mr=0.3 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:21:58 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:21:58 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-15 03:21:58 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-15 03:21:58 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-15 03:21:58 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-15 03:22:18 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-15 03:22:19 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-15 03:22:19 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-15 03:22:19 [INFO]: Total sample number: 3997
2025-08-15 03:22:19 [INFO]: Training set size: 2557 (63.97%)
2025-08-15 03:22:19 [INFO]: Validation set size: 640 (16.01%)
2025-08-15 03:22:19 [INFO]: Test set size: 800 (20.02%)
2025-08-15 03:22:19 [INFO]: Number of steps: 48
2025-08-15 03:22:19 [INFO]: Number of features: 37
2025-08-15 03:22:19 [INFO]: Train set missing rate: 79.68%
2025-08-15 03:22:19 [INFO]: Validating set missing rate: 85.72%
2025-08-15 03:22:19 [INFO]: Test set missing rate: 85.98%
2025-08-15 03:22:19 [INFO]: Using the given device: cpu
2025-08-15 03:22:19 [INFO]: Model files will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:22:19 [INFO]: Tensorboard file will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:22:19 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:22:19 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:22:19 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:22:19 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=physionet_2012 | mr=0.4 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:22:37 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:22:37 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-15 03:22:37 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-15 03:22:37 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-15 03:22:37 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-15 03:23:00 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-15 03:23:00 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-15 03:23:00 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-15 03:23:00 [INFO]: Total sample number: 3997
2025-08-15 03:23:00 [INFO]: Training set size: 2557 (63.97%)
2025-08-15 03:23:00 [INFO]: Validation set size: 640 (16.01%)
2025-08-15 03:23:00 [INFO]: Test set size: 800 (20.02%)
2025-08-15 03:23:00 [INFO]: Number of steps: 48
2025-08-15 03:23:00 [INFO]: Number of features: 37
2025-08-15 03:23:00 [INFO]: Train set missing rate: 79.68%
2025-08-15 03:23:00 [INFO]: Validating set missing rate: 87.77%
2025-08-15 03:23:00 [INFO]: Test set missing rate: 87.98%
2025-08-15 03:23:00 [INFO]: Using the given device: cpu
2025-08-15 03:23:00 [INFO]: Model files will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:23:00 [INFO]: Tensorboard file will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:23:00 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:23:00 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:23:00 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:23:00 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=physionet_2012 | mr=0.5 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:23:21 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:23:22 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-15 03:23:22 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-15 03:23:22 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-15 03:23:22 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-15 03:23:40 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-15 03:23:40 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-15 03:23:40 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-15 03:23:40 [INFO]: Total sample number: 3997
2025-08-15 03:23:40 [INFO]: Training set size: 2557 (63.97%)
2025-08-15 03:23:40 [INFO]: Validation set size: 640 (16.01%)
2025-08-15 03:23:40 [INFO]: Test set size: 800 (20.02%)
2025-08-15 03:23:40 [INFO]: Number of steps: 48
2025-08-15 03:23:40 [INFO]: Number of features: 37
2025-08-15 03:23:40 [INFO]: Train set missing rate: 79.68%
2025-08-15 03:23:40 [INFO]: Validating set missing rate: 89.79%
2025-08-15 03:23:40 [INFO]: Test set missing rate: 89.99%
2025-08-15 03:23:40 [INFO]: Using the given device: cpu
2025-08-15 03:23:40 [INFO]: Model files will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:23:40 [INFO]: Tensorboard file will be saved to output/imputation/saits/physionet_2012/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:23:40 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:23:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:23:40 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:23:40 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=ettm1 | mr=0.1 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:23:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:23:57 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:23:57 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:23:57 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:23:57 [INFO]: Loaded successfully!
2025-08-15 03:23:57 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:23:57 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:23:57 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:23:58 [INFO]: Total sample number: 725
2025-08-15 03:23:58 [INFO]: Training set size: 427 (58.90%)
2025-08-15 03:23:58 [INFO]: Validation set size: 153 (21.10%)
2025-08-15 03:23:58 [INFO]: Test set size: 145 (20.00%)
2025-08-15 03:23:58 [INFO]: Number of steps: 96
2025-08-15 03:23:58 [INFO]: Number of features: 7
2025-08-15 03:23:58 [INFO]: Train set missing rate: 10.00%
2025-08-15 03:23:58 [INFO]: Validating set missing rate: 10.02%
2025-08-15 03:23:58 [INFO]: Test set missing rate: 9.87%
2025-08-15 03:23:58 [INFO]: Using the given device: cpu
2025-08-15 03:23:58 [INFO]: Model files will be saved to output/imputation/saits/ettm1/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:23:58 [INFO]: Tensorboard file will be saved to output/imputation/saits/ettm1/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:23:58 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:23:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:23:58 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:23:58 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'ettm1' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=ettm1 | mr=0.2 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:24:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:24:26 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:24:26 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:24:26 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:24:26 [INFO]: Loaded successfully!
2025-08-15 03:24:26 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:24:26 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:24:26 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:24:26 [INFO]: Total sample number: 725
2025-08-15 03:24:26 [INFO]: Training set size: 427 (58.90%)
2025-08-15 03:24:26 [INFO]: Validation set size: 153 (21.10%)
2025-08-15 03:24:26 [INFO]: Test set size: 145 (20.00%)
2025-08-15 03:24:26 [INFO]: Number of steps: 96
2025-08-15 03:24:26 [INFO]: Number of features: 7
2025-08-15 03:24:26 [INFO]: Train set missing rate: 20.03%
2025-08-15 03:24:26 [INFO]: Validating set missing rate: 19.99%
2025-08-15 03:24:26 [INFO]: Test set missing rate: 20.13%
2025-08-15 03:24:26 [INFO]: Using the given device: cpu
2025-08-15 03:24:26 [INFO]: Model files will be saved to output/imputation/saits/ettm1/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:24:26 [INFO]: Tensorboard file will be saved to output/imputation/saits/ettm1/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:24:26 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:24:26 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:24:26 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:24:26 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'ettm1' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=ettm1 | mr=0.3 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:24:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:24:54 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:24:54 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:24:54 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:24:54 [INFO]: Loaded successfully!
2025-08-15 03:24:55 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:24:55 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:24:55 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:24:55 [INFO]: Total sample number: 725
2025-08-15 03:24:55 [INFO]: Training set size: 427 (58.90%)
2025-08-15 03:24:55 [INFO]: Validation set size: 153 (21.10%)
2025-08-15 03:24:55 [INFO]: Test set size: 145 (20.00%)
2025-08-15 03:24:55 [INFO]: Number of steps: 96
2025-08-15 03:24:55 [INFO]: Number of features: 7
2025-08-15 03:24:55 [INFO]: Train set missing rate: 30.13%
2025-08-15 03:24:55 [INFO]: Validating set missing rate: 29.97%
2025-08-15 03:24:55 [INFO]: Test set missing rate: 30.20%
2025-08-15 03:24:55 [INFO]: Using the given device: cpu
2025-08-15 03:24:55 [INFO]: Model files will be saved to output/imputation/saits/ettm1/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:24:55 [INFO]: Tensorboard file will be saved to output/imputation/saits/ettm1/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:24:55 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:24:55 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:24:55 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:24:55 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'ettm1' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=ettm1 | mr=0.4 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:25:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:25:22 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:25:22 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:25:22 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:25:22 [INFO]: Loaded successfully!
2025-08-15 03:25:22 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:25:22 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:25:22 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:25:22 [INFO]: Total sample number: 725
2025-08-15 03:25:22 [INFO]: Training set size: 427 (58.90%)
2025-08-15 03:25:22 [INFO]: Validation set size: 153 (21.10%)
2025-08-15 03:25:22 [INFO]: Test set size: 145 (20.00%)
2025-08-15 03:25:22 [INFO]: Number of steps: 96
2025-08-15 03:25:22 [INFO]: Number of features: 7
2025-08-15 03:25:22 [INFO]: Train set missing rate: 40.11%
2025-08-15 03:25:22 [INFO]: Validating set missing rate: 39.95%
2025-08-15 03:25:22 [INFO]: Test set missing rate: 39.98%
2025-08-15 03:25:22 [INFO]: Using the given device: cpu
2025-08-15 03:25:22 [INFO]: Model files will be saved to output/imputation/saits/ettm1/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:25:22 [INFO]: Tensorboard file will be saved to output/imputation/saits/ettm1/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:25:22 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:25:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:25:22 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:25:22 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'ettm1' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=ettm1 | mr=0.5 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:25:45 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:25:45 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:25:45 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:25:45 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:25:45 [INFO]: Loaded successfully!
2025-08-15 03:25:45 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:25:45 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:25:45 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:25:45 [INFO]: Total sample number: 725
2025-08-15 03:25:45 [INFO]: Training set size: 427 (58.90%)
2025-08-15 03:25:45 [INFO]: Validation set size: 153 (21.10%)
2025-08-15 03:25:45 [INFO]: Test set size: 145 (20.00%)
2025-08-15 03:25:45 [INFO]: Number of steps: 96
2025-08-15 03:25:45 [INFO]: Number of features: 7
2025-08-15 03:25:45 [INFO]: Train set missing rate: 50.15%
2025-08-15 03:25:45 [INFO]: Validating set missing rate: 49.83%
2025-08-15 03:25:45 [INFO]: Test set missing rate: 49.84%
2025-08-15 03:25:45 [INFO]: Using the given device: cpu
2025-08-15 03:25:45 [INFO]: Model files will be saved to output/imputation/saits/ettm1/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:25:45 [INFO]: Tensorboard file will be saved to output/imputation/saits/ettm1/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:25:45 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:25:45 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:25:45 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:25:45 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'ettm1' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=etth1 | mr=0.1 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:26:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:26:09 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:26:09 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:26:09 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:26:09 [INFO]: Loaded successfully!
2025-08-15 03:26:09 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:26:09 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:26:09 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:26:09 [INFO]: Total sample number: 180
2025-08-15 03:26:09 [INFO]: Training set size: 106 (58.89%)
2025-08-15 03:26:09 [INFO]: Validation set size: 38 (21.11%)
2025-08-15 03:26:09 [INFO]: Test set size: 36 (20.00%)
2025-08-15 03:26:09 [INFO]: Number of steps: 96
2025-08-15 03:26:09 [INFO]: Number of features: 7
2025-08-15 03:26:09 [INFO]: Train set missing rate: 9.86%
2025-08-15 03:26:09 [INFO]: Validating set missing rate: 9.84%
2025-08-15 03:26:09 [INFO]: Test set missing rate: 9.94%
2025-08-15 03:26:09 [INFO]: Using the given device: cpu
2025-08-15 03:26:09 [INFO]: Model files will be saved to output/imputation/saits/etth1/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:26:09 [INFO]: Tensorboard file will be saved to output/imputation/saits/etth1/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:26:09 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:26:09 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:26:09 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:26:09 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'etth1' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=etth1 | mr=0.2 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:26:38 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:26:38 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:26:38 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:26:38 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:26:38 [INFO]: Loaded successfully!
2025-08-15 03:26:38 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:26:38 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:26:38 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:26:38 [INFO]: Total sample number: 180
2025-08-15 03:26:38 [INFO]: Training set size: 106 (58.89%)
2025-08-15 03:26:38 [INFO]: Validation set size: 38 (21.11%)
2025-08-15 03:26:38 [INFO]: Test set size: 36 (20.00%)
2025-08-15 03:26:38 [INFO]: Number of steps: 96
2025-08-15 03:26:38 [INFO]: Number of features: 7
2025-08-15 03:26:38 [INFO]: Train set missing rate: 19.76%
2025-08-15 03:26:38 [INFO]: Validating set missing rate: 19.91%
2025-08-15 03:26:38 [INFO]: Test set missing rate: 19.98%
2025-08-15 03:26:38 [INFO]: Using the given device: cpu
2025-08-15 03:26:38 [INFO]: Model files will be saved to output/imputation/saits/etth1/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:26:38 [INFO]: Tensorboard file will be saved to output/imputation/saits/etth1/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:26:38 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:26:38 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:26:38 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:26:38 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'etth1' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=etth1 | mr=0.3 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:27:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:27:03 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:27:03 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:27:03 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:27:03 [INFO]: Loaded successfully!
2025-08-15 03:27:03 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:27:03 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:27:03 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:27:03 [INFO]: Total sample number: 180
2025-08-15 03:27:03 [INFO]: Training set size: 106 (58.89%)
2025-08-15 03:27:03 [INFO]: Validation set size: 38 (21.11%)
2025-08-15 03:27:03 [INFO]: Test set size: 36 (20.00%)
2025-08-15 03:27:03 [INFO]: Number of steps: 96
2025-08-15 03:27:03 [INFO]: Number of features: 7
2025-08-15 03:27:03 [INFO]: Train set missing rate: 29.75%
2025-08-15 03:27:03 [INFO]: Validating set missing rate: 29.99%
2025-08-15 03:27:03 [INFO]: Test set missing rate: 30.17%
2025-08-15 03:27:03 [INFO]: Using the given device: cpu
2025-08-15 03:27:03 [INFO]: Model files will be saved to output/imputation/saits/etth1/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:27:03 [INFO]: Tensorboard file will be saved to output/imputation/saits/etth1/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:27:03 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:27:03 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:27:03 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:27:03 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'etth1' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=etth1 | mr=0.4 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:27:19 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:27:19 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:27:19 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:27:19 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:27:19 [INFO]: Loaded successfully!
2025-08-15 03:27:19 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:27:19 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:27:19 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:27:19 [INFO]: Total sample number: 180
2025-08-15 03:27:19 [INFO]: Training set size: 106 (58.89%)
2025-08-15 03:27:19 [INFO]: Validation set size: 38 (21.11%)
2025-08-15 03:27:19 [INFO]: Test set size: 36 (20.00%)
2025-08-15 03:27:19 [INFO]: Number of steps: 96
2025-08-15 03:27:19 [INFO]: Number of features: 7
2025-08-15 03:27:19 [INFO]: Train set missing rate: 39.82%
2025-08-15 03:27:19 [INFO]: Validating set missing rate: 39.92%
2025-08-15 03:27:19 [INFO]: Test set missing rate: 40.57%
2025-08-15 03:27:19 [INFO]: Using the given device: cpu
2025-08-15 03:27:19 [INFO]: Model files will be saved to output/imputation/saits/etth1/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:27:19 [INFO]: Tensorboard file will be saved to output/imputation/saits/etth1/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:27:19 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:27:19 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:27:19 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:27:19 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'etth1' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=etth1 | mr=0.5 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:27:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:27:43 [INFO]: You're using dataset electricity_transformer_temperature, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_transformer_temperature
2025-08-15 03:27:43 [INFO]: Dataset electricity_transformer_temperature has already been downloaded. Processing directly...
2025-08-15 03:27:43 [INFO]: Dataset electricity_transformer_temperature has already been cached. Loading from cache directly...
2025-08-15 03:27:43 [INFO]: Loaded successfully!
2025-08-15 03:27:43 [INFO]: months selected as train set are <PeriodArray>
['2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01',
 '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08']
Length: 14, dtype: period[M]
2025-08-15 03:27:43 [INFO]: months selected as val set are <PeriodArray>
['2017-09', '2017-10', '2017-11', '2017-12', '2018-01']
Length: 5, dtype: period[M]
2025-08-15 03:27:43 [INFO]: months selected as test set are <PeriodArray>
['2018-02', '2018-03', '2018-04', '2018-05', '2018-06']
Length: 5, dtype: period[M]
2025-08-15 03:27:43 [INFO]: Total sample number: 180
2025-08-15 03:27:43 [INFO]: Training set size: 106 (58.89%)
2025-08-15 03:27:43 [INFO]: Validation set size: 38 (21.11%)
2025-08-15 03:27:43 [INFO]: Test set size: 36 (20.00%)
2025-08-15 03:27:43 [INFO]: Number of steps: 96
2025-08-15 03:27:43 [INFO]: Number of features: 7
2025-08-15 03:27:43 [INFO]: Train set missing rate: 50.08%
2025-08-15 03:27:43 [INFO]: Validating set missing rate: 50.08%
2025-08-15 03:27:43 [INFO]: Test set missing rate: 50.20%
2025-08-15 03:27:43 [INFO]: Using the given device: cpu
2025-08-15 03:27:43 [INFO]: Model files will be saved to output/imputation/saits/etth1/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:27:43 [INFO]: Tensorboard file will be saved to output/imputation/saits/etth1/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:27:43 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:27:43 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:27:43 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:27:43 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'etth1' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [96, 31].  Tensor sizes: [96, 32]
🚀 SAITS | dataset=italy_air_quality | mr=0.1 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:28:07 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:28:07 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-15 03:28:07 [INFO]: Start downloading...
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
Downloading air+quality.zip: 0.00B [00:00, ?B/s]Downloading air+quality.zip: 11.9kB [00:00, 122kB/s]Downloading air+quality.zip: 39.9kB [00:00, 207kB/s]Downloading air+quality.zip: 95.9kB [00:00, 354kB/s]Downloading air+quality.zip: 208kB [00:00, 629kB/s] Downloading air+quality.zip: 436kB [00:00, 1.17MB/s]Downloading air+quality.zip: 876kB [00:00, 2.17MB/s]Downloading air+quality.zip: 1.22MB [00:00, 2.55MB/s]Downloading air+quality.zip: 1.47MB [00:00, 2.01MB/s]
2025-08-15 03:28:08 [INFO]: Successfully downloaded data to /tmp/tmpmuhrodbu/air+quality.zip
2025-08-15 03:28:08 [INFO]: Successfully extracted data to /home/23r8105_messou/.pypots/tsdb/italy_air_quality
2025-08-15 03:28:09 [INFO]: Successfully saved to /home/23r8105_messou/.pypots/tsdb/italy_air_quality/italy_air_quality_cache.pkl
2025-08-15 03:28:09 [INFO]: Loaded successfully!
2025-08-15 03:28:09 [INFO]: Total sample number: 192
2025-08-15 03:28:09 [INFO]: Training set size: 116 (60.42%)
2025-08-15 03:28:09 [INFO]: Validation set size: 38 (19.79%)
2025-08-15 03:28:09 [INFO]: Test set size: 38 (19.79%)
2025-08-15 03:28:09 [INFO]: Number of steps: 48
2025-08-15 03:28:09 [INFO]: Number of features: 13
2025-08-15 03:28:09 [INFO]: Train set missing rate: 9.87%
2025-08-15 03:28:09 [INFO]: Validating set missing rate: 9.86%
2025-08-15 03:28:09 [INFO]: Test set missing rate: 9.87%
2025-08-15 03:28:09 [INFO]: Using the given device: cpu
2025-08-15 03:28:09 [INFO]: Model files will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:28:09 [INFO]: Tensorboard file will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.1_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:28:09 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:28:09 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:28:09 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:28:09 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
✅ Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=italy_air_quality | mr=0.2 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:28:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:28:34 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-15 03:28:34 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-15 03:28:34 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-15 03:28:34 [INFO]: Loaded successfully!
2025-08-15 03:28:34 [INFO]: Total sample number: 192
2025-08-15 03:28:34 [INFO]: Training set size: 116 (60.42%)
2025-08-15 03:28:34 [INFO]: Validation set size: 38 (19.79%)
2025-08-15 03:28:34 [INFO]: Test set size: 38 (19.79%)
2025-08-15 03:28:34 [INFO]: Number of steps: 48
2025-08-15 03:28:34 [INFO]: Number of features: 13
2025-08-15 03:28:34 [INFO]: Train set missing rate: 19.79%
2025-08-15 03:28:34 [INFO]: Validating set missing rate: 19.86%
2025-08-15 03:28:34 [INFO]: Test set missing rate: 19.88%
2025-08-15 03:28:34 [INFO]: Using the given device: cpu
2025-08-15 03:28:34 [INFO]: Model files will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:28:34 [INFO]: Tensorboard file will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.2_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:28:34 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:28:34 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:28:34 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:28:34 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=italy_air_quality | mr=0.3 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:28:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:28:57 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-15 03:28:57 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-15 03:28:57 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-15 03:28:57 [INFO]: Loaded successfully!
2025-08-15 03:28:57 [INFO]: Total sample number: 192
2025-08-15 03:28:57 [INFO]: Training set size: 116 (60.42%)
2025-08-15 03:28:57 [INFO]: Validation set size: 38 (19.79%)
2025-08-15 03:28:57 [INFO]: Test set size: 38 (19.79%)
2025-08-15 03:28:57 [INFO]: Number of steps: 48
2025-08-15 03:28:57 [INFO]: Number of features: 13
2025-08-15 03:28:57 [INFO]: Train set missing rate: 29.80%
2025-08-15 03:28:57 [INFO]: Validating set missing rate: 29.90%
2025-08-15 03:28:57 [INFO]: Test set missing rate: 30.05%
2025-08-15 03:28:57 [INFO]: Using the given device: cpu
2025-08-15 03:28:57 [INFO]: Model files will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:28:57 [INFO]: Tensorboard file will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.3_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:28:57 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:28:57 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:28:57 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:28:57 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=italy_air_quality | mr=0.4 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:29:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:29:18 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-15 03:29:18 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-15 03:29:18 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-15 03:29:18 [INFO]: Loaded successfully!
2025-08-15 03:29:18 [INFO]: Total sample number: 192
2025-08-15 03:29:18 [INFO]: Training set size: 116 (60.42%)
2025-08-15 03:29:18 [INFO]: Validation set size: 38 (19.79%)
2025-08-15 03:29:18 [INFO]: Test set size: 38 (19.79%)
2025-08-15 03:29:18 [INFO]: Number of steps: 48
2025-08-15 03:29:18 [INFO]: Number of features: 13
2025-08-15 03:29:18 [INFO]: Train set missing rate: 39.84%
2025-08-15 03:29:18 [INFO]: Validating set missing rate: 39.87%
2025-08-15 03:29:18 [INFO]: Test set missing rate: 40.52%
2025-08-15 03:29:18 [INFO]: Using the given device: cpu
2025-08-15 03:29:18 [INFO]: Model files will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:29:18 [INFO]: Tensorboard file will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.4_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:29:18 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:29:18 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:29:18 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:29:18 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
🚀 SAITS | dataset=italy_air_quality | mr=0.5 | bs=32 | d_model=64 | d_ffn=128 | heads=3 | layers=2 | d_k=21 | d_v=21 | profiling=true
2025-08-15 03:29:45 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-15 03:29:45 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-15 03:29:45 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-15 03:29:45 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-15 03:29:45 [INFO]: Loaded successfully!
2025-08-15 03:29:45 [INFO]: Total sample number: 192
2025-08-15 03:29:45 [INFO]: Training set size: 116 (60.42%)
2025-08-15 03:29:45 [INFO]: Validation set size: 38 (19.79%)
2025-08-15 03:29:45 [INFO]: Test set size: 38 (19.79%)
2025-08-15 03:29:45 [INFO]: Number of steps: 48
2025-08-15 03:29:45 [INFO]: Number of features: 13
2025-08-15 03:29:45 [INFO]: Train set missing rate: 50.11%
2025-08-15 03:29:45 [INFO]: Validating set missing rate: 50.03%
2025-08-15 03:29:45 [INFO]: Test set missing rate: 50.12%
2025-08-15 03:29:45 [INFO]: Using the given device: cpu
2025-08-15 03:29:45 [INFO]: Model files will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue
2025-08-15 03:29:45 [INFO]: Tensorboard file will be saved to output/imputation/saits/italy_air_quality/epoch10/mr0.5_bs32_dm64_ffn128_h3_ly2_proftrue/tensorboard
2025-08-15 03:29:45 [INFO]: Using customized MAE as the training loss function.
2025-08-15 03:29:45 [INFO]: Using customized MSE as the validation metric function.
2025-08-15 03:29:45 [WARNING]: ‼️ d_model must = n_heads * d_k, it should be divisible by n_heads and the result should be equal to d_k, but got d_model=64, n_heads=3, d_k=21
2025-08-15 03:29:45 [WARNING]: ⚠️ d_model is reset to 63 = n_heads (3) * d_k (21)
[34m
████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗
╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║
   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║
   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║
   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║
   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: True
🚀 Running imputation pipeline for model: saits
ℹ️ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
✅ Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
🚀 Starting SAITS training pipeline...
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/saits.py", line 38, in train_and_evaluate_saits
    saits = SAITS(
            ^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/model.py", line 190, in __init__
    self.model = _SAITS(
                 ^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/saits/core.py", line 51, in __init__
    self.encoder = BackboneSAITS(
                   ^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/backbone.py", line 41, in __init__
    self.embedding_1 = SaitsEmbedding(
                       ^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/saits/embedding.py", line 50, in __init__
    self.position_enc = PositionalEncoding(d_out, n_positions=n_max_steps) if with_pos else None
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/transformer/embedding.py", line 39, in __init__
    pe[:, 1::2] = torch.cos(position * div_term)
    ~~^^^^^^^^^
RuntimeError: The expanded size of the tensor (31) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [48, 31].  Tensor sizes: [48, 32]
✅ All SAITS runs completed.
