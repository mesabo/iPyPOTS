ai-gpgpu14
Visible GPUs (physical): 0,1,2,3,4,5,6,7
Using devices (logical): cuda:0
Session log: output/imputation/cuda/session_llm4imp_20250824T015507.log
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 01:55:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 01:55:41 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 01:55:41 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 01:55:41 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 01:55:41 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 01:55:51 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 01:55:51 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-24 01:55:51 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-24 01:55:51 [INFO]: Total sample number: 3997
2025-08-24 01:55:51 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 01:55:51 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 01:55:51 [INFO]: Test set size: 800 (20.02%)
2025-08-24 01:55:51 [INFO]: Number of steps: 48
2025-08-24 01:55:51 [INFO]: Number of features: 37
2025-08-24 01:55:51 [INFO]: Train set missing rate: 79.68%
2025-08-24 01:55:51 [INFO]: Validating set missing rate: 81.65%
2025-08-24 01:55:51 [INFO]: Test set missing rate: 81.96%
âœ… Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 01:55:51 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 01:55:51 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 01:55:51 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 01:55:51 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 01:55:51 [INFO]: Using customized MAE as the training loss function.
2025-08-24 01:55:51 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 01:56:02 [INFO]: Model placed on cuda:0
2025-08-24 01:56:02 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 01:56:48 [INFO]: Epoch 001 - training loss (MAE): 0.4789, validation MSE: 0.5448
2025-08-24 01:57:34 [INFO]: Epoch 002 - training loss (MAE): 0.4514, validation MSE: 0.5338
2025-08-24 01:58:18 [INFO]: Epoch 003 - training loss (MAE): 0.4448, validation MSE: 0.5271
2025-08-24 01:59:04 [INFO]: Epoch 004 - training loss (MAE): 0.4431, validation MSE: 0.5240
2025-08-24 01:59:50 [INFO]: Epoch 005 - training loss (MAE): 0.4404, validation MSE: 0.5197
2025-08-24 02:00:36 [INFO]: Epoch 006 - training loss (MAE): 0.4404, validation MSE: 0.5217
2025-08-24 02:01:20 [INFO]: Epoch 007 - training loss (MAE): 0.4371, validation MSE: 0.5198
2025-08-24 02:02:05 [INFO]: Epoch 008 - training loss (MAE): 0.4378, validation MSE: 0.5197
2025-08-24 02:02:50 [INFO]: Epoch 009 - training loss (MAE): 0.4350, validation MSE: 0.5184
2025-08-24 02:03:37 [INFO]: Epoch 010 - training loss (MAE): 0.4376, validation MSE: 0.5147
2025-08-24 02:04:22 [INFO]: Epoch 011 - training loss (MAE): 0.4355, validation MSE: 0.5108
2025-08-24 02:05:09 [INFO]: Epoch 012 - training loss (MAE): 0.4329, validation MSE: 0.5114
2025-08-24 02:05:56 [INFO]: Epoch 013 - training loss (MAE): 0.4323, validation MSE: 0.5099
2025-08-24 02:06:42 [INFO]: Epoch 014 - training loss (MAE): 0.4319, validation MSE: 0.5123
2025-08-24 02:07:29 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: 0.5089
2025-08-24 02:07:29 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 02:07:31 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4188| MSE: 0.4772| RMSE: 0.6908| MRE: 0.5882| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:08:15 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-24 02:08:15 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:08:15 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:08:15 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:08:15 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:08:21 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:08:21 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-24 02:08:21 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-24 02:08:21 [INFO]: Total sample number: 3997
2025-08-24 02:08:21 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:08:21 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:08:21 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:08:21 [INFO]: Number of steps: 48
2025-08-24 02:08:21 [INFO]: Number of features: 37
2025-08-24 02:08:21 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:08:21 [INFO]: Validating set missing rate: 81.65%
2025-08-24 02:08:21 [INFO]: Test set missing rate: 81.96%
âœ… Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 02:08:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:08:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:08:22 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 02:08:22 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 02:08:22 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:08:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:08:34 [INFO]: Model placed on cuda:0
2025-08-24 02:08:34 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:08:36 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5408
2025-08-24 02:08:38 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5252
2025-08-24 02:08:40 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5183
2025-08-24 02:08:41 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5147
2025-08-24 02:08:42 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5130
2025-08-24 02:08:43 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5111
2025-08-24 02:08:43 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5133
2025-08-24 02:08:44 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5146
2025-08-24 02:08:45 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5128
2025-08-24 02:08:46 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5145
2025-08-24 02:08:47 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5131
2025-08-24 02:08:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-24 02:08:47 [INFO]: Finished training. The best model is from epoch#6.
2025-08-24 02:08:48 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4195| MSE: 0.4830| RMSE: 0.6950| MRE: 0.5892| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:09:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:09:22 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:09:22 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:09:22 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:09:22 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:09:30 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:09:30 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-24 02:09:30 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-24 02:09:30 [INFO]: Total sample number: 3997
2025-08-24 02:09:30 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:09:30 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:09:30 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:09:30 [INFO]: Number of steps: 48
2025-08-24 02:09:30 [INFO]: Number of features: 37
2025-08-24 02:09:30 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:09:30 [INFO]: Validating set missing rate: 81.65%
2025-08-24 02:09:30 [INFO]: Test set missing rate: 81.96%
âœ… Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 02:09:30 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:09:30 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:09:31 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 02:09:31 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 02:09:31 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:09:31 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:09:38 [INFO]: Model placed on cuda:0
2025-08-24 02:09:38 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:10:19 [INFO]: Epoch 001 - training loss (MAE): 0.4830, validation MSE: 0.5455
2025-08-24 02:10:59 [INFO]: Epoch 002 - training loss (MAE): 0.4528, validation MSE: 0.5333
2025-08-24 02:11:41 [INFO]: Epoch 003 - training loss (MAE): 0.4451, validation MSE: 0.5248
2025-08-24 02:12:20 [INFO]: Epoch 004 - training loss (MAE): 0.4424, validation MSE: 0.5212
2025-08-24 02:13:00 [INFO]: Epoch 005 - training loss (MAE): 0.4394, validation MSE: 0.5161
2025-08-24 02:13:41 [INFO]: Epoch 006 - training loss (MAE): 0.4393, validation MSE: 0.5168
2025-08-24 02:14:19 [INFO]: Epoch 007 - training loss (MAE): 0.4362, validation MSE: 0.5205
2025-08-24 02:14:58 [INFO]: Epoch 008 - training loss (MAE): 0.4367, validation MSE: 0.5168
2025-08-24 02:15:38 [INFO]: Epoch 009 - training loss (MAE): 0.4337, validation MSE: 0.5188
2025-08-24 02:16:15 [INFO]: Epoch 010 - training loss (MAE): 0.4369, validation MSE: 0.5146
2025-08-24 02:16:50 [INFO]: Epoch 011 - training loss (MAE): 0.4342, validation MSE: 0.5130
2025-08-24 02:17:25 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5125
2025-08-24 02:17:59 [INFO]: Epoch 013 - training loss (MAE): 0.4314, validation MSE: 0.5108
2025-08-24 02:18:34 [INFO]: Epoch 014 - training loss (MAE): 0.4306, validation MSE: 0.5133
2025-08-24 02:19:09 [INFO]: Epoch 015 - training loss (MAE): 0.4308, validation MSE: 0.5095
2025-08-24 02:19:09 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 02:19:11 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4178| MSE: 0.4754| RMSE: 0.6895| MRE: 0.5867| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:20:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-24 02:20:03 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:20:03 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:20:03 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:20:03 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:20:09 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:20:09 [INFO]: 22947 values masked out in the val set as ground truth, take 9.91% of the original observed values
2025-08-24 02:20:09 [INFO]: 28724 values masked out in the test set as ground truth, take 10.08% of the original observed values
2025-08-24 02:20:09 [INFO]: Total sample number: 3997
2025-08-24 02:20:09 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:20:09 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:20:09 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:20:09 [INFO]: Number of steps: 48
2025-08-24 02:20:09 [INFO]: Number of features: 37
2025-08-24 02:20:09 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:20:09 [INFO]: Validating set missing rate: 81.65%
2025-08-24 02:20:09 [INFO]: Test set missing rate: 81.96%
âœ… Dataset 'physionet_2012' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 02:20:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:20:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:20:10 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 02:20:10 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 02:20:10 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:20:10 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:20:22 [INFO]: Model placed on cuda:0
2025-08-24 02:20:22 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:20:23 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5408
2025-08-24 02:20:24 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5252
2025-08-24 02:20:25 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5183
2025-08-24 02:20:26 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5147
2025-08-24 02:20:29 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5130
2025-08-24 02:20:30 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5111
2025-08-24 02:20:31 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5133
2025-08-24 02:20:32 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5146
2025-08-24 02:20:34 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5128
2025-08-24 02:20:35 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5145
2025-08-24 02:20:37 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5131
2025-08-24 02:20:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-24 02:20:37 [INFO]: Finished training. The best model is from epoch#6.
2025-08-24 02:20:38 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4195| MSE: 0.4830| RMSE: 0.6950| MRE: 0.5892| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:21:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:21:12 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:21:12 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:21:12 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:21:13 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:21:20 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:21:20 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-24 02:21:20 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-24 02:21:20 [INFO]: Total sample number: 3997
2025-08-24 02:21:20 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:21:20 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:21:20 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:21:20 [INFO]: Number of steps: 48
2025-08-24 02:21:20 [INFO]: Number of features: 37
2025-08-24 02:21:20 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:21:20 [INFO]: Validating set missing rate: 83.68%
2025-08-24 02:21:20 [INFO]: Test set missing rate: 83.99%
âœ… Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 02:21:20 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:21:20 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:21:20 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 02:21:20 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 02:21:20 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:21:20 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:21:29 [INFO]: Model placed on cuda:0
2025-08-24 02:21:29 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:22:16 [INFO]: Epoch 001 - training loss (MAE): 0.4789, validation MSE: 0.5648
2025-08-24 02:23:02 [INFO]: Epoch 002 - training loss (MAE): 0.4514, validation MSE: 0.5521
2025-08-24 02:23:52 [INFO]: Epoch 003 - training loss (MAE): 0.4448, validation MSE: 0.5436
2025-08-24 02:24:40 [INFO]: Epoch 004 - training loss (MAE): 0.4431, validation MSE: 0.5409
2025-08-24 02:25:24 [INFO]: Epoch 005 - training loss (MAE): 0.4404, validation MSE: 0.5354
2025-08-24 02:26:10 [INFO]: Epoch 006 - training loss (MAE): 0.4404, validation MSE: 0.5376
2025-08-24 02:26:56 [INFO]: Epoch 007 - training loss (MAE): 0.4371, validation MSE: 0.5331
2025-08-24 02:27:41 [INFO]: Epoch 008 - training loss (MAE): 0.4378, validation MSE: 0.5339
2025-08-24 02:28:27 [INFO]: Epoch 009 - training loss (MAE): 0.4350, validation MSE: 0.5306
2025-08-24 02:29:12 [INFO]: Epoch 010 - training loss (MAE): 0.4376, validation MSE: 0.5283
2025-08-24 02:29:57 [INFO]: Epoch 011 - training loss (MAE): 0.4355, validation MSE: 0.5242
2025-08-24 02:30:42 [INFO]: Epoch 012 - training loss (MAE): 0.4329, validation MSE: 0.5260
2025-08-24 02:31:27 [INFO]: Epoch 013 - training loss (MAE): 0.4323, validation MSE: 0.5252
2025-08-24 02:32:13 [INFO]: Epoch 014 - training loss (MAE): 0.4319, validation MSE: 0.5246
2025-08-24 02:33:00 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: 0.5221
2025-08-24 02:33:00 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 02:33:02 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4283| MSE: 0.5078| RMSE: 0.7126| MRE: 0.6046| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:33:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:33:55 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:33:55 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:33:55 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:33:55 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:34:04 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:34:04 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-24 02:34:04 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-24 02:34:04 [INFO]: Total sample number: 3997
2025-08-24 02:34:04 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:34:04 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:34:04 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:34:04 [INFO]: Number of steps: 48
2025-08-24 02:34:04 [INFO]: Number of features: 37
2025-08-24 02:34:04 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:34:04 [INFO]: Validating set missing rate: 83.68%
2025-08-24 02:34:04 [INFO]: Test set missing rate: 83.99%
âœ… Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 02:34:04 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:34:04 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:34:05 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 02:34:05 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 02:34:05 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:34:05 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:34:17 [INFO]: Model placed on cuda:0
2025-08-24 02:34:17 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:34:19 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5644
2025-08-24 02:34:20 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5462
2025-08-24 02:34:21 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5372
2025-08-24 02:34:22 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5333
2025-08-24 02:34:23 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5309
2025-08-24 02:34:24 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5288
2025-08-24 02:34:24 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5278
2025-08-24 02:34:25 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5281
2025-08-24 02:34:26 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5273
2025-08-24 02:34:27 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5278
2025-08-24 02:34:28 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5273
2025-08-24 02:34:29 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5253
2025-08-24 02:34:30 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5247
2025-08-24 02:34:32 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5249
2025-08-24 02:34:34 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5254
2025-08-24 02:34:34 [INFO]: Finished training. The best model is from epoch#13.
2025-08-24 02:34:36 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4259| MSE: 0.5101| RMSE: 0.7142| MRE: 0.6013| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:35:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:35:18 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:35:18 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:35:18 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:35:18 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:35:26 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:35:26 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-24 02:35:26 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-24 02:35:26 [INFO]: Total sample number: 3997
2025-08-24 02:35:26 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:35:26 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:35:26 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:35:26 [INFO]: Number of steps: 48
2025-08-24 02:35:26 [INFO]: Number of features: 37
2025-08-24 02:35:26 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:35:26 [INFO]: Validating set missing rate: 83.68%
2025-08-24 02:35:26 [INFO]: Test set missing rate: 83.99%
âœ… Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 02:35:26 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:35:26 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:35:27 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 02:35:27 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 02:35:27 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:35:27 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:35:32 [INFO]: Model placed on cuda:0
2025-08-24 02:35:32 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:36:15 [INFO]: Epoch 001 - training loss (MAE): 0.4830, validation MSE: 0.5673
2025-08-24 02:36:57 [INFO]: Epoch 002 - training loss (MAE): 0.4528, validation MSE: 0.5534
2025-08-24 02:37:39 [INFO]: Epoch 003 - training loss (MAE): 0.4451, validation MSE: 0.5429
2025-08-24 02:38:21 [INFO]: Epoch 004 - training loss (MAE): 0.4424, validation MSE: 0.5390
2025-08-24 02:39:04 [INFO]: Epoch 005 - training loss (MAE): 0.4394, validation MSE: 0.5335
2025-08-24 02:39:46 [INFO]: Epoch 006 - training loss (MAE): 0.4393, validation MSE: 0.5347
2025-08-24 02:40:29 [INFO]: Epoch 007 - training loss (MAE): 0.4362, validation MSE: 0.5344
2025-08-24 02:41:12 [INFO]: Epoch 008 - training loss (MAE): 0.4367, validation MSE: 0.5311
2025-08-24 02:41:54 [INFO]: Epoch 009 - training loss (MAE): 0.4337, validation MSE: 0.5320
2025-08-24 02:42:36 [INFO]: Epoch 010 - training loss (MAE): 0.4369, validation MSE: 0.5271
2025-08-24 02:43:18 [INFO]: Epoch 011 - training loss (MAE): 0.4342, validation MSE: 0.5269
2025-08-24 02:44:01 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5260
2025-08-24 02:44:43 [INFO]: Epoch 013 - training loss (MAE): 0.4314, validation MSE: 0.5247
2025-08-24 02:45:25 [INFO]: Epoch 014 - training loss (MAE): 0.4306, validation MSE: 0.5252
2025-08-24 02:46:08 [INFO]: Epoch 015 - training loss (MAE): 0.4308, validation MSE: 0.5235
2025-08-24 02:46:08 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 02:46:10 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4270| MSE: 0.5071| RMSE: 0.7121| MRE: 0.6029| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:46:29 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2025-08-24 02:46:29 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:46:29 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:46:29 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:46:29 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:46:36 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:46:36 [INFO]: 46041 values masked out in the val set as ground truth, take 19.88% of the original observed values
2025-08-24 02:46:36 [INFO]: 57450 values masked out in the test set as ground truth, take 20.16% of the original observed values
2025-08-24 02:46:36 [INFO]: Total sample number: 3997
2025-08-24 02:46:36 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:46:36 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:46:36 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:46:36 [INFO]: Number of steps: 48
2025-08-24 02:46:36 [INFO]: Number of features: 37
2025-08-24 02:46:36 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:46:36 [INFO]: Validating set missing rate: 83.68%
2025-08-24 02:46:36 [INFO]: Test set missing rate: 83.99%
âœ… Dataset 'physionet_2012' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 02:46:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:46:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:46:36 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 02:46:36 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 02:46:36 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:46:36 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:46:40 [INFO]: Model placed on cuda:0
2025-08-24 02:46:40 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:46:42 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5644
2025-08-24 02:46:42 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5462
2025-08-24 02:46:43 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5372
2025-08-24 02:46:44 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5333
2025-08-24 02:46:45 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5309
2025-08-24 02:46:45 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5288
2025-08-24 02:46:46 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5278
2025-08-24 02:46:47 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5281
2025-08-24 02:46:48 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5273
2025-08-24 02:46:50 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5278
2025-08-24 02:46:51 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5273
2025-08-24 02:46:52 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5253
2025-08-24 02:46:53 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5247
2025-08-24 02:46:54 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5249
2025-08-24 02:46:55 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5254
2025-08-24 02:46:55 [INFO]: Finished training. The best model is from epoch#13.
2025-08-24 02:46:57 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4259| MSE: 0.5101| RMSE: 0.7142| MRE: 0.6013| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:47:13 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:47:13 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:47:13 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:47:13 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:47:14 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:47:22 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:47:22 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-24 02:47:22 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-24 02:47:22 [INFO]: Total sample number: 3997
2025-08-24 02:47:22 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:47:22 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:47:22 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:47:22 [INFO]: Number of steps: 48
2025-08-24 02:47:22 [INFO]: Number of features: 37
2025-08-24 02:47:22 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:47:22 [INFO]: Validating set missing rate: 85.72%
2025-08-24 02:47:22 [INFO]: Test set missing rate: 85.98%
âœ… Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 02:47:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:47:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:47:22 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 02:47:22 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 02:47:22 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:47:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 02:47:26 [INFO]: Model placed on cuda:0
2025-08-24 02:47:26 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 02:48:14 [INFO]: Epoch 001 - training loss (MAE): 0.4789, validation MSE: 0.5715
2025-08-24 02:49:01 [INFO]: Epoch 002 - training loss (MAE): 0.4514, validation MSE: 0.5581
2025-08-24 02:49:49 [INFO]: Epoch 003 - training loss (MAE): 0.4448, validation MSE: 0.5489
2025-08-24 02:50:37 [INFO]: Epoch 004 - training loss (MAE): 0.4431, validation MSE: 0.5469
2025-08-24 02:51:24 [INFO]: Epoch 005 - training loss (MAE): 0.4404, validation MSE: 0.5417
2025-08-24 02:52:12 [INFO]: Epoch 006 - training loss (MAE): 0.4404, validation MSE: 0.5417
2025-08-24 02:52:59 [INFO]: Epoch 007 - training loss (MAE): 0.4371, validation MSE: 0.5372
2025-08-24 02:53:46 [INFO]: Epoch 008 - training loss (MAE): 0.4378, validation MSE: 0.5379
2025-08-24 02:54:34 [INFO]: Epoch 009 - training loss (MAE): 0.4350, validation MSE: 0.5353
2025-08-24 02:55:22 [INFO]: Epoch 010 - training loss (MAE): 0.4376, validation MSE: 0.5338
2025-08-24 02:56:09 [INFO]: Epoch 011 - training loss (MAE): 0.4355, validation MSE: 0.5276
2025-08-24 02:56:57 [INFO]: Epoch 012 - training loss (MAE): 0.4329, validation MSE: 0.5312
2025-08-24 02:57:46 [INFO]: Epoch 013 - training loss (MAE): 0.4323, validation MSE: 0.5300
2025-08-24 02:58:34 [INFO]: Epoch 014 - training loss (MAE): 0.4319, validation MSE: 0.5282
2025-08-24 02:59:21 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: 0.5258
2025-08-24 02:59:21 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 02:59:24 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4445| MSE: 0.5153| RMSE: 0.7179| MRE: 0.6291| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 02:59:48 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 02:59:48 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 02:59:48 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 02:59:48 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 02:59:48 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 02:59:57 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 02:59:57 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-24 02:59:57 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-24 02:59:57 [INFO]: Total sample number: 3997
2025-08-24 02:59:57 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 02:59:57 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 02:59:57 [INFO]: Test set size: 800 (20.02%)
2025-08-24 02:59:57 [INFO]: Number of steps: 48
2025-08-24 02:59:57 [INFO]: Number of features: 37
2025-08-24 02:59:57 [INFO]: Train set missing rate: 79.68%
2025-08-24 02:59:57 [INFO]: Validating set missing rate: 85.72%
2025-08-24 02:59:57 [INFO]: Test set missing rate: 85.98%
âœ… Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 02:59:57 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 02:59:57 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 02:59:58 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 02:59:58 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 02:59:58 [INFO]: Using customized MAE as the training loss function.
2025-08-24 02:59:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:00:02 [INFO]: Model placed on cuda:0
2025-08-24 03:00:02 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:00:03 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5724
2025-08-24 03:00:05 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5541
2025-08-24 03:00:06 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5430
2025-08-24 03:00:07 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5376
2025-08-24 03:00:09 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5349
2025-08-24 03:00:10 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5324
2025-08-24 03:00:11 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5318
2025-08-24 03:00:12 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5314
2025-08-24 03:00:12 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5312
2025-08-24 03:00:13 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5317
2025-08-24 03:00:14 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5303
2025-08-24 03:00:15 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5298
2025-08-24 03:00:17 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5283
2025-08-24 03:00:18 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5275
2025-08-24 03:00:19 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5278
2025-08-24 03:00:19 [INFO]: Finished training. The best model is from epoch#14.
2025-08-24 03:00:21 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4429| MSE: 0.5196| RMSE: 0.7208| MRE: 0.6268| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:00:37 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:00:37 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:00:37 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:00:37 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:00:38 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:00:45 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:00:45 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-24 03:00:45 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-24 03:00:45 [INFO]: Total sample number: 3997
2025-08-24 03:00:45 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:00:45 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:00:45 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:00:45 [INFO]: Number of steps: 48
2025-08-24 03:00:45 [INFO]: Number of features: 37
2025-08-24 03:00:45 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:00:45 [INFO]: Validating set missing rate: 85.72%
2025-08-24 03:00:45 [INFO]: Test set missing rate: 85.98%
âœ… Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 03:00:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:00:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:00:45 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 03:00:45 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 03:00:45 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:00:45 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:00:49 [INFO]: Model placed on cuda:0
2025-08-24 03:00:49 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:01:30 [INFO]: Epoch 001 - training loss (MAE): 0.4830, validation MSE: 0.5752
2025-08-24 03:02:11 [INFO]: Epoch 002 - training loss (MAE): 0.4528, validation MSE: 0.5602
2025-08-24 03:02:52 [INFO]: Epoch 003 - training loss (MAE): 0.4451, validation MSE: 0.5497
2025-08-24 03:03:33 [INFO]: Epoch 004 - training loss (MAE): 0.4424, validation MSE: 0.5447
2025-08-24 03:04:12 [INFO]: Epoch 005 - training loss (MAE): 0.4394, validation MSE: 0.5397
2025-08-24 03:04:53 [INFO]: Epoch 006 - training loss (MAE): 0.4393, validation MSE: 0.5400
2025-08-24 03:05:35 [INFO]: Epoch 007 - training loss (MAE): 0.4362, validation MSE: 0.5390
2025-08-24 03:06:16 [INFO]: Epoch 008 - training loss (MAE): 0.4367, validation MSE: 0.5354
2025-08-24 03:06:56 [INFO]: Epoch 009 - training loss (MAE): 0.4337, validation MSE: 0.5364
2025-08-24 03:07:37 [INFO]: Epoch 010 - training loss (MAE): 0.4369, validation MSE: 0.5326
2025-08-24 03:08:18 [INFO]: Epoch 011 - training loss (MAE): 0.4342, validation MSE: 0.5314
2025-08-24 03:08:59 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5321
2025-08-24 03:09:40 [INFO]: Epoch 013 - training loss (MAE): 0.4314, validation MSE: 0.5306
2025-08-24 03:10:21 [INFO]: Epoch 014 - training loss (MAE): 0.4306, validation MSE: 0.5294
2025-08-24 03:11:02 [INFO]: Epoch 015 - training loss (MAE): 0.4308, validation MSE: 0.5270
2025-08-24 03:11:02 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:11:04 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4442| MSE: 0.5155| RMSE: 0.7180| MRE: 0.6286| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:11:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:11:28 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:11:28 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:11:28 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:11:28 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:11:37 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:11:37 [INFO]: 69227 values masked out in the val set as ground truth, take 29.90% of the original observed values
2025-08-24 03:11:37 [INFO]: 85805 values masked out in the test set as ground truth, take 30.11% of the original observed values
2025-08-24 03:11:37 [INFO]: Total sample number: 3997
2025-08-24 03:11:37 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:11:37 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:11:37 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:11:37 [INFO]: Number of steps: 48
2025-08-24 03:11:37 [INFO]: Number of features: 37
2025-08-24 03:11:37 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:11:37 [INFO]: Validating set missing rate: 85.72%
2025-08-24 03:11:37 [INFO]: Test set missing rate: 85.98%
âœ… Dataset 'physionet_2012' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 03:11:37 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:11:37 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:11:38 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 03:11:38 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 03:11:38 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:11:38 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:11:41 [INFO]: Model placed on cuda:0
2025-08-24 03:11:41 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:11:43 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5724
2025-08-24 03:11:44 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5541
2025-08-24 03:11:45 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5430
2025-08-24 03:11:46 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5376
2025-08-24 03:11:47 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5349
2025-08-24 03:11:49 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5324
2025-08-24 03:11:50 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5318
2025-08-24 03:11:51 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5314
2025-08-24 03:11:53 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5312
2025-08-24 03:11:53 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5317
2025-08-24 03:11:54 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5303
2025-08-24 03:11:55 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5298
2025-08-24 03:11:56 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5283
2025-08-24 03:11:57 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5275
2025-08-24 03:11:59 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5278
2025-08-24 03:11:59 [INFO]: Finished training. The best model is from epoch#14.
2025-08-24 03:12:00 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4429| MSE: 0.5196| RMSE: 0.7208| MRE: 0.6268| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:12:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:12:12 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:12:12 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:12:12 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:12:13 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:12:22 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:12:22 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-24 03:12:22 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-24 03:12:22 [INFO]: Total sample number: 3997
2025-08-24 03:12:22 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:12:22 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:12:22 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:12:22 [INFO]: Number of steps: 48
2025-08-24 03:12:22 [INFO]: Number of features: 37
2025-08-24 03:12:22 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:12:22 [INFO]: Validating set missing rate: 87.77%
2025-08-24 03:12:22 [INFO]: Test set missing rate: 87.98%
âœ… Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 03:12:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:12:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:12:23 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 03:12:23 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 03:12:23 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:12:23 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:12:26 [INFO]: Model placed on cuda:0
2025-08-24 03:12:26 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:13:13 [INFO]: Epoch 001 - training loss (MAE): 0.4789, validation MSE: 0.5886
2025-08-24 03:13:59 [INFO]: Epoch 002 - training loss (MAE): 0.4514, validation MSE: 0.5743
2025-08-24 03:14:45 [INFO]: Epoch 003 - training loss (MAE): 0.4448, validation MSE: 0.5644
2025-08-24 03:15:31 [INFO]: Epoch 004 - training loss (MAE): 0.4431, validation MSE: 0.5630
2025-08-24 03:16:20 [INFO]: Epoch 005 - training loss (MAE): 0.4404, validation MSE: 0.5576
2025-08-24 03:17:09 [INFO]: Epoch 006 - training loss (MAE): 0.4404, validation MSE: 0.5573
2025-08-24 03:17:59 [INFO]: Epoch 007 - training loss (MAE): 0.4371, validation MSE: 0.5517
2025-08-24 03:18:49 [INFO]: Epoch 008 - training loss (MAE): 0.4378, validation MSE: 0.5531
2025-08-24 03:19:38 [INFO]: Epoch 009 - training loss (MAE): 0.4350, validation MSE: 0.5501
2025-08-24 03:20:28 [INFO]: Epoch 010 - training loss (MAE): 0.4376, validation MSE: 0.5495
2025-08-24 03:21:18 [INFO]: Epoch 011 - training loss (MAE): 0.4355, validation MSE: 0.5423
2025-08-24 03:22:08 [INFO]: Epoch 012 - training loss (MAE): 0.4329, validation MSE: 0.5461
2025-08-24 03:22:57 [INFO]: Epoch 013 - training loss (MAE): 0.4323, validation MSE: 0.5454
2025-08-24 03:23:47 [INFO]: Epoch 014 - training loss (MAE): 0.4319, validation MSE: 0.5432
2025-08-24 03:24:37 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: 0.5394
2025-08-24 03:24:37 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:24:39 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4674| MSE: 0.5550| RMSE: 0.7450| MRE: 0.6624| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:24:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:24:55 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:24:55 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:24:55 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:24:56 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:25:01 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:25:01 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-24 03:25:01 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-24 03:25:01 [INFO]: Total sample number: 3997
2025-08-24 03:25:01 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:25:01 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:25:01 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:25:01 [INFO]: Number of steps: 48
2025-08-24 03:25:01 [INFO]: Number of features: 37
2025-08-24 03:25:01 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:25:01 [INFO]: Validating set missing rate: 87.77%
2025-08-24 03:25:01 [INFO]: Test set missing rate: 87.98%
âœ… Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 03:25:01 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:25:01 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:25:02 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 03:25:02 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 03:25:02 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:25:02 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:25:05 [INFO]: Model placed on cuda:0
2025-08-24 03:25:05 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:25:06 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5907
2025-08-24 03:25:07 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5724
2025-08-24 03:25:07 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5605
2025-08-24 03:25:08 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5553
2025-08-24 03:25:09 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5520
2025-08-24 03:25:10 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5493
2025-08-24 03:25:11 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5480
2025-08-24 03:25:12 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5484
2025-08-24 03:25:12 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5478
2025-08-24 03:25:13 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5486
2025-08-24 03:25:14 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5464
2025-08-24 03:25:15 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5462
2025-08-24 03:25:15 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5452
2025-08-24 03:25:16 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5447
2025-08-24 03:25:17 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5442
2025-08-24 03:25:17 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:25:19 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4691| MSE: 0.5600| RMSE: 0.7483| MRE: 0.6648| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:25:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:25:26 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:25:26 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:25:26 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:25:27 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:25:32 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:25:32 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-24 03:25:32 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-24 03:25:32 [INFO]: Total sample number: 3997
2025-08-24 03:25:32 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:25:32 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:25:32 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:25:32 [INFO]: Number of steps: 48
2025-08-24 03:25:32 [INFO]: Number of features: 37
2025-08-24 03:25:32 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:25:32 [INFO]: Validating set missing rate: 87.77%
2025-08-24 03:25:32 [INFO]: Test set missing rate: 87.98%
âœ… Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 03:25:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:25:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:25:33 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 03:25:33 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 03:25:33 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:25:33 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:25:35 [INFO]: Model placed on cuda:0
2025-08-24 03:25:35 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:26:20 [INFO]: Epoch 001 - training loss (MAE): 0.4830, validation MSE: 0.5934
2025-08-24 03:27:04 [INFO]: Epoch 002 - training loss (MAE): 0.4528, validation MSE: 0.5776
2025-08-24 03:27:49 [INFO]: Epoch 003 - training loss (MAE): 0.4451, validation MSE: 0.5665
2025-08-24 03:28:33 [INFO]: Epoch 004 - training loss (MAE): 0.4424, validation MSE: 0.5622
2025-08-24 03:29:17 [INFO]: Epoch 005 - training loss (MAE): 0.4394, validation MSE: 0.5555
2025-08-24 03:30:01 [INFO]: Epoch 006 - training loss (MAE): 0.4393, validation MSE: 0.5562
2025-08-24 03:30:46 [INFO]: Epoch 007 - training loss (MAE): 0.4362, validation MSE: 0.5544
2025-08-24 03:31:30 [INFO]: Epoch 008 - training loss (MAE): 0.4367, validation MSE: 0.5511
2025-08-24 03:32:15 [INFO]: Epoch 009 - training loss (MAE): 0.4337, validation MSE: 0.5519
2025-08-24 03:32:59 [INFO]: Epoch 010 - training loss (MAE): 0.4369, validation MSE: 0.5475
2025-08-24 03:33:43 [INFO]: Epoch 011 - training loss (MAE): 0.4342, validation MSE: 0.5464
2025-08-24 03:34:28 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5469
2025-08-24 03:35:11 [INFO]: Epoch 013 - training loss (MAE): 0.4314, validation MSE: 0.5454
2025-08-24 03:35:55 [INFO]: Epoch 014 - training loss (MAE): 0.4306, validation MSE: 0.5444
2025-08-24 03:36:39 [INFO]: Epoch 015 - training loss (MAE): 0.4308, validation MSE: 0.5413
2025-08-24 03:36:39 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:36:41 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4676| MSE: 0.5556| RMSE: 0.7454| MRE: 0.6628| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:36:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:36:57 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:36:57 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:36:57 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:36:57 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:37:03 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:37:03 [INFO]: 92507 values masked out in the val set as ground truth, take 39.95% of the original observed values
2025-08-24 03:37:03 [INFO]: 114242 values masked out in the test set as ground truth, take 40.09% of the original observed values
2025-08-24 03:37:03 [INFO]: Total sample number: 3997
2025-08-24 03:37:03 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:37:03 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:37:03 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:37:03 [INFO]: Number of steps: 48
2025-08-24 03:37:03 [INFO]: Number of features: 37
2025-08-24 03:37:03 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:37:03 [INFO]: Validating set missing rate: 87.77%
2025-08-24 03:37:03 [INFO]: Test set missing rate: 87.98%
âœ… Dataset 'physionet_2012' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 03:37:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:37:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:37:03 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 03:37:03 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 03:37:03 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:37:03 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:37:06 [INFO]: Model placed on cuda:0
2025-08-24 03:37:06 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:37:07 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.5907
2025-08-24 03:37:08 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.5724
2025-08-24 03:37:09 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.5605
2025-08-24 03:37:09 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5553
2025-08-24 03:37:10 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5520
2025-08-24 03:37:11 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5493
2025-08-24 03:37:12 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5480
2025-08-24 03:37:13 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5484
2025-08-24 03:37:14 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5478
2025-08-24 03:37:14 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5486
2025-08-24 03:37:15 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5464
2025-08-24 03:37:16 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5462
2025-08-24 03:37:17 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5452
2025-08-24 03:37:18 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5447
2025-08-24 03:37:18 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5442
2025-08-24 03:37:18 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:37:20 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4691| MSE: 0.5600| RMSE: 0.7483| MRE: 0.6648| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:37:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:37:28 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:37:28 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:37:28 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:37:28 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:37:34 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:37:34 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-24 03:37:34 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-24 03:37:34 [INFO]: Total sample number: 3997
2025-08-24 03:37:34 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:37:34 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:37:34 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:37:34 [INFO]: Number of steps: 48
2025-08-24 03:37:34 [INFO]: Number of features: 37
2025-08-24 03:37:34 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:37:34 [INFO]: Validating set missing rate: 89.79%
2025-08-24 03:37:34 [INFO]: Test set missing rate: 89.99%
âœ… Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 03:37:34 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:37:34 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:37:34 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 03:37:34 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 03:37:34 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:37:34 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:37:36 [INFO]: Model placed on cuda:0
2025-08-24 03:37:36 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:38:26 [INFO]: Epoch 001 - training loss (MAE): 0.4789, validation MSE: 0.6281
2025-08-24 03:39:15 [INFO]: Epoch 002 - training loss (MAE): 0.4514, validation MSE: 0.6127
2025-08-24 03:40:04 [INFO]: Epoch 003 - training loss (MAE): 0.4448, validation MSE: 0.6027
2025-08-24 03:40:54 [INFO]: Epoch 004 - training loss (MAE): 0.4431, validation MSE: 0.6030
2025-08-24 03:41:43 [INFO]: Epoch 005 - training loss (MAE): 0.4404, validation MSE: 0.5946
2025-08-24 03:42:32 [INFO]: Epoch 006 - training loss (MAE): 0.4404, validation MSE: 0.5951
2025-08-24 03:43:21 [INFO]: Epoch 007 - training loss (MAE): 0.4371, validation MSE: 0.5902
2025-08-24 03:44:11 [INFO]: Epoch 008 - training loss (MAE): 0.4378, validation MSE: 0.5911
2025-08-24 03:45:00 [INFO]: Epoch 009 - training loss (MAE): 0.4350, validation MSE: 0.5878
2025-08-24 03:45:50 [INFO]: Epoch 010 - training loss (MAE): 0.4376, validation MSE: 0.5871
2025-08-24 03:46:39 [INFO]: Epoch 011 - training loss (MAE): 0.4355, validation MSE: 0.5793
2025-08-24 03:47:29 [INFO]: Epoch 012 - training loss (MAE): 0.4329, validation MSE: 0.5839
2025-08-24 03:48:18 [INFO]: Epoch 013 - training loss (MAE): 0.4323, validation MSE: 0.5834
2025-08-24 03:49:07 [INFO]: Epoch 014 - training loss (MAE): 0.4319, validation MSE: 0.5793
2025-08-24 03:49:56 [INFO]: Epoch 015 - training loss (MAE): 0.4323, validation MSE: 0.5759
2025-08-24 03:49:56 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:49:57 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4954| MSE: 0.6079| RMSE: 0.7797| MRE: 0.7022| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
2025-08-24 03:50:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:50:14 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:50:14 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:50:14 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:50:15 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:50:21 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:50:21 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-24 03:50:21 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-24 03:50:21 [INFO]: Total sample number: 3997
2025-08-24 03:50:21 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:50:21 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:50:21 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:50:21 [INFO]: Number of steps: 48
2025-08-24 03:50:21 [INFO]: Number of features: 37
2025-08-24 03:50:21 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:50:21 [INFO]: Validating set missing rate: 89.79%
2025-08-24 03:50:21 [INFO]: Test set missing rate: 89.99%
âœ… Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 03:50:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:50:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:50:21 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 03:50:21 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 03:50:21 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:50:21 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:50:24 [INFO]: Model placed on cuda:0
2025-08-24 03:50:24 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:50:25 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.6329
2025-08-24 03:50:26 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.6144
2025-08-24 03:50:27 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.6020
2025-08-24 03:50:28 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5968
2025-08-24 03:50:29 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5927
2025-08-24 03:50:29 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5902
2025-08-24 03:50:30 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5888
2025-08-24 03:50:31 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5893
2025-08-24 03:50:32 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5892
2025-08-24 03:50:33 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5891
2025-08-24 03:50:33 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5866
2025-08-24 03:50:34 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5874
2025-08-24 03:50:35 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5872
2025-08-24 03:50:36 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5858
2025-08-24 03:50:37 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5851
2025-08-24 03:50:37 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 03:50:38 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5012| MSE: 0.6204| RMSE: 0.7877| MRE: 0.7103| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 03:50:46 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 03:50:46 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 03:50:46 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 03:50:46 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 03:50:46 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 03:50:52 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 03:50:52 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-24 03:50:52 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-24 03:50:52 [INFO]: Total sample number: 3997
2025-08-24 03:50:52 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 03:50:52 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 03:50:52 [INFO]: Test set size: 800 (20.02%)
2025-08-24 03:50:52 [INFO]: Number of steps: 48
2025-08-24 03:50:52 [INFO]: Number of features: 37
2025-08-24 03:50:52 [INFO]: Train set missing rate: 79.68%
2025-08-24 03:50:52 [INFO]: Validating set missing rate: 89.79%
2025-08-24 03:50:52 [INFO]: Test set missing rate: 89.99%
âœ… Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 03:50:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 03:50:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 03:50:53 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 03:50:53 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 03:50:53 [INFO]: Using customized MAE as the training loss function.
2025-08-24 03:50:53 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 03:50:55 [INFO]: Model placed on cuda:0
2025-08-24 03:50:55 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 03:51:38 [INFO]: Epoch 001 - training loss (MAE): 0.4830, validation MSE: 0.6321
2025-08-24 03:52:22 [INFO]: Epoch 002 - training loss (MAE): 0.4528, validation MSE: 0.6164
2025-08-24 03:53:05 [INFO]: Epoch 003 - training loss (MAE): 0.4451, validation MSE: 0.6045
2025-08-24 03:53:49 [INFO]: Epoch 004 - training loss (MAE): 0.4424, validation MSE: 0.6014
2025-08-24 03:54:33 [INFO]: Epoch 005 - training loss (MAE): 0.4394, validation MSE: 0.5938
2025-08-24 03:55:17 [INFO]: Epoch 006 - training loss (MAE): 0.4393, validation MSE: 0.5941
2025-08-24 03:56:01 [INFO]: Epoch 007 - training loss (MAE): 0.4362, validation MSE: 0.5925
2025-08-24 03:56:45 [INFO]: Epoch 008 - training loss (MAE): 0.4367, validation MSE: 0.5886
2025-08-24 03:57:28 [INFO]: Epoch 009 - training loss (MAE): 0.4337, validation MSE: 0.5902
2025-08-24 03:58:13 [INFO]: Epoch 010 - training loss (MAE): 0.4369, validation MSE: 0.5842
2025-08-24 03:58:56 [INFO]: Epoch 011 - training loss (MAE): 0.4342, validation MSE: 0.5831
2025-08-24 03:59:40 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5836
2025-08-24 04:00:24 [INFO]: Epoch 013 - training loss (MAE): 0.4314, validation MSE: 0.5826
2025-08-24 04:01:08 [INFO]: Epoch 014 - training loss (MAE): 0.4306, validation MSE: 0.5810
2025-08-24 04:01:52 [INFO]: Epoch 015 - training loss (MAE): 0.4308, validation MSE: 0.5776
2025-08-24 04:01:52 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:01:54 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4967| MSE: 0.6108| RMSE: 0.7816| MRE: 0.7040| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=35)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:02:11 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:02:11 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012
2025-08-24 04:02:11 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...
2025-08-24 04:02:11 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...
2025-08-24 04:02:11 [INFO]: Loaded successfully!
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/physionet_2012.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  X = X.groupby("RecordID").apply(apply_func)
2025-08-24 04:02:17 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. 
2025-08-24 04:02:17 [INFO]: 115521 values masked out in the val set as ground truth, take 49.89% of the original observed values
2025-08-24 04:02:17 [INFO]: 142704 values masked out in the test set as ground truth, take 50.08% of the original observed values
2025-08-24 04:02:17 [INFO]: Total sample number: 3997
2025-08-24 04:02:17 [INFO]: Training set size: 2557 (63.97%)
2025-08-24 04:02:17 [INFO]: Validation set size: 640 (16.01%)
2025-08-24 04:02:17 [INFO]: Test set size: 800 (20.02%)
2025-08-24 04:02:17 [INFO]: Number of steps: 48
2025-08-24 04:02:17 [INFO]: Number of features: 37
2025-08-24 04:02:17 [INFO]: Train set missing rate: 79.68%
2025-08-24 04:02:17 [INFO]: Validating set missing rate: 89.79%
2025-08-24 04:02:17 [INFO]: Test set missing rate: 89.99%
âœ… Dataset 'physionet_2012' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:02:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:02:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:02:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:02:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:02:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:02:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:02:20 [INFO]: Model placed on cuda:0
2025-08-24 04:02:20 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:02:21 [INFO]: Epoch 001 - training loss (MAE): 0.4827, validation MSE: 0.6329
2025-08-24 04:02:22 [INFO]: Epoch 002 - training loss (MAE): 0.4477, validation MSE: 0.6144
2025-08-24 04:02:23 [INFO]: Epoch 003 - training loss (MAE): 0.4402, validation MSE: 0.6020
2025-08-24 04:02:24 [INFO]: Epoch 004 - training loss (MAE): 0.4376, validation MSE: 0.5968
2025-08-24 04:02:25 [INFO]: Epoch 005 - training loss (MAE): 0.4355, validation MSE: 0.5927
2025-08-24 04:02:25 [INFO]: Epoch 006 - training loss (MAE): 0.4358, validation MSE: 0.5902
2025-08-24 04:02:26 [INFO]: Epoch 007 - training loss (MAE): 0.4330, validation MSE: 0.5888
2025-08-24 04:02:27 [INFO]: Epoch 008 - training loss (MAE): 0.4340, validation MSE: 0.5893
2025-08-24 04:02:28 [INFO]: Epoch 009 - training loss (MAE): 0.4328, validation MSE: 0.5892
2025-08-24 04:02:29 [INFO]: Epoch 010 - training loss (MAE): 0.4355, validation MSE: 0.5891
2025-08-24 04:02:30 [INFO]: Epoch 011 - training loss (MAE): 0.4336, validation MSE: 0.5866
2025-08-24 04:02:31 [INFO]: Epoch 012 - training loss (MAE): 0.4315, validation MSE: 0.5874
2025-08-24 04:02:31 [INFO]: Epoch 013 - training loss (MAE): 0.4307, validation MSE: 0.5872
2025-08-24 04:02:32 [INFO]: Epoch 014 - training loss (MAE): 0.4316, validation MSE: 0.5858
2025-08-24 04:02:33 [INFO]: Epoch 015 - training loss (MAE): 0.4320, validation MSE: 0.5851
2025-08-24 04:02:33 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:02:35 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5012| MSE: 0.6204| RMSE: 0.7877| MRE: 0.7103| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/physionet_2012/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: physionet_2012 | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:02:42 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:02:42 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:02:42 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:02:42 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:02:43 [INFO]: Loaded successfully!
2025-08-24 04:02:43 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:02:43 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:02:43 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:02:44 [INFO]: Total sample number: 833
2025-08-24 04:02:44 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:02:44 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:02:44 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:02:44 [INFO]: Number of steps: 168
2025-08-24 04:02:44 [INFO]: Number of features: 370
2025-08-24 04:02:44 [INFO]: Train set missing rate: 10.00%
2025-08-24 04:02:44 [INFO]: Validating set missing rate: 10.00%
2025-08-24 04:02:44 [INFO]: Test set missing rate: 10.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:02:44 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:02:44 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:02:45 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:02:45 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:02:45 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:02:45 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:02:47 [INFO]: Model placed on cuda:0
2025-08-24 04:02:47 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:02:49 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:02:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:02:56 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:02:56 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:02:56 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:02:57 [INFO]: Loaded successfully!
2025-08-24 04:02:57 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:02:57 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:02:57 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:02:58 [INFO]: Total sample number: 833
2025-08-24 04:02:58 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:02:58 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:02:58 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:02:58 [INFO]: Number of steps: 168
2025-08-24 04:02:58 [INFO]: Number of features: 370
2025-08-24 04:02:58 [INFO]: Train set missing rate: 10.00%
2025-08-24 04:02:58 [INFO]: Validating set missing rate: 10.00%
2025-08-24 04:02:58 [INFO]: Test set missing rate: 10.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:02:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:02:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:02:58 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:02:58 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:02:58 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:02:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:03:00 [INFO]: Model placed on cuda:0
2025-08-24 04:03:00 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:03:21 [INFO]: Epoch 001 - training loss (MAE): 0.6641, validation MSE: 1.3788
2025-08-24 04:03:43 [INFO]: Epoch 002 - training loss (MAE): 0.5556, validation MSE: 1.1711
2025-08-24 04:04:05 [INFO]: Epoch 003 - training loss (MAE): 0.4836, validation MSE: 1.0891
2025-08-24 04:04:18 [INFO]: Epoch 004 - training loss (MAE): 0.4493, validation MSE: 1.0176
2025-08-24 04:04:42 [INFO]: Epoch 005 - training loss (MAE): 0.4319, validation MSE: 0.9867
2025-08-24 04:05:07 [INFO]: Epoch 006 - training loss (MAE): 0.4180, validation MSE: 0.9430
2025-08-24 04:05:19 [INFO]: Epoch 007 - training loss (MAE): 0.4099, validation MSE: 0.9167
2025-08-24 04:05:30 [INFO]: Epoch 008 - training loss (MAE): 0.4047, validation MSE: 0.8880
2025-08-24 04:05:39 [INFO]: Epoch 009 - training loss (MAE): 0.4010, validation MSE: 0.8668
2025-08-24 04:05:47 [INFO]: Epoch 010 - training loss (MAE): 0.3974, validation MSE: 0.8468
2025-08-24 04:05:55 [INFO]: Epoch 011 - training loss (MAE): 0.3938, validation MSE: 0.8339
2025-08-24 04:06:03 [INFO]: Epoch 012 - training loss (MAE): 0.3919, validation MSE: 0.8137
2025-08-24 04:06:09 [INFO]: Epoch 013 - training loss (MAE): 0.3895, validation MSE: 0.8034
2025-08-24 04:06:15 [INFO]: Epoch 014 - training loss (MAE): 0.3882, validation MSE: 0.7884
2025-08-24 04:06:20 [INFO]: Epoch 015 - training loss (MAE): 0.3871, validation MSE: 0.7794
2025-08-24 04:06:20 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:06:22 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2826| MSE: 0.1725| RMSE: 0.4154| MRE: 0.1513| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:06:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:06:34 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:06:34 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:06:34 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:06:35 [INFO]: Loaded successfully!
2025-08-24 04:06:35 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:06:35 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:06:35 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:06:36 [INFO]: Total sample number: 833
2025-08-24 04:06:36 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:06:36 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:06:36 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:06:36 [INFO]: Number of steps: 168
2025-08-24 04:06:36 [INFO]: Number of features: 370
2025-08-24 04:06:36 [INFO]: Train set missing rate: 10.00%
2025-08-24 04:06:36 [INFO]: Validating set missing rate: 10.00%
2025-08-24 04:06:36 [INFO]: Test set missing rate: 10.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:06:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:06:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:06:36 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:06:36 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:06:36 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:06:36 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:06:39 [INFO]: Model placed on cuda:0
2025-08-24 04:06:39 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:06:41 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:06:48 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:06:48 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:06:48 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:06:48 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:06:48 [INFO]: Loaded successfully!
2025-08-24 04:06:48 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:06:48 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:06:48 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:06:50 [INFO]: Total sample number: 833
2025-08-24 04:06:50 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:06:50 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:06:50 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:06:50 [INFO]: Number of steps: 168
2025-08-24 04:06:50 [INFO]: Number of features: 370
2025-08-24 04:06:50 [INFO]: Train set missing rate: 10.00%
2025-08-24 04:06:50 [INFO]: Validating set missing rate: 10.00%
2025-08-24 04:06:50 [INFO]: Test set missing rate: 10.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:06:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:06:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:06:50 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:06:50 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:06:50 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:06:50 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:06:53 [INFO]: Model placed on cuda:0
2025-08-24 04:06:53 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:07:01 [INFO]: Epoch 001 - training loss (MAE): 0.6641, validation MSE: 1.3788
2025-08-24 04:07:11 [INFO]: Epoch 002 - training loss (MAE): 0.5556, validation MSE: 1.1711
2025-08-24 04:07:18 [INFO]: Epoch 003 - training loss (MAE): 0.4836, validation MSE: 1.0891
2025-08-24 04:07:24 [INFO]: Epoch 004 - training loss (MAE): 0.4493, validation MSE: 1.0176
2025-08-24 04:07:33 [INFO]: Epoch 005 - training loss (MAE): 0.4319, validation MSE: 0.9867
2025-08-24 04:07:39 [INFO]: Epoch 006 - training loss (MAE): 0.4180, validation MSE: 0.9430
2025-08-24 04:07:46 [INFO]: Epoch 007 - training loss (MAE): 0.4099, validation MSE: 0.9167
2025-08-24 04:07:54 [INFO]: Epoch 008 - training loss (MAE): 0.4047, validation MSE: 0.8880
2025-08-24 04:08:03 [INFO]: Epoch 009 - training loss (MAE): 0.4010, validation MSE: 0.8668
2025-08-24 04:08:13 [INFO]: Epoch 010 - training loss (MAE): 0.3974, validation MSE: 0.8468
2025-08-24 04:08:20 [INFO]: Epoch 011 - training loss (MAE): 0.3938, validation MSE: 0.8339
2025-08-24 04:08:29 [INFO]: Epoch 012 - training loss (MAE): 0.3919, validation MSE: 0.8137
2025-08-24 04:08:37 [INFO]: Epoch 013 - training loss (MAE): 0.3895, validation MSE: 0.8034
2025-08-24 04:08:44 [INFO]: Epoch 014 - training loss (MAE): 0.3882, validation MSE: 0.7884
2025-08-24 04:08:52 [INFO]: Epoch 015 - training loss (MAE): 0.3871, validation MSE: 0.7794
2025-08-24 04:08:52 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:08:53 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2826| MSE: 0.1725| RMSE: 0.4154| MRE: 0.1513| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:09:05 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:09:05 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:09:05 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:09:05 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:09:05 [INFO]: Loaded successfully!
2025-08-24 04:09:05 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:09:05 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:09:05 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:09:06 [INFO]: Total sample number: 833
2025-08-24 04:09:06 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:09:06 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:09:06 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:09:06 [INFO]: Number of steps: 168
2025-08-24 04:09:06 [INFO]: Number of features: 370
2025-08-24 04:09:06 [INFO]: Train set missing rate: 20.00%
2025-08-24 04:09:06 [INFO]: Validating set missing rate: 20.01%
2025-08-24 04:09:06 [INFO]: Test set missing rate: 20.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:09:06 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:09:06 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:09:06 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:09:06 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:09:06 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:09:06 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:09:09 [INFO]: Model placed on cuda:0
2025-08-24 04:09:09 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:09:10 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:09:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:09:20 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:09:20 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:09:20 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:09:20 [INFO]: Loaded successfully!
2025-08-24 04:09:20 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:09:20 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:09:20 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:09:21 [INFO]: Total sample number: 833
2025-08-24 04:09:21 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:09:21 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:09:21 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:09:21 [INFO]: Number of steps: 168
2025-08-24 04:09:21 [INFO]: Number of features: 370
2025-08-24 04:09:21 [INFO]: Train set missing rate: 20.00%
2025-08-24 04:09:21 [INFO]: Validating set missing rate: 20.01%
2025-08-24 04:09:21 [INFO]: Test set missing rate: 20.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:09:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:09:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:09:22 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:09:22 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:09:22 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:09:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:09:24 [INFO]: Model placed on cuda:0
2025-08-24 04:09:24 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:09:29 [INFO]: Epoch 001 - training loss (MAE): 0.6789, validation MSE: 1.5384
2025-08-24 04:09:34 [INFO]: Epoch 002 - training loss (MAE): 0.5790, validation MSE: 1.3389
2025-08-24 04:09:38 [INFO]: Epoch 003 - training loss (MAE): 0.5064, validation MSE: 1.2627
2025-08-24 04:09:44 [INFO]: Epoch 004 - training loss (MAE): 0.4719, validation MSE: 1.1763
2025-08-24 04:09:50 [INFO]: Epoch 005 - training loss (MAE): 0.4540, validation MSE: 1.1381
2025-08-24 04:09:58 [INFO]: Epoch 006 - training loss (MAE): 0.4400, validation MSE: 1.0987
2025-08-24 04:10:04 [INFO]: Epoch 007 - training loss (MAE): 0.4321, validation MSE: 1.0659
2025-08-24 04:10:07 [INFO]: Epoch 008 - training loss (MAE): 0.4271, validation MSE: 1.0400
2025-08-24 04:10:14 [INFO]: Epoch 009 - training loss (MAE): 0.4239, validation MSE: 1.0183
2025-08-24 04:10:22 [INFO]: Epoch 010 - training loss (MAE): 0.4204, validation MSE: 0.9970
2025-08-24 04:10:29 [INFO]: Epoch 011 - training loss (MAE): 0.4167, validation MSE: 0.9872
2025-08-24 04:10:37 [INFO]: Epoch 012 - training loss (MAE): 0.4148, validation MSE: 0.9677
2025-08-24 04:10:47 [INFO]: Epoch 013 - training loss (MAE): 0.4124, validation MSE: 0.9582
2025-08-24 04:10:55 [INFO]: Epoch 014 - training loss (MAE): 0.4113, validation MSE: 0.9442
2025-08-24 04:11:02 [INFO]: Epoch 015 - training loss (MAE): 0.4105, validation MSE: 0.9364
2025-08-24 04:11:02 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:11:04 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4089| MSE: 0.3606| RMSE: 0.6005| MRE: 0.2189| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:11:15 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:11:15 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:11:15 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:11:15 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:11:15 [INFO]: Loaded successfully!
2025-08-24 04:11:15 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:11:15 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:11:15 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:11:16 [INFO]: Total sample number: 833
2025-08-24 04:11:16 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:11:16 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:11:16 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:11:16 [INFO]: Number of steps: 168
2025-08-24 04:11:16 [INFO]: Number of features: 370
2025-08-24 04:11:16 [INFO]: Train set missing rate: 20.00%
2025-08-24 04:11:16 [INFO]: Validating set missing rate: 20.01%
2025-08-24 04:11:16 [INFO]: Test set missing rate: 20.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:11:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:11:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:11:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:11:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:11:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:11:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:11:19 [INFO]: Model placed on cuda:0
2025-08-24 04:11:19 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:11:21 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:11:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:11:31 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:11:31 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:11:31 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:11:32 [INFO]: Loaded successfully!
2025-08-24 04:11:32 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:11:32 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:11:32 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:11:33 [INFO]: Total sample number: 833
2025-08-24 04:11:33 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:11:33 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:11:33 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:11:33 [INFO]: Number of steps: 168
2025-08-24 04:11:33 [INFO]: Number of features: 370
2025-08-24 04:11:33 [INFO]: Train set missing rate: 20.00%
2025-08-24 04:11:33 [INFO]: Validating set missing rate: 20.01%
2025-08-24 04:11:33 [INFO]: Test set missing rate: 20.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:11:33 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:11:33 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:11:34 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:11:34 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:11:34 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:11:34 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:11:36 [INFO]: Model placed on cuda:0
2025-08-24 04:11:36 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:11:44 [INFO]: Epoch 001 - training loss (MAE): 0.6789, validation MSE: 1.5384
2025-08-24 04:11:48 [INFO]: Epoch 002 - training loss (MAE): 0.5790, validation MSE: 1.3389
2025-08-24 04:11:52 [INFO]: Epoch 003 - training loss (MAE): 0.5064, validation MSE: 1.2627
2025-08-24 04:11:58 [INFO]: Epoch 004 - training loss (MAE): 0.4719, validation MSE: 1.1763
2025-08-24 04:12:01 [INFO]: Epoch 005 - training loss (MAE): 0.4540, validation MSE: 1.1381
2025-08-24 04:12:05 [INFO]: Epoch 006 - training loss (MAE): 0.4400, validation MSE: 1.0987
2025-08-24 04:12:08 [INFO]: Epoch 007 - training loss (MAE): 0.4321, validation MSE: 1.0659
2025-08-24 04:12:12 [INFO]: Epoch 008 - training loss (MAE): 0.4271, validation MSE: 1.0400
2025-08-24 04:12:15 [INFO]: Epoch 009 - training loss (MAE): 0.4239, validation MSE: 1.0183
2025-08-24 04:12:20 [INFO]: Epoch 010 - training loss (MAE): 0.4204, validation MSE: 0.9970
2025-08-24 04:12:23 [INFO]: Epoch 011 - training loss (MAE): 0.4167, validation MSE: 0.9872
2025-08-24 04:12:30 [INFO]: Epoch 012 - training loss (MAE): 0.4148, validation MSE: 0.9677
2025-08-24 04:12:34 [INFO]: Epoch 013 - training loss (MAE): 0.4124, validation MSE: 0.9582
2025-08-24 04:12:38 [INFO]: Epoch 014 - training loss (MAE): 0.4113, validation MSE: 0.9442
2025-08-24 04:12:41 [INFO]: Epoch 015 - training loss (MAE): 0.4105, validation MSE: 0.9364
2025-08-24 04:12:41 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:12:42 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4089| MSE: 0.3606| RMSE: 0.6005| MRE: 0.2189| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:12:52 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:12:52 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:12:52 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:12:52 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:12:53 [INFO]: Loaded successfully!
2025-08-24 04:12:53 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:12:53 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:12:53 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:12:54 [INFO]: Total sample number: 833
2025-08-24 04:12:54 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:12:54 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:12:54 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:12:54 [INFO]: Number of steps: 168
2025-08-24 04:12:54 [INFO]: Number of features: 370
2025-08-24 04:12:54 [INFO]: Train set missing rate: 30.01%
2025-08-24 04:12:54 [INFO]: Validating set missing rate: 30.02%
2025-08-24 04:12:54 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:12:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:12:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:12:54 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:12:54 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:12:54 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:12:54 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:12:56 [INFO]: Model placed on cuda:0
2025-08-24 04:12:56 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:12:58 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:13:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:13:10 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:13:10 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:13:10 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:13:10 [INFO]: Loaded successfully!
2025-08-24 04:13:10 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:13:10 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:13:10 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:13:11 [INFO]: Total sample number: 833
2025-08-24 04:13:11 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:13:11 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:13:11 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:13:11 [INFO]: Number of steps: 168
2025-08-24 04:13:11 [INFO]: Number of features: 370
2025-08-24 04:13:11 [INFO]: Train set missing rate: 30.01%
2025-08-24 04:13:11 [INFO]: Validating set missing rate: 30.02%
2025-08-24 04:13:11 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:13:11 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:13:11 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:13:12 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:13:12 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:13:12 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:13:12 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:13:14 [INFO]: Model placed on cuda:0
2025-08-24 04:13:14 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:13:34 [INFO]: Epoch 001 - training loss (MAE): 0.6945, validation MSE: 1.6643
2025-08-24 04:13:51 [INFO]: Epoch 002 - training loss (MAE): 0.6046, validation MSE: 1.4689
2025-08-24 04:14:14 [INFO]: Epoch 003 - training loss (MAE): 0.5319, validation MSE: 1.4151
2025-08-24 04:14:36 [INFO]: Epoch 004 - training loss (MAE): 0.4970, validation MSE: 1.3321
2025-08-24 04:15:02 [INFO]: Epoch 005 - training loss (MAE): 0.4784, validation MSE: 1.2899
2025-08-24 04:15:28 [INFO]: Epoch 006 - training loss (MAE): 0.4642, validation MSE: 1.2561
2025-08-24 04:15:53 [INFO]: Epoch 007 - training loss (MAE): 0.4566, validation MSE: 1.2232
2025-08-24 04:16:12 [INFO]: Epoch 008 - training loss (MAE): 0.4518, validation MSE: 1.1976
2025-08-24 04:16:30 [INFO]: Epoch 009 - training loss (MAE): 0.4489, validation MSE: 1.1744
2025-08-24 04:16:49 [INFO]: Epoch 010 - training loss (MAE): 0.4454, validation MSE: 1.1509
2025-08-24 04:17:04 [INFO]: Epoch 011 - training loss (MAE): 0.4417, validation MSE: 1.1399
2025-08-24 04:17:16 [INFO]: Epoch 012 - training loss (MAE): 0.4398, validation MSE: 1.1209
2025-08-24 04:17:38 [INFO]: Epoch 013 - training loss (MAE): 0.4374, validation MSE: 1.1134
2025-08-24 04:17:57 [INFO]: Epoch 014 - training loss (MAE): 0.4365, validation MSE: 1.0950
2025-08-24 04:18:18 [INFO]: Epoch 015 - training loss (MAE): 0.4360, validation MSE: 1.0862
2025-08-24 04:18:18 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:18:22 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5573| MSE: 0.6751| RMSE: 0.8217| MRE: 0.2982| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:18:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:18:35 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:18:35 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:18:35 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:18:35 [INFO]: Loaded successfully!
2025-08-24 04:18:35 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:18:35 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:18:35 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:18:36 [INFO]: Total sample number: 833
2025-08-24 04:18:36 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:18:36 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:18:36 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:18:36 [INFO]: Number of steps: 168
2025-08-24 04:18:36 [INFO]: Number of features: 370
2025-08-24 04:18:36 [INFO]: Train set missing rate: 30.01%
2025-08-24 04:18:36 [INFO]: Validating set missing rate: 30.02%
2025-08-24 04:18:36 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:18:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:18:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:18:36 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:18:36 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:18:36 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:18:36 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:18:39 [INFO]: Model placed on cuda:0
2025-08-24 04:18:39 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:18:40 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:18:52 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:18:52 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:18:52 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:18:52 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:18:52 [INFO]: Loaded successfully!
2025-08-24 04:18:52 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:18:52 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:18:52 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:18:53 [INFO]: Total sample number: 833
2025-08-24 04:18:53 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:18:53 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:18:53 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:18:53 [INFO]: Number of steps: 168
2025-08-24 04:18:53 [INFO]: Number of features: 370
2025-08-24 04:18:53 [INFO]: Train set missing rate: 30.01%
2025-08-24 04:18:53 [INFO]: Validating set missing rate: 30.02%
2025-08-24 04:18:53 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:18:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:18:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:18:54 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:18:54 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:18:54 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:18:54 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:18:57 [INFO]: Model placed on cuda:0
2025-08-24 04:18:57 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:19:21 [INFO]: Epoch 001 - training loss (MAE): 0.6945, validation MSE: 1.6643
2025-08-24 04:19:46 [INFO]: Epoch 002 - training loss (MAE): 0.6046, validation MSE: 1.4689
2025-08-24 04:20:09 [INFO]: Epoch 003 - training loss (MAE): 0.5319, validation MSE: 1.4151
2025-08-24 04:20:32 [INFO]: Epoch 004 - training loss (MAE): 0.4970, validation MSE: 1.3321
2025-08-24 04:20:55 [INFO]: Epoch 005 - training loss (MAE): 0.4784, validation MSE: 1.2899
2025-08-24 04:21:17 [INFO]: Epoch 006 - training loss (MAE): 0.4642, validation MSE: 1.2561
2025-08-24 04:21:39 [INFO]: Epoch 007 - training loss (MAE): 0.4566, validation MSE: 1.2232
2025-08-24 04:22:03 [INFO]: Epoch 008 - training loss (MAE): 0.4518, validation MSE: 1.1976
2025-08-24 04:22:28 [INFO]: Epoch 009 - training loss (MAE): 0.4489, validation MSE: 1.1744
2025-08-24 04:22:46 [INFO]: Epoch 010 - training loss (MAE): 0.4454, validation MSE: 1.1509
2025-08-24 04:23:08 [INFO]: Epoch 011 - training loss (MAE): 0.4417, validation MSE: 1.1399
2025-08-24 04:23:28 [INFO]: Epoch 012 - training loss (MAE): 0.4398, validation MSE: 1.1209
2025-08-24 04:23:52 [INFO]: Epoch 013 - training loss (MAE): 0.4374, validation MSE: 1.1134
2025-08-24 04:24:14 [INFO]: Epoch 014 - training loss (MAE): 0.4365, validation MSE: 1.0950
2025-08-24 04:24:33 [INFO]: Epoch 015 - training loss (MAE): 0.4360, validation MSE: 1.0862
2025-08-24 04:24:33 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:24:34 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5573| MSE: 0.6751| RMSE: 0.8217| MRE: 0.2982| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:24:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:24:49 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:24:49 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:24:49 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:24:49 [INFO]: Loaded successfully!
2025-08-24 04:24:49 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:24:49 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:24:49 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:24:50 [INFO]: Total sample number: 833
2025-08-24 04:24:50 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:24:50 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:24:50 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:24:50 [INFO]: Number of steps: 168
2025-08-24 04:24:50 [INFO]: Number of features: 370
2025-08-24 04:24:50 [INFO]: Train set missing rate: 40.00%
2025-08-24 04:24:50 [INFO]: Validating set missing rate: 40.02%
2025-08-24 04:24:50 [INFO]: Test set missing rate: 40.03%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:24:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:24:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:24:51 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:24:51 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:24:51 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:24:51 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:24:53 [INFO]: Model placed on cuda:0
2025-08-24 04:24:53 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:24:55 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:25:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:25:09 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:25:09 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:25:09 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:25:09 [INFO]: Loaded successfully!
2025-08-24 04:25:09 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:25:09 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:25:09 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:25:10 [INFO]: Total sample number: 833
2025-08-24 04:25:10 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:25:10 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:25:10 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:25:10 [INFO]: Number of steps: 168
2025-08-24 04:25:10 [INFO]: Number of features: 370
2025-08-24 04:25:10 [INFO]: Train set missing rate: 40.00%
2025-08-24 04:25:10 [INFO]: Validating set missing rate: 40.02%
2025-08-24 04:25:10 [INFO]: Test set missing rate: 40.03%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:25:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:25:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:25:11 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:25:11 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:25:11 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:25:11 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:25:14 [INFO]: Model placed on cuda:0
2025-08-24 04:25:14 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:25:21 [INFO]: Epoch 001 - training loss (MAE): 0.7113, validation MSE: 1.9009
2025-08-24 04:25:27 [INFO]: Epoch 002 - training loss (MAE): 0.6325, validation MSE: 1.7104
2025-08-24 04:25:32 [INFO]: Epoch 003 - training loss (MAE): 0.5610, validation MSE: 1.6693
2025-08-24 04:25:36 [INFO]: Epoch 004 - training loss (MAE): 0.5246, validation MSE: 1.6145
2025-08-24 04:25:44 [INFO]: Epoch 005 - training loss (MAE): 0.5055, validation MSE: 1.5660
2025-08-24 04:25:50 [INFO]: Epoch 006 - training loss (MAE): 0.4907, validation MSE: 1.5338
2025-08-24 04:25:55 [INFO]: Epoch 007 - training loss (MAE): 0.4834, validation MSE: 1.5028
2025-08-24 04:26:02 [INFO]: Epoch 008 - training loss (MAE): 0.4787, validation MSE: 1.4795
2025-08-24 04:26:08 [INFO]: Epoch 009 - training loss (MAE): 0.4763, validation MSE: 1.4554
2025-08-24 04:26:15 [INFO]: Epoch 010 - training loss (MAE): 0.4726, validation MSE: 1.4326
2025-08-24 04:26:23 [INFO]: Epoch 011 - training loss (MAE): 0.4692, validation MSE: 1.4219
2025-08-24 04:26:29 [INFO]: Epoch 012 - training loss (MAE): 0.4672, validation MSE: 1.4044
2025-08-24 04:26:35 [INFO]: Epoch 013 - training loss (MAE): 0.4648, validation MSE: 1.3971
2025-08-24 04:26:39 [INFO]: Epoch 014 - training loss (MAE): 0.4641, validation MSE: 1.3791
2025-08-24 04:26:43 [INFO]: Epoch 015 - training loss (MAE): 0.4640, validation MSE: 1.3710
2025-08-24 04:26:43 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:26:45 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.7163| MSE: 1.1150| RMSE: 1.0559| MRE: 0.3834| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:26:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:26:56 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:26:56 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:26:56 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:26:56 [INFO]: Loaded successfully!
2025-08-24 04:26:56 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:26:56 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:26:56 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:26:58 [INFO]: Total sample number: 833
2025-08-24 04:26:58 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:26:58 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:26:58 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:26:58 [INFO]: Number of steps: 168
2025-08-24 04:26:58 [INFO]: Number of features: 370
2025-08-24 04:26:58 [INFO]: Train set missing rate: 40.00%
2025-08-24 04:26:58 [INFO]: Validating set missing rate: 40.02%
2025-08-24 04:26:58 [INFO]: Test set missing rate: 40.03%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:26:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:26:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:26:58 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:26:58 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:26:58 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:26:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:27:01 [INFO]: Model placed on cuda:0
2025-08-24 04:27:01 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:27:03 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:27:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:27:12 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:27:12 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:27:12 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:27:12 [INFO]: Loaded successfully!
2025-08-24 04:27:12 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:27:12 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:27:12 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:27:14 [INFO]: Total sample number: 833
2025-08-24 04:27:14 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:27:14 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:27:14 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:27:14 [INFO]: Number of steps: 168
2025-08-24 04:27:14 [INFO]: Number of features: 370
2025-08-24 04:27:14 [INFO]: Train set missing rate: 40.00%
2025-08-24 04:27:14 [INFO]: Validating set missing rate: 40.02%
2025-08-24 04:27:14 [INFO]: Test set missing rate: 40.03%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:27:14 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:27:14 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:27:14 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:27:14 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:27:14 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:27:14 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:27:17 [INFO]: Model placed on cuda:0
2025-08-24 04:27:17 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:27:22 [INFO]: Epoch 001 - training loss (MAE): 0.7113, validation MSE: 1.9009
2025-08-24 04:27:25 [INFO]: Epoch 002 - training loss (MAE): 0.6325, validation MSE: 1.7104
2025-08-24 04:27:29 [INFO]: Epoch 003 - training loss (MAE): 0.5610, validation MSE: 1.6693
2025-08-24 04:27:32 [INFO]: Epoch 004 - training loss (MAE): 0.5246, validation MSE: 1.6145
2025-08-24 04:27:35 [INFO]: Epoch 005 - training loss (MAE): 0.5055, validation MSE: 1.5660
2025-08-24 04:27:40 [INFO]: Epoch 006 - training loss (MAE): 0.4907, validation MSE: 1.5338
2025-08-24 04:27:44 [INFO]: Epoch 007 - training loss (MAE): 0.4834, validation MSE: 1.5028
2025-08-24 04:27:48 [INFO]: Epoch 008 - training loss (MAE): 0.4787, validation MSE: 1.4795
2025-08-24 04:27:50 [INFO]: Epoch 009 - training loss (MAE): 0.4763, validation MSE: 1.4554
2025-08-24 04:27:53 [INFO]: Epoch 010 - training loss (MAE): 0.4726, validation MSE: 1.4326
2025-08-24 04:27:55 [INFO]: Epoch 011 - training loss (MAE): 0.4692, validation MSE: 1.4219
2025-08-24 04:27:58 [INFO]: Epoch 012 - training loss (MAE): 0.4672, validation MSE: 1.4044
2025-08-24 04:28:00 [INFO]: Epoch 013 - training loss (MAE): 0.4648, validation MSE: 1.3971
2025-08-24 04:28:04 [INFO]: Epoch 014 - training loss (MAE): 0.4641, validation MSE: 1.3791
2025-08-24 04:28:09 [INFO]: Epoch 015 - training loss (MAE): 0.4640, validation MSE: 1.3710
2025-08-24 04:28:09 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:28:10 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.7163| MSE: 1.1150| RMSE: 1.0559| MRE: 0.3834| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:28:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:28:23 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:28:23 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:28:23 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:28:23 [INFO]: Loaded successfully!
2025-08-24 04:28:23 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:28:23 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:28:23 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:28:24 [INFO]: Total sample number: 833
2025-08-24 04:28:24 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:28:24 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:28:24 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:28:24 [INFO]: Number of steps: 168
2025-08-24 04:28:24 [INFO]: Number of features: 370
2025-08-24 04:28:24 [INFO]: Train set missing rate: 50.00%
2025-08-24 04:28:24 [INFO]: Validating set missing rate: 50.04%
2025-08-24 04:28:24 [INFO]: Test set missing rate: 50.01%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:28:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:28:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:28:25 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:28:25 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:28:25 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:28:25 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:28:27 [INFO]: Model placed on cuda:0
2025-08-24 04:28:27 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:28:28 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.09 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.16 GiB memory in use. Of the allocated memory 720.33 MiB is allocated by PyTorch, and 165.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:28:37 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:28:37 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:28:37 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:28:37 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:28:38 [INFO]: Loaded successfully!
2025-08-24 04:28:38 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:28:38 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:28:38 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:28:39 [INFO]: Total sample number: 833
2025-08-24 04:28:39 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:28:39 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:28:39 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:28:39 [INFO]: Number of steps: 168
2025-08-24 04:28:39 [INFO]: Number of features: 370
2025-08-24 04:28:39 [INFO]: Train set missing rate: 50.00%
2025-08-24 04:28:39 [INFO]: Validating set missing rate: 50.04%
2025-08-24 04:28:39 [INFO]: Test set missing rate: 50.01%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:28:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:28:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:28:40 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:28:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:28:40 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:28:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:28:43 [INFO]: Model placed on cuda:0
2025-08-24 04:28:43 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:28:52 [INFO]: Epoch 001 - training loss (MAE): 0.7291, validation MSE: 2.1193
2025-08-24 04:28:55 [INFO]: Epoch 002 - training loss (MAE): 0.6621, validation MSE: 1.9304
2025-08-24 04:28:59 [INFO]: Epoch 003 - training loss (MAE): 0.5942, validation MSE: 1.8719
2025-08-24 04:29:05 [INFO]: Epoch 004 - training loss (MAE): 0.5553, validation MSE: 1.8524
2025-08-24 04:29:12 [INFO]: Epoch 005 - training loss (MAE): 0.5356, validation MSE: 1.8088
2025-08-24 04:29:22 [INFO]: Epoch 006 - training loss (MAE): 0.5201, validation MSE: 1.7816
2025-08-24 04:29:32 [INFO]: Epoch 007 - training loss (MAE): 0.5130, validation MSE: 1.7584
2025-08-24 04:29:40 [INFO]: Epoch 008 - training loss (MAE): 0.5082, validation MSE: 1.7389
2025-08-24 04:29:48 [INFO]: Epoch 009 - training loss (MAE): 0.5060, validation MSE: 1.7186
2025-08-24 04:29:57 [INFO]: Epoch 010 - training loss (MAE): 0.5027, validation MSE: 1.6960
2025-08-24 04:30:07 [INFO]: Epoch 011 - training loss (MAE): 0.4989, validation MSE: 1.6881
2025-08-24 04:30:15 [INFO]: Epoch 012 - training loss (MAE): 0.4971, validation MSE: 1.6740
2025-08-24 04:30:24 [INFO]: Epoch 013 - training loss (MAE): 0.4947, validation MSE: 1.6646
2025-08-24 04:30:31 [INFO]: Epoch 014 - training loss (MAE): 0.4940, validation MSE: 1.6499
2025-08-24 04:30:40 [INFO]: Epoch 015 - training loss (MAE): 0.4941, validation MSE: 1.6418
2025-08-24 04:30:40 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:30:42 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.8822| MSE: 1.6837| RMSE: 1.2976| MRE: 0.4723| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:30:58 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:30:58 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:30:58 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:30:58 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:30:58 [INFO]: Loaded successfully!
2025-08-24 04:30:58 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:30:58 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:30:58 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:30:59 [INFO]: Total sample number: 833
2025-08-24 04:30:59 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:30:59 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:30:59 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:30:59 [INFO]: Number of steps: 168
2025-08-24 04:30:59 [INFO]: Number of features: 370
2025-08-24 04:30:59 [INFO]: Train set missing rate: 50.00%
2025-08-24 04:30:59 [INFO]: Validating set missing rate: 50.04%
2025-08-24 04:30:59 [INFO]: Test set missing rate: 50.01%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:31:00 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:31:00 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:31:00 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:31:00 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:31:00 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:31:00 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:31:02 [INFO]: Model placed on cuda:0
2025-08-24 04:31:02 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:31:04 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.94 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.04 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.21 GiB memory in use. Of the allocated memory 767.21 MiB is allocated by PyTorch, and 166.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=168, N_FEATURES=370)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:31:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:31:20 [INFO]: You're using dataset electricity_load_diagrams, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/electricity_load_diagrams
2025-08-24 04:31:20 [INFO]: Dataset electricity_load_diagrams has already been downloaded. Processing directly...
2025-08-24 04:31:20 [INFO]: Dataset electricity_load_diagrams has already been cached. Loading from cache directly...
2025-08-24 04:31:20 [INFO]: Loaded successfully!
2025-08-24 04:31:20 [INFO]: months selected as test set are <PeriodArray>
['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07',
 '2011-08', '2011-09', '2011-10']
Length: 10, dtype: period[M]
2025-08-24 04:31:20 [INFO]: months selected as val set are <PeriodArray>
['2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05',
 '2012-06', '2012-07', '2012-08']
Length: 10, dtype: period[M]
2025-08-24 04:31:20 [INFO]: months selected as train set are <PeriodArray>
['2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03',
 '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10',
 '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05',
 '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12',
 '2015-01']
Length: 29, dtype: period[M]
2025-08-24 04:31:21 [INFO]: Total sample number: 833
2025-08-24 04:31:21 [INFO]: Training set size: 486 (58.34%)
2025-08-24 04:31:21 [INFO]: Validation set size: 174 (20.89%)
2025-08-24 04:31:21 [INFO]: Test set size: 173 (20.77%)
2025-08-24 04:31:21 [INFO]: Number of steps: 168
2025-08-24 04:31:21 [INFO]: Number of features: 370
2025-08-24 04:31:21 [INFO]: Train set missing rate: 50.00%
2025-08-24 04:31:21 [INFO]: Validating set missing rate: 50.04%
2025-08-24 04:31:21 [INFO]: Test set missing rate: 50.01%
âœ… Dataset 'electricity_load_diagrams' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:31:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:31:22 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:31:22 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:31:22 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:31:22 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:31:22 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:31:28 [INFO]: Model placed on cuda:0
2025-08-24 04:31:28 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:31:47 [INFO]: Epoch 001 - training loss (MAE): 0.7291, validation MSE: 2.1193
2025-08-24 04:32:07 [INFO]: Epoch 002 - training loss (MAE): 0.6621, validation MSE: 1.9304
2025-08-24 04:32:26 [INFO]: Epoch 003 - training loss (MAE): 0.5942, validation MSE: 1.8719
2025-08-24 04:32:42 [INFO]: Epoch 004 - training loss (MAE): 0.5553, validation MSE: 1.8524
2025-08-24 04:32:56 [INFO]: Epoch 005 - training loss (MAE): 0.5356, validation MSE: 1.8088
2025-08-24 04:33:18 [INFO]: Epoch 006 - training loss (MAE): 0.5201, validation MSE: 1.7816
2025-08-24 04:33:36 [INFO]: Epoch 007 - training loss (MAE): 0.5130, validation MSE: 1.7584
2025-08-24 04:33:54 [INFO]: Epoch 008 - training loss (MAE): 0.5082, validation MSE: 1.7389
2025-08-24 04:34:11 [INFO]: Epoch 009 - training loss (MAE): 0.5060, validation MSE: 1.7186
2025-08-24 04:34:31 [INFO]: Epoch 010 - training loss (MAE): 0.5027, validation MSE: 1.6960
2025-08-24 04:34:48 [INFO]: Epoch 011 - training loss (MAE): 0.4989, validation MSE: 1.6881
2025-08-24 04:35:03 [INFO]: Epoch 012 - training loss (MAE): 0.4971, validation MSE: 1.6740
2025-08-24 04:35:18 [INFO]: Epoch 013 - training loss (MAE): 0.4947, validation MSE: 1.6646
2025-08-24 04:35:36 [INFO]: Epoch 014 - training loss (MAE): 0.4940, validation MSE: 1.6499
2025-08-24 04:35:50 [INFO]: Epoch 015 - training loss (MAE): 0.4941, validation MSE: 1.6418
2025-08-24 04:35:50 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:35:52 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.8822| MSE: 1.6837| RMSE: 1.2976| MRE: 0.4723| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/electricity_load_diagrams/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: electricity_load_diagrams | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:36:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:36:10 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:36:10 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:36:10 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:36:10 [INFO]: Loaded successfully!
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:10 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:36:10 [INFO]: Original df missing rate: 0.016
2025-08-24 04:36:10 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:36:10 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:36:10 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:36:10 [INFO]: Total sample number: 730
2025-08-24 04:36:10 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:36:10 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:36:10 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:36:10 [INFO]: Number of steps: 48
2025-08-24 04:36:10 [INFO]: Number of features: 132
2025-08-24 04:36:10 [INFO]: Train set missing rate: 11.67%
2025-08-24 04:36:10 [INFO]: Validating set missing rate: 10.80%
2025-08-24 04:36:10 [INFO]: Test set missing rate: 11.13%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:36:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:36:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:36:11 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:36:11 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:36:11 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:36:11 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:36:14 [INFO]: Model placed on cuda:0
2025-08-24 04:36:14 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:36:14 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:36:30 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:36:30 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:36:30 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:36:30 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:36:30 [INFO]: Loaded successfully!
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:31 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:36:31 [INFO]: Original df missing rate: 0.016
2025-08-24 04:36:31 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:36:31 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:36:31 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:36:31 [INFO]: Total sample number: 730
2025-08-24 04:36:31 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:36:31 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:36:31 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:36:31 [INFO]: Number of steps: 48
2025-08-24 04:36:31 [INFO]: Number of features: 132
2025-08-24 04:36:31 [INFO]: Train set missing rate: 11.67%
2025-08-24 04:36:31 [INFO]: Validating set missing rate: 10.80%
2025-08-24 04:36:31 [INFO]: Test set missing rate: 11.13%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:36:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:36:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:36:31 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:36:31 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:36:31 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:36:31 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:36:37 [INFO]: Model placed on cuda:0
2025-08-24 04:36:37 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:36:37 [INFO]: Epoch 001 - training loss (MAE): 0.4383, validation MSE: 0.3977
2025-08-24 04:36:38 [INFO]: Epoch 002 - training loss (MAE): 0.3782, validation MSE: 0.3002
2025-08-24 04:36:38 [INFO]: Epoch 003 - training loss (MAE): 0.3351, validation MSE: 0.2594
2025-08-24 04:36:38 [INFO]: Epoch 004 - training loss (MAE): 0.3101, validation MSE: 0.2423
2025-08-24 04:36:38 [INFO]: Epoch 005 - training loss (MAE): 0.2952, validation MSE: 0.2357
2025-08-24 04:36:38 [INFO]: Epoch 006 - training loss (MAE): 0.2871, validation MSE: 0.2323
2025-08-24 04:36:39 [INFO]: Epoch 007 - training loss (MAE): 0.2790, validation MSE: 0.2317
2025-08-24 04:36:39 [INFO]: Epoch 008 - training loss (MAE): 0.2787, validation MSE: 0.2311
2025-08-24 04:36:39 [INFO]: Epoch 009 - training loss (MAE): 0.2765, validation MSE: 0.2300
2025-08-24 04:36:39 [INFO]: Epoch 010 - training loss (MAE): 0.2734, validation MSE: 0.2302
2025-08-24 04:36:39 [INFO]: Epoch 011 - training loss (MAE): 0.2707, validation MSE: 0.2304
2025-08-24 04:36:40 [INFO]: Epoch 012 - training loss (MAE): 0.2716, validation MSE: 0.2306
2025-08-24 04:36:40 [INFO]: Epoch 013 - training loss (MAE): 0.2705, validation MSE: 0.2299
2025-08-24 04:36:40 [INFO]: Epoch 014 - training loss (MAE): 0.2695, validation MSE: 0.2294
2025-08-24 04:36:40 [INFO]: Epoch 015 - training loss (MAE): 0.2681, validation MSE: 0.2302
2025-08-24 04:36:40 [INFO]: Finished training. The best model is from epoch#14.
2025-08-24 04:36:41 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2159| MSE: 0.1904| RMSE: 0.4363| MRE: 0.2868| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:36:58 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:36:58 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:36:58 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:36:58 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:36:58 [INFO]: Loaded successfully!
2025-08-24 04:36:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:36:59 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:36:59 [INFO]: Original df missing rate: 0.016
2025-08-24 04:36:59 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:36:59 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:36:59 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:36:59 [INFO]: Total sample number: 730
2025-08-24 04:36:59 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:36:59 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:36:59 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:36:59 [INFO]: Number of steps: 48
2025-08-24 04:36:59 [INFO]: Number of features: 132
2025-08-24 04:36:59 [INFO]: Train set missing rate: 11.67%
2025-08-24 04:36:59 [INFO]: Validating set missing rate: 10.80%
2025-08-24 04:36:59 [INFO]: Test set missing rate: 11.13%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:36:59 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:36:59 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:36:59 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:36:59 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:36:59 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:36:59 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:37:03 [INFO]: Model placed on cuda:0
2025-08-24 04:37:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:37:04 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:37:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:37:23 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:37:23 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:37:23 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:37:23 [INFO]: Loaded successfully!
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:23 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:37:23 [INFO]: Original df missing rate: 0.016
2025-08-24 04:37:23 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:37:23 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:37:23 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:37:23 [INFO]: Total sample number: 730
2025-08-24 04:37:23 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:37:23 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:37:23 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:37:23 [INFO]: Number of steps: 48
2025-08-24 04:37:23 [INFO]: Number of features: 132
2025-08-24 04:37:23 [INFO]: Train set missing rate: 11.67%
2025-08-24 04:37:23 [INFO]: Validating set missing rate: 10.80%
2025-08-24 04:37:23 [INFO]: Test set missing rate: 11.13%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:37:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:37:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:37:24 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:37:24 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:37:24 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:37:24 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:37:28 [INFO]: Model placed on cuda:0
2025-08-24 04:37:28 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:37:29 [INFO]: Epoch 001 - training loss (MAE): 0.4383, validation MSE: 0.3977
2025-08-24 04:37:29 [INFO]: Epoch 002 - training loss (MAE): 0.3782, validation MSE: 0.3002
2025-08-24 04:37:29 [INFO]: Epoch 003 - training loss (MAE): 0.3351, validation MSE: 0.2594
2025-08-24 04:37:29 [INFO]: Epoch 004 - training loss (MAE): 0.3101, validation MSE: 0.2423
2025-08-24 04:37:30 [INFO]: Epoch 005 - training loss (MAE): 0.2952, validation MSE: 0.2357
2025-08-24 04:37:30 [INFO]: Epoch 006 - training loss (MAE): 0.2871, validation MSE: 0.2323
2025-08-24 04:37:30 [INFO]: Epoch 007 - training loss (MAE): 0.2790, validation MSE: 0.2317
2025-08-24 04:37:30 [INFO]: Epoch 008 - training loss (MAE): 0.2787, validation MSE: 0.2311
2025-08-24 04:37:31 [INFO]: Epoch 009 - training loss (MAE): 0.2765, validation MSE: 0.2300
2025-08-24 04:37:31 [INFO]: Epoch 010 - training loss (MAE): 0.2734, validation MSE: 0.2302
2025-08-24 04:37:31 [INFO]: Epoch 011 - training loss (MAE): 0.2707, validation MSE: 0.2304
2025-08-24 04:37:31 [INFO]: Epoch 012 - training loss (MAE): 0.2716, validation MSE: 0.2306
2025-08-24 04:37:32 [INFO]: Epoch 013 - training loss (MAE): 0.2705, validation MSE: 0.2299
2025-08-24 04:37:32 [INFO]: Epoch 014 - training loss (MAE): 0.2695, validation MSE: 0.2294
2025-08-24 04:37:32 [INFO]: Epoch 015 - training loss (MAE): 0.2681, validation MSE: 0.2302
2025-08-24 04:37:32 [INFO]: Finished training. The best model is from epoch#14.
2025-08-24 04:37:33 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2159| MSE: 0.1904| RMSE: 0.4363| MRE: 0.2868| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:37:51 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:37:51 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:37:51 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:37:51 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:37:51 [INFO]: Loaded successfully!
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:37:52 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:37:52 [INFO]: Original df missing rate: 0.016
2025-08-24 04:37:52 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:37:52 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:37:52 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:37:52 [INFO]: Total sample number: 730
2025-08-24 04:37:52 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:37:52 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:37:52 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:37:52 [INFO]: Number of steps: 48
2025-08-24 04:37:52 [INFO]: Number of features: 132
2025-08-24 04:37:52 [INFO]: Train set missing rate: 21.51%
2025-08-24 04:37:52 [INFO]: Validating set missing rate: 20.70%
2025-08-24 04:37:52 [INFO]: Test set missing rate: 20.98%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:37:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:37:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:37:52 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:37:52 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:37:52 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:37:52 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:37:57 [INFO]: Model placed on cuda:0
2025-08-24 04:37:57 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:37:57 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:38:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:38:16 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:38:16 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:38:16 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:38:16 [INFO]: Loaded successfully!
2025-08-24 04:38:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:17 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:38:17 [INFO]: Original df missing rate: 0.016
2025-08-24 04:38:17 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:38:17 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:38:17 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:38:17 [INFO]: Total sample number: 730
2025-08-24 04:38:17 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:38:17 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:38:17 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:38:17 [INFO]: Number of steps: 48
2025-08-24 04:38:17 [INFO]: Number of features: 132
2025-08-24 04:38:17 [INFO]: Train set missing rate: 21.51%
2025-08-24 04:38:17 [INFO]: Validating set missing rate: 20.70%
2025-08-24 04:38:17 [INFO]: Test set missing rate: 20.98%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:38:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:38:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:38:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:38:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:38:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:38:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:38:22 [INFO]: Model placed on cuda:0
2025-08-24 04:38:22 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:38:23 [INFO]: Epoch 001 - training loss (MAE): 0.4630, validation MSE: 0.4168
2025-08-24 04:38:23 [INFO]: Epoch 002 - training loss (MAE): 0.4102, validation MSE: 0.3184
2025-08-24 04:38:23 [INFO]: Epoch 003 - training loss (MAE): 0.3699, validation MSE: 0.2739
2025-08-24 04:38:23 [INFO]: Epoch 004 - training loss (MAE): 0.3430, validation MSE: 0.2506
2025-08-24 04:38:24 [INFO]: Epoch 005 - training loss (MAE): 0.3253, validation MSE: 0.2401
2025-08-24 04:38:24 [INFO]: Epoch 006 - training loss (MAE): 0.3151, validation MSE: 0.2339
2025-08-24 04:38:24 [INFO]: Epoch 007 - training loss (MAE): 0.3048, validation MSE: 0.2321
2025-08-24 04:38:24 [INFO]: Epoch 008 - training loss (MAE): 0.3029, validation MSE: 0.2316
2025-08-24 04:38:25 [INFO]: Epoch 009 - training loss (MAE): 0.2994, validation MSE: 0.2305
2025-08-24 04:38:25 [INFO]: Epoch 010 - training loss (MAE): 0.2954, validation MSE: 0.2311
2025-08-24 04:38:25 [INFO]: Epoch 011 - training loss (MAE): 0.2921, validation MSE: 0.2308
2025-08-24 04:38:25 [INFO]: Epoch 012 - training loss (MAE): 0.2928, validation MSE: 0.2309
2025-08-24 04:38:25 [INFO]: Epoch 013 - training loss (MAE): 0.2912, validation MSE: 0.2315
2025-08-24 04:38:26 [INFO]: Epoch 014 - training loss (MAE): 0.2896, validation MSE: 0.2317
2025-08-24 04:38:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-24 04:38:26 [INFO]: Finished training. The best model is from epoch#9.
2025-08-24 04:38:27 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2279| MSE: 0.2399| RMSE: 0.4898| MRE: 0.3022| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:38:52 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:38:52 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:38:52 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:38:52 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:38:52 [INFO]: Loaded successfully!
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:38:52 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:38:52 [INFO]: Original df missing rate: 0.016
2025-08-24 04:38:52 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:38:52 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:38:52 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:38:52 [INFO]: Total sample number: 730
2025-08-24 04:38:52 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:38:52 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:38:52 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:38:52 [INFO]: Number of steps: 48
2025-08-24 04:38:52 [INFO]: Number of features: 132
2025-08-24 04:38:52 [INFO]: Train set missing rate: 21.51%
2025-08-24 04:38:52 [INFO]: Validating set missing rate: 20.70%
2025-08-24 04:38:52 [INFO]: Test set missing rate: 20.98%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:38:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:38:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:38:53 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:38:53 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:38:53 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:38:53 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:38:58 [INFO]: Model placed on cuda:0
2025-08-24 04:38:58 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:38:58 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:39:17 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:39:17 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:39:17 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:39:17 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:39:17 [INFO]: Loaded successfully!
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:17 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:39:17 [INFO]: Original df missing rate: 0.016
2025-08-24 04:39:17 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:39:17 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:39:17 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:39:18 [INFO]: Total sample number: 730
2025-08-24 04:39:18 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:39:18 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:39:18 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:39:18 [INFO]: Number of steps: 48
2025-08-24 04:39:18 [INFO]: Number of features: 132
2025-08-24 04:39:18 [INFO]: Train set missing rate: 21.51%
2025-08-24 04:39:18 [INFO]: Validating set missing rate: 20.70%
2025-08-24 04:39:18 [INFO]: Test set missing rate: 20.98%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:39:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:39:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:39:18 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:39:18 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:39:18 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:39:18 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:39:23 [INFO]: Model placed on cuda:0
2025-08-24 04:39:23 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:39:24 [INFO]: Epoch 001 - training loss (MAE): 0.4630, validation MSE: 0.4168
2025-08-24 04:39:24 [INFO]: Epoch 002 - training loss (MAE): 0.4102, validation MSE: 0.3184
2025-08-24 04:39:24 [INFO]: Epoch 003 - training loss (MAE): 0.3699, validation MSE: 0.2739
2025-08-24 04:39:25 [INFO]: Epoch 004 - training loss (MAE): 0.3430, validation MSE: 0.2506
2025-08-24 04:39:25 [INFO]: Epoch 005 - training loss (MAE): 0.3253, validation MSE: 0.2401
2025-08-24 04:39:25 [INFO]: Epoch 006 - training loss (MAE): 0.3151, validation MSE: 0.2339
2025-08-24 04:39:25 [INFO]: Epoch 007 - training loss (MAE): 0.3048, validation MSE: 0.2321
2025-08-24 04:39:25 [INFO]: Epoch 008 - training loss (MAE): 0.3029, validation MSE: 0.2316
2025-08-24 04:39:26 [INFO]: Epoch 009 - training loss (MAE): 0.2994, validation MSE: 0.2305
2025-08-24 04:39:26 [INFO]: Epoch 010 - training loss (MAE): 0.2954, validation MSE: 0.2311
2025-08-24 04:39:26 [INFO]: Epoch 011 - training loss (MAE): 0.2921, validation MSE: 0.2308
2025-08-24 04:39:26 [INFO]: Epoch 012 - training loss (MAE): 0.2928, validation MSE: 0.2309
2025-08-24 04:39:27 [INFO]: Epoch 013 - training loss (MAE): 0.2912, validation MSE: 0.2315
2025-08-24 04:39:27 [INFO]: Epoch 014 - training loss (MAE): 0.2896, validation MSE: 0.2317
2025-08-24 04:39:27 [INFO]: Exceeded the training patience. Terminating the training procedure...
2025-08-24 04:39:27 [INFO]: Finished training. The best model is from epoch#9.
2025-08-24 04:39:28 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2279| MSE: 0.2399| RMSE: 0.4898| MRE: 0.3022| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:39:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:39:49 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:39:49 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:39:49 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:39:49 [INFO]: Loaded successfully!
2025-08-24 04:39:49 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:39:50 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:39:50 [INFO]: Original df missing rate: 0.016
2025-08-24 04:39:50 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:39:50 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:39:50 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:39:50 [INFO]: Total sample number: 730
2025-08-24 04:39:50 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:39:50 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:39:50 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:39:50 [INFO]: Number of steps: 48
2025-08-24 04:39:50 [INFO]: Number of features: 132
2025-08-24 04:39:50 [INFO]: Train set missing rate: 31.34%
2025-08-24 04:39:50 [INFO]: Validating set missing rate: 30.61%
2025-08-24 04:39:50 [INFO]: Test set missing rate: 30.86%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:39:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:39:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:39:50 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:39:50 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:39:50 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:39:50 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:39:55 [INFO]: Model placed on cuda:0
2025-08-24 04:39:55 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:39:56 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:40:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:40:16 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:40:16 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:40:16 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:40:16 [INFO]: Loaded successfully!
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:17 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:40:17 [INFO]: Original df missing rate: 0.016
2025-08-24 04:40:17 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:40:17 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:40:17 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:40:17 [INFO]: Total sample number: 730
2025-08-24 04:40:17 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:40:17 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:40:17 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:40:17 [INFO]: Number of steps: 48
2025-08-24 04:40:17 [INFO]: Number of features: 132
2025-08-24 04:40:17 [INFO]: Train set missing rate: 31.34%
2025-08-24 04:40:17 [INFO]: Validating set missing rate: 30.61%
2025-08-24 04:40:17 [INFO]: Test set missing rate: 30.86%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:40:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:40:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:40:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:40:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:40:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:40:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:40:23 [INFO]: Model placed on cuda:0
2025-08-24 04:40:23 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:40:23 [INFO]: Epoch 001 - training loss (MAE): 0.4900, validation MSE: 0.4695
2025-08-24 04:40:23 [INFO]: Epoch 002 - training loss (MAE): 0.4445, validation MSE: 0.3731
2025-08-24 04:40:24 [INFO]: Epoch 003 - training loss (MAE): 0.4097, validation MSE: 0.3241
2025-08-24 04:40:24 [INFO]: Epoch 004 - training loss (MAE): 0.3806, validation MSE: 0.2940
2025-08-24 04:40:24 [INFO]: Epoch 005 - training loss (MAE): 0.3618, validation MSE: 0.2799
2025-08-24 04:40:24 [INFO]: Epoch 006 - training loss (MAE): 0.3489, validation MSE: 0.2696
2025-08-24 04:40:24 [INFO]: Epoch 007 - training loss (MAE): 0.3360, validation MSE: 0.2641
2025-08-24 04:40:25 [INFO]: Epoch 008 - training loss (MAE): 0.3321, validation MSE: 0.2618
2025-08-24 04:40:25 [INFO]: Epoch 009 - training loss (MAE): 0.3270, validation MSE: 0.2587
2025-08-24 04:40:25 [INFO]: Epoch 010 - training loss (MAE): 0.3209, validation MSE: 0.2585
2025-08-24 04:40:25 [INFO]: Epoch 011 - training loss (MAE): 0.3165, validation MSE: 0.2574
2025-08-24 04:40:26 [INFO]: Epoch 012 - training loss (MAE): 0.3163, validation MSE: 0.2569
2025-08-24 04:40:26 [INFO]: Epoch 013 - training loss (MAE): 0.3130, validation MSE: 0.2578
2025-08-24 04:40:26 [INFO]: Epoch 014 - training loss (MAE): 0.3119, validation MSE: 0.2579
2025-08-24 04:40:26 [INFO]: Epoch 015 - training loss (MAE): 0.3100, validation MSE: 0.2578
2025-08-24 04:40:26 [INFO]: Finished training. The best model is from epoch#12.
2025-08-24 04:40:28 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2453| MSE: 0.2394| RMSE: 0.4893| MRE: 0.3259| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:40:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:40:50 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:40:50 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:40:50 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:40:50 [INFO]: Loaded successfully!
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:40:50 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:40:50 [INFO]: Original df missing rate: 0.016
2025-08-24 04:40:50 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:40:50 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:40:50 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:40:50 [INFO]: Total sample number: 730
2025-08-24 04:40:50 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:40:50 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:40:50 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:40:50 [INFO]: Number of steps: 48
2025-08-24 04:40:50 [INFO]: Number of features: 132
2025-08-24 04:40:50 [INFO]: Train set missing rate: 31.34%
2025-08-24 04:40:50 [INFO]: Validating set missing rate: 30.61%
2025-08-24 04:40:50 [INFO]: Test set missing rate: 30.86%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:40:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:40:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:40:51 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:40:51 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:40:51 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:40:51 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:40:56 [INFO]: Model placed on cuda:0
2025-08-24 04:40:56 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:40:57 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:41:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:41:16 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:41:16 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:41:16 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:41:16 [INFO]: Loaded successfully!
2025-08-24 04:41:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:16 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:17 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:41:17 [INFO]: Original df missing rate: 0.016
2025-08-24 04:41:17 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:41:17 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:41:17 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:41:17 [INFO]: Total sample number: 730
2025-08-24 04:41:17 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:41:17 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:41:17 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:41:17 [INFO]: Number of steps: 48
2025-08-24 04:41:17 [INFO]: Number of features: 132
2025-08-24 04:41:17 [INFO]: Train set missing rate: 31.34%
2025-08-24 04:41:17 [INFO]: Validating set missing rate: 30.61%
2025-08-24 04:41:17 [INFO]: Test set missing rate: 30.86%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:41:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:41:17 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:41:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:41:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:41:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:41:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:41:23 [INFO]: Model placed on cuda:0
2025-08-24 04:41:23 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:41:23 [INFO]: Epoch 001 - training loss (MAE): 0.4900, validation MSE: 0.4695
2025-08-24 04:41:24 [INFO]: Epoch 002 - training loss (MAE): 0.4445, validation MSE: 0.3731
2025-08-24 04:41:24 [INFO]: Epoch 003 - training loss (MAE): 0.4097, validation MSE: 0.3241
2025-08-24 04:41:24 [INFO]: Epoch 004 - training loss (MAE): 0.3806, validation MSE: 0.2940
2025-08-24 04:41:24 [INFO]: Epoch 005 - training loss (MAE): 0.3618, validation MSE: 0.2799
2025-08-24 04:41:25 [INFO]: Epoch 006 - training loss (MAE): 0.3489, validation MSE: 0.2696
2025-08-24 04:41:25 [INFO]: Epoch 007 - training loss (MAE): 0.3360, validation MSE: 0.2641
2025-08-24 04:41:25 [INFO]: Epoch 008 - training loss (MAE): 0.3321, validation MSE: 0.2618
2025-08-24 04:41:25 [INFO]: Epoch 009 - training loss (MAE): 0.3270, validation MSE: 0.2587
2025-08-24 04:41:26 [INFO]: Epoch 010 - training loss (MAE): 0.3209, validation MSE: 0.2585
2025-08-24 04:41:26 [INFO]: Epoch 011 - training loss (MAE): 0.3165, validation MSE: 0.2574
2025-08-24 04:41:26 [INFO]: Epoch 012 - training loss (MAE): 0.3163, validation MSE: 0.2569
2025-08-24 04:41:26 [INFO]: Epoch 013 - training loss (MAE): 0.3130, validation MSE: 0.2578
2025-08-24 04:41:26 [INFO]: Epoch 014 - training loss (MAE): 0.3119, validation MSE: 0.2579
2025-08-24 04:41:27 [INFO]: Epoch 015 - training loss (MAE): 0.3100, validation MSE: 0.2578
2025-08-24 04:41:27 [INFO]: Finished training. The best model is from epoch#12.
2025-08-24 04:41:28 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2453| MSE: 0.2394| RMSE: 0.4893| MRE: 0.3259| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:41:51 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:41:51 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:41:51 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:41:51 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:41:51 [INFO]: Loaded successfully!
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:41:51 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:41:51 [INFO]: Original df missing rate: 0.016
2025-08-24 04:41:51 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:41:51 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:41:51 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:41:51 [INFO]: Total sample number: 730
2025-08-24 04:41:51 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:41:51 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:41:51 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:41:51 [INFO]: Number of steps: 48
2025-08-24 04:41:51 [INFO]: Number of features: 132
2025-08-24 04:41:51 [INFO]: Train set missing rate: 41.13%
2025-08-24 04:41:51 [INFO]: Validating set missing rate: 40.51%
2025-08-24 04:41:51 [INFO]: Test set missing rate: 40.77%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:41:51 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:41:51 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:41:52 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:41:52 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:41:52 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:41:52 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:41:58 [INFO]: Model placed on cuda:0
2025-08-24 04:41:58 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:41:58 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:42:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:42:20 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:42:20 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:42:20 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:42:21 [INFO]: Loaded successfully!
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:21 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:42:21 [INFO]: Original df missing rate: 0.016
2025-08-24 04:42:21 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:42:21 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:42:21 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:42:21 [INFO]: Total sample number: 730
2025-08-24 04:42:21 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:42:21 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:42:21 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:42:21 [INFO]: Number of steps: 48
2025-08-24 04:42:21 [INFO]: Number of features: 132
2025-08-24 04:42:21 [INFO]: Train set missing rate: 41.13%
2025-08-24 04:42:21 [INFO]: Validating set missing rate: 40.51%
2025-08-24 04:42:21 [INFO]: Test set missing rate: 40.77%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:42:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:42:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:42:21 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:42:21 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:42:21 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:42:21 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:42:28 [INFO]: Model placed on cuda:0
2025-08-24 04:42:28 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:42:29 [INFO]: Epoch 001 - training loss (MAE): 0.5186, validation MSE: 0.5448
2025-08-24 04:42:29 [INFO]: Epoch 002 - training loss (MAE): 0.4813, validation MSE: 0.4525
2025-08-24 04:42:29 [INFO]: Epoch 003 - training loss (MAE): 0.4523, validation MSE: 0.3988
2025-08-24 04:42:29 [INFO]: Epoch 004 - training loss (MAE): 0.4238, validation MSE: 0.3612
2025-08-24 04:42:30 [INFO]: Epoch 005 - training loss (MAE): 0.4031, validation MSE: 0.3429
2025-08-24 04:42:30 [INFO]: Epoch 006 - training loss (MAE): 0.3892, validation MSE: 0.3266
2025-08-24 04:42:30 [INFO]: Epoch 007 - training loss (MAE): 0.3745, validation MSE: 0.3153
2025-08-24 04:42:30 [INFO]: Epoch 008 - training loss (MAE): 0.3689, validation MSE: 0.3088
2025-08-24 04:42:31 [INFO]: Epoch 009 - training loss (MAE): 0.3611, validation MSE: 0.3007
2025-08-24 04:42:31 [INFO]: Epoch 010 - training loss (MAE): 0.3539, validation MSE: 0.2968
2025-08-24 04:42:31 [INFO]: Epoch 011 - training loss (MAE): 0.3472, validation MSE: 0.2934
2025-08-24 04:42:31 [INFO]: Epoch 012 - training loss (MAE): 0.3453, validation MSE: 0.2909
2025-08-24 04:42:32 [INFO]: Epoch 013 - training loss (MAE): 0.3401, validation MSE: 0.2899
2025-08-24 04:42:32 [INFO]: Epoch 014 - training loss (MAE): 0.3386, validation MSE: 0.2891
2025-08-24 04:42:32 [INFO]: Epoch 015 - training loss (MAE): 0.3353, validation MSE: 0.2884
2025-08-24 04:42:32 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:42:34 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2646| MSE: 0.2601| RMSE: 0.5100| MRE: 0.3517| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:42:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:42:57 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:42:57 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:42:57 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:42:57 [INFO]: Loaded successfully!
2025-08-24 04:42:57 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:42:58 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:42:58 [INFO]: Original df missing rate: 0.016
2025-08-24 04:42:58 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:42:58 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:42:58 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:42:58 [INFO]: Total sample number: 730
2025-08-24 04:42:58 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:42:58 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:42:58 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:42:58 [INFO]: Number of steps: 48
2025-08-24 04:42:58 [INFO]: Number of features: 132
2025-08-24 04:42:58 [INFO]: Train set missing rate: 41.13%
2025-08-24 04:42:58 [INFO]: Validating set missing rate: 40.51%
2025-08-24 04:42:58 [INFO]: Test set missing rate: 40.77%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:42:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:42:58 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:42:58 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:42:58 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:42:58 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:42:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:43:04 [INFO]: Model placed on cuda:0
2025-08-24 04:43:04 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:43:05 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:43:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:43:26 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:43:26 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:43:26 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:43:26 [INFO]: Loaded successfully!
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:26 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:43:27 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:43:27 [INFO]: Original df missing rate: 0.016
2025-08-24 04:43:27 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:43:27 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:43:27 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:43:27 [INFO]: Total sample number: 730
2025-08-24 04:43:27 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:43:27 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:43:27 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:43:27 [INFO]: Number of steps: 48
2025-08-24 04:43:27 [INFO]: Number of features: 132
2025-08-24 04:43:27 [INFO]: Train set missing rate: 41.13%
2025-08-24 04:43:27 [INFO]: Validating set missing rate: 40.51%
2025-08-24 04:43:27 [INFO]: Test set missing rate: 40.77%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:43:27 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:43:27 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:43:27 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:43:27 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:43:27 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:43:27 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:43:34 [INFO]: Model placed on cuda:0
2025-08-24 04:43:34 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:43:34 [INFO]: Epoch 001 - training loss (MAE): 0.5186, validation MSE: 0.5448
2025-08-24 04:43:34 [INFO]: Epoch 002 - training loss (MAE): 0.4813, validation MSE: 0.4525
2025-08-24 04:43:35 [INFO]: Epoch 003 - training loss (MAE): 0.4523, validation MSE: 0.3988
2025-08-24 04:43:35 [INFO]: Epoch 004 - training loss (MAE): 0.4238, validation MSE: 0.3612
2025-08-24 04:43:35 [INFO]: Epoch 005 - training loss (MAE): 0.4031, validation MSE: 0.3429
2025-08-24 04:43:35 [INFO]: Epoch 006 - training loss (MAE): 0.3892, validation MSE: 0.3266
2025-08-24 04:43:36 [INFO]: Epoch 007 - training loss (MAE): 0.3745, validation MSE: 0.3153
2025-08-24 04:43:36 [INFO]: Epoch 008 - training loss (MAE): 0.3689, validation MSE: 0.3088
2025-08-24 04:43:36 [INFO]: Epoch 009 - training loss (MAE): 0.3611, validation MSE: 0.3007
2025-08-24 04:43:36 [INFO]: Epoch 010 - training loss (MAE): 0.3539, validation MSE: 0.2968
2025-08-24 04:43:36 [INFO]: Epoch 011 - training loss (MAE): 0.3472, validation MSE: 0.2934
2025-08-24 04:43:37 [INFO]: Epoch 012 - training loss (MAE): 0.3453, validation MSE: 0.2909
2025-08-24 04:43:37 [INFO]: Epoch 013 - training loss (MAE): 0.3401, validation MSE: 0.2899
2025-08-24 04:43:37 [INFO]: Epoch 014 - training loss (MAE): 0.3386, validation MSE: 0.2891
2025-08-24 04:43:37 [INFO]: Epoch 015 - training loss (MAE): 0.3353, validation MSE: 0.2884
2025-08-24 04:43:37 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:43:39 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2646| MSE: 0.2601| RMSE: 0.5100| MRE: 0.3517| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:44:01 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:44:01 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:44:01 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:44:01 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:44:01 [INFO]: Loaded successfully!
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:01 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:44:01 [INFO]: Original df missing rate: 0.016
2025-08-24 04:44:01 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:44:01 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:44:01 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:44:01 [INFO]: Total sample number: 730
2025-08-24 04:44:01 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:44:01 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:44:01 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:44:01 [INFO]: Number of steps: 48
2025-08-24 04:44:01 [INFO]: Number of features: 132
2025-08-24 04:44:01 [INFO]: Train set missing rate: 50.93%
2025-08-24 04:44:01 [INFO]: Validating set missing rate: 50.44%
2025-08-24 04:44:01 [INFO]: Test set missing rate: 50.71%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:44:01 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:44:01 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:44:02 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:44:02 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:44:02 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:44:02 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:44:08 [INFO]: Model placed on cuda:0
2025-08-24 04:44:08 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:44:09 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.75 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.50 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 66.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:44:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:44:31 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:44:31 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:44:31 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:44:31 [INFO]: Loaded successfully!
2025-08-24 04:44:31 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:44:32 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:44:32 [INFO]: Original df missing rate: 0.016
2025-08-24 04:44:32 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:44:32 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:44:32 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:44:32 [INFO]: Total sample number: 730
2025-08-24 04:44:32 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:44:32 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:44:32 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:44:32 [INFO]: Number of steps: 48
2025-08-24 04:44:32 [INFO]: Number of features: 132
2025-08-24 04:44:32 [INFO]: Train set missing rate: 50.93%
2025-08-24 04:44:32 [INFO]: Validating set missing rate: 50.44%
2025-08-24 04:44:32 [INFO]: Test set missing rate: 50.71%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:44:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:44:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:44:32 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:44:32 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:44:32 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:44:32 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:44:39 [INFO]: Model placed on cuda:0
2025-08-24 04:44:39 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:44:40 [INFO]: Epoch 001 - training loss (MAE): 0.5492, validation MSE: 0.6104
2025-08-24 04:44:40 [INFO]: Epoch 002 - training loss (MAE): 0.5191, validation MSE: 0.5275
2025-08-24 04:44:40 [INFO]: Epoch 003 - training loss (MAE): 0.4970, validation MSE: 0.4749
2025-08-24 04:44:40 [INFO]: Epoch 004 - training loss (MAE): 0.4718, validation MSE: 0.4323
2025-08-24 04:44:41 [INFO]: Epoch 005 - training loss (MAE): 0.4509, validation MSE: 0.4107
2025-08-24 04:44:41 [INFO]: Epoch 006 - training loss (MAE): 0.4377, validation MSE: 0.3887
2025-08-24 04:44:41 [INFO]: Epoch 007 - training loss (MAE): 0.4209, validation MSE: 0.3751
2025-08-24 04:44:41 [INFO]: Epoch 008 - training loss (MAE): 0.4147, validation MSE: 0.3630
2025-08-24 04:44:41 [INFO]: Epoch 009 - training loss (MAE): 0.4059, validation MSE: 0.3492
2025-08-24 04:44:42 [INFO]: Epoch 010 - training loss (MAE): 0.3974, validation MSE: 0.3411
2025-08-24 04:44:42 [INFO]: Epoch 011 - training loss (MAE): 0.3880, validation MSE: 0.3324
2025-08-24 04:44:42 [INFO]: Epoch 012 - training loss (MAE): 0.3851, validation MSE: 0.3262
2025-08-24 04:44:42 [INFO]: Epoch 013 - training loss (MAE): 0.3780, validation MSE: 0.3211
2025-08-24 04:44:43 [INFO]: Epoch 014 - training loss (MAE): 0.3738, validation MSE: 0.3172
2025-08-24 04:44:43 [INFO]: Epoch 015 - training loss (MAE): 0.3690, validation MSE: 0.3133
2025-08-24 04:44:43 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:44:45 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2915| MSE: 0.2967| RMSE: 0.5447| MRE: 0.3874| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:45:08 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:45:08 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:45:08 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:45:08 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:45:08 [INFO]: Loaded successfully!
2025-08-24 04:45:08 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:09 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:45:09 [INFO]: Original df missing rate: 0.016
2025-08-24 04:45:09 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:45:09 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:45:09 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:45:09 [INFO]: Total sample number: 730
2025-08-24 04:45:09 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:45:09 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:45:09 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:45:09 [INFO]: Number of steps: 48
2025-08-24 04:45:09 [INFO]: Number of features: 132
2025-08-24 04:45:09 [INFO]: Train set missing rate: 50.93%
2025-08-24 04:45:09 [INFO]: Validating set missing rate: 50.44%
2025-08-24 04:45:09 [INFO]: Test set missing rate: 50.71%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:45:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:45:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:45:09 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:45:09 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:45:09 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:45:09 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:45:16 [INFO]: Model placed on cuda:0
2025-08-24 04:45:16 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:45:16 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.04 GiB. GPU 0 has a total capacity of 23.58 GiB of which 2.71 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 13.55 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 67.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:45:38 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:45:38 [INFO]: You're using dataset beijing_multisite_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/beijing_multisite_air_quality
2025-08-24 04:45:38 [INFO]: Dataset beijing_multisite_air_quality has already been downloaded. Processing directly...
2025-08-24 04:45:38 [INFO]: Dataset beijing_multisite_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:45:38 [INFO]: Loaded successfully!
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: Current dataframe shape: (35064, 18)
/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/benchpots/datasets/beijing_multisite_air_quality.py:60: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  current_df["date_time"] = pd.to_datetime(
2025-08-24 04:45:38 [INFO]: There are total 12 stations, they are ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan', 'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan', 'Wanliu', 'Wanshouxigong']
2025-08-24 04:45:38 [INFO]: Original df missing rate: 0.016
2025-08-24 04:45:38 [INFO]: months selected as train set are <PeriodArray>
['2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09',
 '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04',
 '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11',
 '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06']
Length: 28, dtype: period[M]
2025-08-24 04:45:38 [INFO]: months selected as val set are <PeriodArray>
['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01',
 '2016-02', '2016-03', '2016-04']
Length: 10, dtype: period[M]
2025-08-24 04:45:38 [INFO]: months selected as test set are <PeriodArray>
['2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11',
 '2016-12', '2017-01', '2017-02']
Length: 10, dtype: period[M]
2025-08-24 04:45:38 [INFO]: Total sample number: 730
2025-08-24 04:45:38 [INFO]: Training set size: 426 (58.36%)
2025-08-24 04:45:38 [INFO]: Validation set size: 152 (20.82%)
2025-08-24 04:45:38 [INFO]: Test set size: 152 (20.82%)
2025-08-24 04:45:38 [INFO]: Number of steps: 48
2025-08-24 04:45:38 [INFO]: Number of features: 132
2025-08-24 04:45:38 [INFO]: Train set missing rate: 50.93%
2025-08-24 04:45:38 [INFO]: Validating set missing rate: 50.44%
2025-08-24 04:45:38 [INFO]: Test set missing rate: 50.71%
âœ… Dataset 'beijing_multisite_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:45:38 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:45:38 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:45:39 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:45:39 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:45:39 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:45:39 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:45:48 [INFO]: Model placed on cuda:0
2025-08-24 04:45:48 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:45:49 [INFO]: Epoch 001 - training loss (MAE): 0.5492, validation MSE: 0.6104
2025-08-24 04:45:49 [INFO]: Epoch 002 - training loss (MAE): 0.5191, validation MSE: 0.5275
2025-08-24 04:45:49 [INFO]: Epoch 003 - training loss (MAE): 0.4970, validation MSE: 0.4749
2025-08-24 04:45:50 [INFO]: Epoch 004 - training loss (MAE): 0.4718, validation MSE: 0.4323
2025-08-24 04:45:50 [INFO]: Epoch 005 - training loss (MAE): 0.4509, validation MSE: 0.4107
2025-08-24 04:45:50 [INFO]: Epoch 006 - training loss (MAE): 0.4377, validation MSE: 0.3887
2025-08-24 04:45:50 [INFO]: Epoch 007 - training loss (MAE): 0.4209, validation MSE: 0.3751
2025-08-24 04:45:50 [INFO]: Epoch 008 - training loss (MAE): 0.4147, validation MSE: 0.3630
2025-08-24 04:45:51 [INFO]: Epoch 009 - training loss (MAE): 0.4059, validation MSE: 0.3492
2025-08-24 04:45:51 [INFO]: Epoch 010 - training loss (MAE): 0.3974, validation MSE: 0.3411
2025-08-24 04:45:51 [INFO]: Epoch 011 - training loss (MAE): 0.3880, validation MSE: 0.3324
2025-08-24 04:45:51 [INFO]: Epoch 012 - training loss (MAE): 0.3851, validation MSE: 0.3262
2025-08-24 04:45:52 [INFO]: Epoch 013 - training loss (MAE): 0.3780, validation MSE: 0.3211
2025-08-24 04:45:52 [INFO]: Epoch 014 - training loss (MAE): 0.3738, validation MSE: 0.3172
2025-08-24 04:45:52 [INFO]: Epoch 015 - training loss (MAE): 0.3690, validation MSE: 0.3133
2025-08-24 04:45:52 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:45:53 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2915| MSE: 0.2967| RMSE: 0.5447| MRE: 0.3874| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/beijing_multisite_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: beijing_multisite_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:46:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:46:16 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:46:16 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:46:16 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:46:16 [INFO]: Loaded successfully!
2025-08-24 04:46:16 [INFO]: Total sample number: 192
2025-08-24 04:46:16 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:46:16 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:46:16 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:46:16 [INFO]: Number of steps: 48
2025-08-24 04:46:16 [INFO]: Number of features: 13
2025-08-24 04:46:16 [INFO]: Train set missing rate: 9.87%
2025-08-24 04:46:16 [INFO]: Validating set missing rate: 9.86%
2025-08-24 04:46:16 [INFO]: Test set missing rate: 9.87%
âœ… Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:46:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:46:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:46:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:46:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:46:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:46:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:46:24 [INFO]: Model placed on cuda:0
2025-08-24 04:46:24 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:46:25 [INFO]: Epoch 001 - training loss (MAE): 0.4649, validation MSE: 0.7467
2025-08-24 04:46:26 [INFO]: Epoch 002 - training loss (MAE): 0.4364, validation MSE: 0.6692
2025-08-24 04:46:27 [INFO]: Epoch 003 - training loss (MAE): 0.4010, validation MSE: 0.6271
2025-08-24 04:46:28 [INFO]: Epoch 004 - training loss (MAE): 0.3854, validation MSE: 0.5818
2025-08-24 04:46:29 [INFO]: Epoch 005 - training loss (MAE): 0.3610, validation MSE: 0.5394
2025-08-24 04:46:29 [INFO]: Epoch 006 - training loss (MAE): 0.3503, validation MSE: 0.5263
2025-08-24 04:46:30 [INFO]: Epoch 007 - training loss (MAE): 0.3283, validation MSE: 0.5043
2025-08-24 04:46:31 [INFO]: Epoch 008 - training loss (MAE): 0.3200, validation MSE: 0.4840
2025-08-24 04:46:32 [INFO]: Epoch 009 - training loss (MAE): 0.3178, validation MSE: 0.4754
2025-08-24 04:46:33 [INFO]: Epoch 010 - training loss (MAE): 0.3173, validation MSE: 0.4667
2025-08-24 04:46:34 [INFO]: Epoch 011 - training loss (MAE): 0.3002, validation MSE: 0.4665
2025-08-24 04:46:35 [INFO]: Epoch 012 - training loss (MAE): 0.3044, validation MSE: 0.4560
2025-08-24 04:46:36 [INFO]: Epoch 013 - training loss (MAE): 0.2984, validation MSE: 0.4308
2025-08-24 04:46:37 [INFO]: Epoch 014 - training loss (MAE): 0.2894, validation MSE: 0.4179
2025-08-24 04:46:37 [INFO]: Epoch 015 - training loss (MAE): 0.2925, validation MSE: 0.4167
2025-08-24 04:46:37 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:46:39 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3215| MSE: 0.3027| RMSE: 0.5502| MRE: 0.4184| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:47:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:47:03 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:47:03 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:47:03 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:47:03 [INFO]: Loaded successfully!
2025-08-24 04:47:03 [INFO]: Total sample number: 192
2025-08-24 04:47:03 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:47:03 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:47:03 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:47:03 [INFO]: Number of steps: 48
2025-08-24 04:47:03 [INFO]: Number of features: 13
2025-08-24 04:47:03 [INFO]: Train set missing rate: 9.87%
2025-08-24 04:47:03 [INFO]: Validating set missing rate: 9.86%
2025-08-24 04:47:03 [INFO]: Test set missing rate: 9.87%
âœ… Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:47:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:47:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:47:04 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:47:04 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:47:04 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:47:04 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:47:10 [INFO]: Model placed on cuda:0
2025-08-24 04:47:10 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:47:11 [INFO]: Epoch 001 - training loss (MAE): 0.4624, validation MSE: 0.6763
2025-08-24 04:47:11 [INFO]: Epoch 002 - training loss (MAE): 0.4410, validation MSE: 0.6458
2025-08-24 04:47:11 [INFO]: Epoch 003 - training loss (MAE): 0.4104, validation MSE: 0.6311
2025-08-24 04:47:11 [INFO]: Epoch 004 - training loss (MAE): 0.4118, validation MSE: 0.6049
2025-08-24 04:47:11 [INFO]: Epoch 005 - training loss (MAE): 0.3937, validation MSE: 0.5747
2025-08-24 04:47:11 [INFO]: Epoch 006 - training loss (MAE): 0.3899, validation MSE: 0.5481
2025-08-24 04:47:11 [INFO]: Epoch 007 - training loss (MAE): 0.3669, validation MSE: 0.5212
2025-08-24 04:47:11 [INFO]: Epoch 008 - training loss (MAE): 0.3500, validation MSE: 0.5009
2025-08-24 04:47:11 [INFO]: Epoch 009 - training loss (MAE): 0.3480, validation MSE: 0.4874
2025-08-24 04:47:11 [INFO]: Epoch 010 - training loss (MAE): 0.3381, validation MSE: 0.4794
2025-08-24 04:47:11 [INFO]: Epoch 011 - training loss (MAE): 0.3284, validation MSE: 0.4689
2025-08-24 04:47:11 [INFO]: Epoch 012 - training loss (MAE): 0.3255, validation MSE: 0.4544
2025-08-24 04:47:11 [INFO]: Epoch 013 - training loss (MAE): 0.3198, validation MSE: 0.4421
2025-08-24 04:47:11 [INFO]: Epoch 014 - training loss (MAE): 0.3096, validation MSE: 0.4317
2025-08-24 04:47:11 [INFO]: Epoch 015 - training loss (MAE): 0.3170, validation MSE: 0.4216
2025-08-24 04:47:11 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:47:13 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3297| MSE: 0.3183| RMSE: 0.5641| MRE: 0.4291| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:47:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:47:35 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:47:35 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:47:35 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:47:35 [INFO]: Loaded successfully!
âœ… Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:47:35 [INFO]: Total sample number: 192
2025-08-24 04:47:35 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:47:35 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:47:35 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:47:35 [INFO]: Number of steps: 48
2025-08-24 04:47:35 [INFO]: Number of features: 13
2025-08-24 04:47:35 [INFO]: Train set missing rate: 9.87%
2025-08-24 04:47:35 [INFO]: Validating set missing rate: 9.86%
2025-08-24 04:47:35 [INFO]: Test set missing rate: 9.87%
2025-08-24 04:47:35 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:47:35 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:47:35 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:47:35 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:47:35 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:47:35 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:47:41 [INFO]: Model placed on cuda:0
2025-08-24 04:47:41 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:47:43 [INFO]: Epoch 001 - training loss (MAE): 0.4669, validation MSE: 0.7307
2025-08-24 04:47:43 [INFO]: Epoch 002 - training loss (MAE): 0.4367, validation MSE: 0.6931
2025-08-24 04:47:44 [INFO]: Epoch 003 - training loss (MAE): 0.4072, validation MSE: 0.6467
2025-08-24 04:47:45 [INFO]: Epoch 004 - training loss (MAE): 0.3988, validation MSE: 0.6053
2025-08-24 04:47:46 [INFO]: Epoch 005 - training loss (MAE): 0.3806, validation MSE: 0.5706
2025-08-24 04:47:47 [INFO]: Epoch 006 - training loss (MAE): 0.3707, validation MSE: 0.5430
2025-08-24 04:47:47 [INFO]: Epoch 007 - training loss (MAE): 0.3491, validation MSE: 0.5263
2025-08-24 04:47:48 [INFO]: Epoch 008 - training loss (MAE): 0.3392, validation MSE: 0.5125
2025-08-24 04:47:49 [INFO]: Epoch 009 - training loss (MAE): 0.3400, validation MSE: 0.5026
2025-08-24 04:47:50 [INFO]: Epoch 010 - training loss (MAE): 0.3310, validation MSE: 0.4982
2025-08-24 04:47:51 [INFO]: Epoch 011 - training loss (MAE): 0.3200, validation MSE: 0.4870
2025-08-24 04:47:51 [INFO]: Epoch 012 - training loss (MAE): 0.3222, validation MSE: 0.4851
2025-08-24 04:47:52 [INFO]: Epoch 013 - training loss (MAE): 0.3161, validation MSE: 0.4812
2025-08-24 04:47:53 [INFO]: Epoch 014 - training loss (MAE): 0.3077, validation MSE: 0.4740
2025-08-24 04:47:54 [INFO]: Epoch 015 - training loss (MAE): 0.3119, validation MSE: 0.4611
2025-08-24 04:47:54 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:47:55 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3274| MSE: 0.3220| RMSE: 0.5674| MRE: 0.4260| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:48:19 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:48:19 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:48:19 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:48:19 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:48:19 [INFO]: Loaded successfully!
2025-08-24 04:48:19 [INFO]: Total sample number: 192
2025-08-24 04:48:19 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:48:19 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:48:19 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:48:19 [INFO]: Number of steps: 48
2025-08-24 04:48:19 [INFO]: Number of features: 13
2025-08-24 04:48:19 [INFO]: Train set missing rate: 9.87%
2025-08-24 04:48:19 [INFO]: Validating set missing rate: 9.86%
2025-08-24 04:48:19 [INFO]: Test set missing rate: 9.87%
âœ… Dataset 'italy_air_quality' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 04:48:19 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:48:19 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:48:20 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:48:20 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:48:20 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:48:20 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:48:27 [INFO]: Model placed on cuda:0
2025-08-24 04:48:27 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:48:28 [INFO]: Epoch 001 - training loss (MAE): 0.4624, validation MSE: 0.6763
2025-08-24 04:48:28 [INFO]: Epoch 002 - training loss (MAE): 0.4410, validation MSE: 0.6458
2025-08-24 04:48:28 [INFO]: Epoch 003 - training loss (MAE): 0.4104, validation MSE: 0.6311
2025-08-24 04:48:28 [INFO]: Epoch 004 - training loss (MAE): 0.4118, validation MSE: 0.6049
2025-08-24 04:48:28 [INFO]: Epoch 005 - training loss (MAE): 0.3937, validation MSE: 0.5747
2025-08-24 04:48:28 [INFO]: Epoch 006 - training loss (MAE): 0.3899, validation MSE: 0.5481
2025-08-24 04:48:28 [INFO]: Epoch 007 - training loss (MAE): 0.3669, validation MSE: 0.5212
2025-08-24 04:48:28 [INFO]: Epoch 008 - training loss (MAE): 0.3500, validation MSE: 0.5009
2025-08-24 04:48:28 [INFO]: Epoch 009 - training loss (MAE): 0.3480, validation MSE: 0.4874
2025-08-24 04:48:28 [INFO]: Epoch 010 - training loss (MAE): 0.3381, validation MSE: 0.4794
2025-08-24 04:48:28 [INFO]: Epoch 011 - training loss (MAE): 0.3284, validation MSE: 0.4689
2025-08-24 04:48:28 [INFO]: Epoch 012 - training loss (MAE): 0.3255, validation MSE: 0.4544
2025-08-24 04:48:28 [INFO]: Epoch 013 - training loss (MAE): 0.3198, validation MSE: 0.4421
2025-08-24 04:48:28 [INFO]: Epoch 014 - training loss (MAE): 0.3096, validation MSE: 0.4317
2025-08-24 04:48:28 [INFO]: Epoch 015 - training loss (MAE): 0.3170, validation MSE: 0.4216
2025-08-24 04:48:28 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:48:30 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3297| MSE: 0.3183| RMSE: 0.5641| MRE: 0.4291| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:48:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:48:57 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:48:57 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:48:57 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:48:57 [INFO]: Loaded successfully!
2025-08-24 04:48:57 [INFO]: Total sample number: 192
2025-08-24 04:48:57 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:48:57 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:48:57 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:48:57 [INFO]: Number of steps: 48
2025-08-24 04:48:57 [INFO]: Number of features: 13
2025-08-24 04:48:57 [INFO]: Train set missing rate: 19.79%
2025-08-24 04:48:57 [INFO]: Validating set missing rate: 19.86%
2025-08-24 04:48:57 [INFO]: Test set missing rate: 19.88%
âœ… Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:48:57 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:48:57 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:48:58 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:48:58 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:48:58 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:48:58 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:49:04 [INFO]: Model placed on cuda:0
2025-08-24 04:49:04 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:49:05 [INFO]: Epoch 001 - training loss (MAE): 0.4773, validation MSE: 0.8899
2025-08-24 04:49:06 [INFO]: Epoch 002 - training loss (MAE): 0.4541, validation MSE: 0.8166
2025-08-24 04:49:07 [INFO]: Epoch 003 - training loss (MAE): 0.4209, validation MSE: 0.7635
2025-08-24 04:49:08 [INFO]: Epoch 004 - training loss (MAE): 0.4080, validation MSE: 0.6908
2025-08-24 04:49:09 [INFO]: Epoch 005 - training loss (MAE): 0.3859, validation MSE: 0.6465
2025-08-24 04:49:10 [INFO]: Epoch 006 - training loss (MAE): 0.3798, validation MSE: 0.6096
2025-08-24 04:49:11 [INFO]: Epoch 007 - training loss (MAE): 0.3544, validation MSE: 0.5923
2025-08-24 04:49:11 [INFO]: Epoch 008 - training loss (MAE): 0.3485, validation MSE: 0.5572
2025-08-24 04:49:12 [INFO]: Epoch 009 - training loss (MAE): 0.3411, validation MSE: 0.5300
2025-08-24 04:49:13 [INFO]: Epoch 010 - training loss (MAE): 0.3385, validation MSE: 0.5216
2025-08-24 04:49:14 [INFO]: Epoch 011 - training loss (MAE): 0.3263, validation MSE: 0.5148
2025-08-24 04:49:15 [INFO]: Epoch 012 - training loss (MAE): 0.3268, validation MSE: 0.5001
2025-08-24 04:49:16 [INFO]: Epoch 013 - training loss (MAE): 0.3207, validation MSE: 0.4787
2025-08-24 04:49:17 [INFO]: Epoch 014 - training loss (MAE): 0.3160, validation MSE: 0.4604
2025-08-24 04:49:17 [INFO]: Epoch 015 - training loss (MAE): 0.3183, validation MSE: 0.4496
2025-08-24 04:49:17 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:49:19 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3579| MSE: 0.3714| RMSE: 0.6094| MRE: 0.4618| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:49:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:49:43 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:49:43 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:49:43 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:49:43 [INFO]: Loaded successfully!
2025-08-24 04:49:43 [INFO]: Total sample number: 192
2025-08-24 04:49:43 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:49:43 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:49:43 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:49:43 [INFO]: Number of steps: 48
2025-08-24 04:49:43 [INFO]: Number of features: 13
2025-08-24 04:49:43 [INFO]: Train set missing rate: 19.79%
2025-08-24 04:49:43 [INFO]: Validating set missing rate: 19.86%
2025-08-24 04:49:43 [INFO]: Test set missing rate: 19.88%
âœ… Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:49:43 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:49:43 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:49:43 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:49:43 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:49:43 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:49:43 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:49:52 [INFO]: Model placed on cuda:0
2025-08-24 04:49:52 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:49:52 [INFO]: Epoch 001 - training loss (MAE): 0.4755, validation MSE: 0.7875
2025-08-24 04:49:52 [INFO]: Epoch 002 - training loss (MAE): 0.4568, validation MSE: 0.7805
2025-08-24 04:49:52 [INFO]: Epoch 003 - training loss (MAE): 0.4274, validation MSE: 0.7778
2025-08-24 04:49:52 [INFO]: Epoch 004 - training loss (MAE): 0.4304, validation MSE: 0.7499
2025-08-24 04:49:52 [INFO]: Epoch 005 - training loss (MAE): 0.4134, validation MSE: 0.7114
2025-08-24 04:49:52 [INFO]: Epoch 006 - training loss (MAE): 0.4150, validation MSE: 0.6690
2025-08-24 04:49:52 [INFO]: Epoch 007 - training loss (MAE): 0.3913, validation MSE: 0.6310
2025-08-24 04:49:52 [INFO]: Epoch 008 - training loss (MAE): 0.3788, validation MSE: 0.6013
2025-08-24 04:49:52 [INFO]: Epoch 009 - training loss (MAE): 0.3744, validation MSE: 0.5775
2025-08-24 04:49:52 [INFO]: Epoch 010 - training loss (MAE): 0.3635, validation MSE: 0.5583
2025-08-24 04:49:52 [INFO]: Epoch 011 - training loss (MAE): 0.3548, validation MSE: 0.5381
2025-08-24 04:49:53 [INFO]: Epoch 012 - training loss (MAE): 0.3499, validation MSE: 0.5195
2025-08-24 04:49:53 [INFO]: Epoch 013 - training loss (MAE): 0.3436, validation MSE: 0.5073
2025-08-24 04:49:53 [INFO]: Epoch 014 - training loss (MAE): 0.3358, validation MSE: 0.4960
2025-08-24 04:49:53 [INFO]: Epoch 015 - training loss (MAE): 0.3393, validation MSE: 0.4796
2025-08-24 04:49:53 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:49:54 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3664| MSE: 0.3835| RMSE: 0.6193| MRE: 0.4728| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:50:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:50:18 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:50:18 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:50:18 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:50:18 [INFO]: Loaded successfully!
2025-08-24 04:50:18 [INFO]: Total sample number: 192
2025-08-24 04:50:18 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:50:18 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:50:18 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:50:18 [INFO]: Number of steps: 48
2025-08-24 04:50:18 [INFO]: Number of features: 13
2025-08-24 04:50:18 [INFO]: Train set missing rate: 19.79%
2025-08-24 04:50:18 [INFO]: Validating set missing rate: 19.86%
2025-08-24 04:50:18 [INFO]: Test set missing rate: 19.88%
âœ… Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:50:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:50:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:50:18 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:50:18 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:50:18 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:50:18 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:50:26 [INFO]: Model placed on cuda:0
2025-08-24 04:50:26 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:50:27 [INFO]: Epoch 001 - training loss (MAE): 0.4792, validation MSE: 0.8505
2025-08-24 04:50:27 [INFO]: Epoch 002 - training loss (MAE): 0.4535, validation MSE: 0.8434
2025-08-24 04:50:28 [INFO]: Epoch 003 - training loss (MAE): 0.4253, validation MSE: 0.7905
2025-08-24 04:50:29 [INFO]: Epoch 004 - training loss (MAE): 0.4180, validation MSE: 0.7277
2025-08-24 04:50:30 [INFO]: Epoch 005 - training loss (MAE): 0.4031, validation MSE: 0.6825
2025-08-24 04:50:31 [INFO]: Epoch 006 - training loss (MAE): 0.3990, validation MSE: 0.6388
2025-08-24 04:50:31 [INFO]: Epoch 007 - training loss (MAE): 0.3768, validation MSE: 0.6117
2025-08-24 04:50:32 [INFO]: Epoch 008 - training loss (MAE): 0.3683, validation MSE: 0.5985
2025-08-24 04:50:33 [INFO]: Epoch 009 - training loss (MAE): 0.3626, validation MSE: 0.5791
2025-08-24 04:50:34 [INFO]: Epoch 010 - training loss (MAE): 0.3552, validation MSE: 0.5641
2025-08-24 04:50:35 [INFO]: Epoch 011 - training loss (MAE): 0.3465, validation MSE: 0.5445
2025-08-24 04:50:35 [INFO]: Epoch 012 - training loss (MAE): 0.3453, validation MSE: 0.5340
2025-08-24 04:50:36 [INFO]: Epoch 013 - training loss (MAE): 0.3404, validation MSE: 0.5263
2025-08-24 04:50:37 [INFO]: Epoch 014 - training loss (MAE): 0.3331, validation MSE: 0.5094
2025-08-24 04:50:38 [INFO]: Epoch 015 - training loss (MAE): 0.3350, validation MSE: 0.4941
2025-08-24 04:50:38 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:50:39 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3607| MSE: 0.3788| RMSE: 0.6155| MRE: 0.4654| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:51:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:51:10 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:51:10 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:51:10 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:51:10 [INFO]: Loaded successfully!
2025-08-24 04:51:10 [INFO]: Total sample number: 192
2025-08-24 04:51:10 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:51:10 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:51:10 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:51:10 [INFO]: Number of steps: 48
2025-08-24 04:51:10 [INFO]: Number of features: 13
2025-08-24 04:51:10 [INFO]: Train set missing rate: 19.79%
2025-08-24 04:51:10 [INFO]: Validating set missing rate: 19.86%
2025-08-24 04:51:10 [INFO]: Test set missing rate: 19.88%
âœ… Dataset 'italy_air_quality' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 04:51:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:51:10 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:51:11 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:51:11 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:51:11 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:51:11 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:51:18 [INFO]: Model placed on cuda:0
2025-08-24 04:51:18 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:51:18 [INFO]: Epoch 001 - training loss (MAE): 0.4755, validation MSE: 0.7875
2025-08-24 04:51:18 [INFO]: Epoch 002 - training loss (MAE): 0.4568, validation MSE: 0.7805
2025-08-24 04:51:18 [INFO]: Epoch 003 - training loss (MAE): 0.4274, validation MSE: 0.7778
2025-08-24 04:51:18 [INFO]: Epoch 004 - training loss (MAE): 0.4304, validation MSE: 0.7499
2025-08-24 04:51:18 [INFO]: Epoch 005 - training loss (MAE): 0.4134, validation MSE: 0.7114
2025-08-24 04:51:18 [INFO]: Epoch 006 - training loss (MAE): 0.4150, validation MSE: 0.6690
2025-08-24 04:51:19 [INFO]: Epoch 007 - training loss (MAE): 0.3913, validation MSE: 0.6310
2025-08-24 04:51:19 [INFO]: Epoch 008 - training loss (MAE): 0.3788, validation MSE: 0.6013
2025-08-24 04:51:19 [INFO]: Epoch 009 - training loss (MAE): 0.3744, validation MSE: 0.5775
2025-08-24 04:51:19 [INFO]: Epoch 010 - training loss (MAE): 0.3635, validation MSE: 0.5583
2025-08-24 04:51:19 [INFO]: Epoch 011 - training loss (MAE): 0.3548, validation MSE: 0.5381
2025-08-24 04:51:19 [INFO]: Epoch 012 - training loss (MAE): 0.3499, validation MSE: 0.5195
2025-08-24 04:51:19 [INFO]: Epoch 013 - training loss (MAE): 0.3436, validation MSE: 0.5073
2025-08-24 04:51:19 [INFO]: Epoch 014 - training loss (MAE): 0.3358, validation MSE: 0.4960
2025-08-24 04:51:19 [INFO]: Epoch 015 - training loss (MAE): 0.3393, validation MSE: 0.4796
2025-08-24 04:51:19 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:51:21 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3664| MSE: 0.3835| RMSE: 0.6193| MRE: 0.4728| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:51:45 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:51:45 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:51:45 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:51:45 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:51:45 [INFO]: Loaded successfully!
2025-08-24 04:51:45 [INFO]: Total sample number: 192
2025-08-24 04:51:45 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:51:45 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:51:45 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:51:45 [INFO]: Number of steps: 48
2025-08-24 04:51:45 [INFO]: Number of features: 13
2025-08-24 04:51:45 [INFO]: Train set missing rate: 29.80%
2025-08-24 04:51:45 [INFO]: Validating set missing rate: 29.90%
2025-08-24 04:51:45 [INFO]: Test set missing rate: 30.05%
âœ… Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:51:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:51:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:51:46 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:51:46 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:51:46 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:51:46 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:51:55 [INFO]: Model placed on cuda:0
2025-08-24 04:51:55 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:51:57 [INFO]: Epoch 001 - training loss (MAE): 0.4918, validation MSE: 0.9911
2025-08-24 04:51:57 [INFO]: Epoch 002 - training loss (MAE): 0.4748, validation MSE: 0.9375
2025-08-24 04:51:58 [INFO]: Epoch 003 - training loss (MAE): 0.4427, validation MSE: 0.8690
2025-08-24 04:51:59 [INFO]: Epoch 004 - training loss (MAE): 0.4349, validation MSE: 0.8099
2025-08-24 04:52:00 [INFO]: Epoch 005 - training loss (MAE): 0.4163, validation MSE: 0.7736
2025-08-24 04:52:01 [INFO]: Epoch 006 - training loss (MAE): 0.4085, validation MSE: 0.7408
2025-08-24 04:52:02 [INFO]: Epoch 007 - training loss (MAE): 0.3865, validation MSE: 0.7156
2025-08-24 04:52:03 [INFO]: Epoch 008 - training loss (MAE): 0.3773, validation MSE: 0.7095
2025-08-24 04:52:04 [INFO]: Epoch 009 - training loss (MAE): 0.3725, validation MSE: 0.6653
2025-08-24 04:52:04 [INFO]: Epoch 010 - training loss (MAE): 0.3663, validation MSE: 0.6739
2025-08-24 04:52:05 [INFO]: Epoch 011 - training loss (MAE): 0.3618, validation MSE: 0.6548
2025-08-24 04:52:06 [INFO]: Epoch 012 - training loss (MAE): 0.3570, validation MSE: 0.6435
2025-08-24 04:52:07 [INFO]: Epoch 013 - training loss (MAE): 0.3484, validation MSE: 0.6398
2025-08-24 04:52:08 [INFO]: Epoch 014 - training loss (MAE): 0.3473, validation MSE: 0.6175
2025-08-24 04:52:09 [INFO]: Epoch 015 - training loss (MAE): 0.3429, validation MSE: 0.6098
2025-08-24 04:52:09 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:52:10 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4059| MSE: 0.4707| RMSE: 0.6861| MRE: 0.5193| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:52:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:52:34 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:52:34 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:52:34 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:52:34 [INFO]: Loaded successfully!
2025-08-24 04:52:34 [INFO]: Total sample number: 192
2025-08-24 04:52:34 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:52:34 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:52:34 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:52:34 [INFO]: Number of steps: 48
2025-08-24 04:52:34 [INFO]: Number of features: 13
2025-08-24 04:52:34 [INFO]: Train set missing rate: 29.80%
2025-08-24 04:52:34 [INFO]: Validating set missing rate: 29.90%
2025-08-24 04:52:34 [INFO]: Test set missing rate: 30.05%
âœ… Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:52:34 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:52:34 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:52:35 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:52:35 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:52:35 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:52:35 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:52:41 [INFO]: Model placed on cuda:0
2025-08-24 04:52:41 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:52:42 [INFO]: Epoch 001 - training loss (MAE): 0.4896, validation MSE: 0.8886
2025-08-24 04:52:42 [INFO]: Epoch 002 - training loss (MAE): 0.4754, validation MSE: 0.8799
2025-08-24 04:52:42 [INFO]: Epoch 003 - training loss (MAE): 0.4460, validation MSE: 0.8776
2025-08-24 04:52:42 [INFO]: Epoch 004 - training loss (MAE): 0.4517, validation MSE: 0.8551
2025-08-24 04:52:42 [INFO]: Epoch 005 - training loss (MAE): 0.4399, validation MSE: 0.8222
2025-08-24 04:52:42 [INFO]: Epoch 006 - training loss (MAE): 0.4396, validation MSE: 0.7851
2025-08-24 04:52:42 [INFO]: Epoch 007 - training loss (MAE): 0.4162, validation MSE: 0.7491
2025-08-24 04:52:42 [INFO]: Epoch 008 - training loss (MAE): 0.4056, validation MSE: 0.7161
2025-08-24 04:52:42 [INFO]: Epoch 009 - training loss (MAE): 0.4048, validation MSE: 0.6855
2025-08-24 04:52:42 [INFO]: Epoch 010 - training loss (MAE): 0.3933, validation MSE: 0.6655
2025-08-24 04:52:42 [INFO]: Epoch 011 - training loss (MAE): 0.3885, validation MSE: 0.6498
2025-08-24 04:52:42 [INFO]: Epoch 012 - training loss (MAE): 0.3821, validation MSE: 0.6363
2025-08-24 04:52:42 [INFO]: Epoch 013 - training loss (MAE): 0.3727, validation MSE: 0.6289
2025-08-24 04:52:42 [INFO]: Epoch 014 - training loss (MAE): 0.3664, validation MSE: 0.6210
2025-08-24 04:52:42 [INFO]: Epoch 015 - training loss (MAE): 0.3669, validation MSE: 0.6065
2025-08-24 04:52:42 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:52:44 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4115| MSE: 0.4868| RMSE: 0.6977| MRE: 0.5265| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:53:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:53:09 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:53:09 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:53:09 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:53:09 [INFO]: Loaded successfully!
2025-08-24 04:53:09 [INFO]: Total sample number: 192
2025-08-24 04:53:09 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:53:09 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:53:09 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:53:09 [INFO]: Number of steps: 48
2025-08-24 04:53:09 [INFO]: Number of features: 13
2025-08-24 04:53:09 [INFO]: Train set missing rate: 29.80%
2025-08-24 04:53:09 [INFO]: Validating set missing rate: 29.90%
2025-08-24 04:53:09 [INFO]: Test set missing rate: 30.05%
âœ… Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:53:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:53:09 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:53:09 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:53:09 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:53:09 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:53:09 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:53:16 [INFO]: Model placed on cuda:0
2025-08-24 04:53:16 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:53:17 [INFO]: Epoch 001 - training loss (MAE): 0.4937, validation MSE: 0.9501
2025-08-24 04:53:18 [INFO]: Epoch 002 - training loss (MAE): 0.4726, validation MSE: 0.9447
2025-08-24 04:53:18 [INFO]: Epoch 003 - training loss (MAE): 0.4452, validation MSE: 0.8976
2025-08-24 04:53:19 [INFO]: Epoch 004 - training loss (MAE): 0.4421, validation MSE: 0.8485
2025-08-24 04:53:20 [INFO]: Epoch 005 - training loss (MAE): 0.4299, validation MSE: 0.8049
2025-08-24 04:53:21 [INFO]: Epoch 006 - training loss (MAE): 0.4258, validation MSE: 0.7670
2025-08-24 04:53:22 [INFO]: Epoch 007 - training loss (MAE): 0.4062, validation MSE: 0.7385
2025-08-24 04:53:23 [INFO]: Epoch 008 - training loss (MAE): 0.3963, validation MSE: 0.7209
2025-08-24 04:53:23 [INFO]: Epoch 009 - training loss (MAE): 0.3920, validation MSE: 0.7027
2025-08-24 04:53:24 [INFO]: Epoch 010 - training loss (MAE): 0.3831, validation MSE: 0.6916
2025-08-24 04:53:25 [INFO]: Epoch 011 - training loss (MAE): 0.3800, validation MSE: 0.6743
2025-08-24 04:53:26 [INFO]: Epoch 012 - training loss (MAE): 0.3746, validation MSE: 0.6634
2025-08-24 04:53:27 [INFO]: Epoch 013 - training loss (MAE): 0.3654, validation MSE: 0.6588
2025-08-24 04:53:27 [INFO]: Epoch 014 - training loss (MAE): 0.3637, validation MSE: 0.6436
2025-08-24 04:53:28 [INFO]: Epoch 015 - training loss (MAE): 0.3591, validation MSE: 0.6279
2025-08-24 04:53:28 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:53:30 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4076| MSE: 0.4853| RMSE: 0.6966| MRE: 0.5215| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:53:53 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:53:53 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:53:53 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:53:53 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:53:53 [INFO]: Loaded successfully!
2025-08-24 04:53:53 [INFO]: Total sample number: 192
2025-08-24 04:53:53 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:53:53 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:53:53 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:53:53 [INFO]: Number of steps: 48
2025-08-24 04:53:53 [INFO]: Number of features: 13
2025-08-24 04:53:53 [INFO]: Train set missing rate: 29.80%
2025-08-24 04:53:53 [INFO]: Validating set missing rate: 29.90%
2025-08-24 04:53:53 [INFO]: Test set missing rate: 30.05%
âœ… Dataset 'italy_air_quality' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 04:53:53 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:53:53 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:53:53 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:53:53 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:53:53 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:53:53 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:54:03 [INFO]: Model placed on cuda:0
2025-08-24 04:54:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:54:03 [INFO]: Epoch 001 - training loss (MAE): 0.4896, validation MSE: 0.8886
2025-08-24 04:54:03 [INFO]: Epoch 002 - training loss (MAE): 0.4754, validation MSE: 0.8799
2025-08-24 04:54:03 [INFO]: Epoch 003 - training loss (MAE): 0.4460, validation MSE: 0.8776
2025-08-24 04:54:03 [INFO]: Epoch 004 - training loss (MAE): 0.4517, validation MSE: 0.8551
2025-08-24 04:54:03 [INFO]: Epoch 005 - training loss (MAE): 0.4399, validation MSE: 0.8222
2025-08-24 04:54:03 [INFO]: Epoch 006 - training loss (MAE): 0.4396, validation MSE: 0.7851
2025-08-24 04:54:03 [INFO]: Epoch 007 - training loss (MAE): 0.4162, validation MSE: 0.7491
2025-08-24 04:54:04 [INFO]: Epoch 008 - training loss (MAE): 0.4056, validation MSE: 0.7161
2025-08-24 04:54:04 [INFO]: Epoch 009 - training loss (MAE): 0.4048, validation MSE: 0.6855
2025-08-24 04:54:04 [INFO]: Epoch 010 - training loss (MAE): 0.3933, validation MSE: 0.6655
2025-08-24 04:54:04 [INFO]: Epoch 011 - training loss (MAE): 0.3885, validation MSE: 0.6498
2025-08-24 04:54:04 [INFO]: Epoch 012 - training loss (MAE): 0.3821, validation MSE: 0.6363
2025-08-24 04:54:04 [INFO]: Epoch 013 - training loss (MAE): 0.3727, validation MSE: 0.6289
2025-08-24 04:54:04 [INFO]: Epoch 014 - training loss (MAE): 0.3664, validation MSE: 0.6210
2025-08-24 04:54:04 [INFO]: Epoch 015 - training loss (MAE): 0.3669, validation MSE: 0.6065
2025-08-24 04:54:04 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:54:05 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4115| MSE: 0.4868| RMSE: 0.6977| MRE: 0.5265| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:54:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:54:31 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:54:31 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:54:31 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:54:31 [INFO]: Loaded successfully!
2025-08-24 04:54:31 [INFO]: Total sample number: 192
2025-08-24 04:54:31 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:54:31 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:54:31 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:54:31 [INFO]: Number of steps: 48
2025-08-24 04:54:31 [INFO]: Number of features: 13
2025-08-24 04:54:31 [INFO]: Train set missing rate: 39.84%
2025-08-24 04:54:31 [INFO]: Validating set missing rate: 39.87%
2025-08-24 04:54:31 [INFO]: Test set missing rate: 40.52%
âœ… Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:54:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:54:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:54:31 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:54:31 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:54:31 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:54:31 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:54:38 [INFO]: Model placed on cuda:0
2025-08-24 04:54:38 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:54:40 [INFO]: Epoch 001 - training loss (MAE): 0.5108, validation MSE: 1.2070
2025-08-24 04:54:40 [INFO]: Epoch 002 - training loss (MAE): 0.4940, validation MSE: 1.1788
2025-08-24 04:54:41 [INFO]: Epoch 003 - training loss (MAE): 0.4684, validation MSE: 1.1108
2025-08-24 04:54:42 [INFO]: Epoch 004 - training loss (MAE): 0.4636, validation MSE: 1.0843
2025-08-24 04:54:43 [INFO]: Epoch 005 - training loss (MAE): 0.4435, validation MSE: 1.0392
2025-08-24 04:54:44 [INFO]: Epoch 006 - training loss (MAE): 0.4429, validation MSE: 1.0199
2025-08-24 04:54:45 [INFO]: Epoch 007 - training loss (MAE): 0.4160, validation MSE: 0.9737
2025-08-24 04:54:46 [INFO]: Epoch 008 - training loss (MAE): 0.4089, validation MSE: 0.9507
2025-08-24 04:54:46 [INFO]: Epoch 009 - training loss (MAE): 0.4091, validation MSE: 0.9189
2025-08-24 04:54:47 [INFO]: Epoch 010 - training loss (MAE): 0.3996, validation MSE: 0.9474
2025-08-24 04:54:48 [INFO]: Epoch 011 - training loss (MAE): 0.3930, validation MSE: 0.9275
2025-08-24 04:54:49 [INFO]: Epoch 012 - training loss (MAE): 0.3883, validation MSE: 0.9444
2025-08-24 04:54:50 [INFO]: Epoch 013 - training loss (MAE): 0.3729, validation MSE: 0.9250
2025-08-24 04:54:51 [INFO]: Epoch 014 - training loss (MAE): 0.3752, validation MSE: 0.9139
2025-08-24 04:54:52 [INFO]: Epoch 015 - training loss (MAE): 0.3674, validation MSE: 0.8733
2025-08-24 04:54:52 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:54:53 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4597| MSE: 0.5624| RMSE: 0.7499| MRE: 0.5902| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:55:21 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:55:21 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:55:21 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:55:21 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:55:21 [INFO]: Loaded successfully!
2025-08-24 04:55:21 [INFO]: Total sample number: 192
2025-08-24 04:55:21 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:55:21 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:55:21 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:55:21 [INFO]: Number of steps: 48
2025-08-24 04:55:21 [INFO]: Number of features: 13
2025-08-24 04:55:21 [INFO]: Train set missing rate: 39.84%
2025-08-24 04:55:21 [INFO]: Validating set missing rate: 39.87%
2025-08-24 04:55:21 [INFO]: Test set missing rate: 40.52%
âœ… Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:55:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:55:21 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:55:21 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:55:21 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:55:21 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:55:21 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:55:28 [INFO]: Model placed on cuda:0
2025-08-24 04:55:28 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:55:28 [INFO]: Epoch 001 - training loss (MAE): 0.5108, validation MSE: 1.1362
2025-08-24 04:55:28 [INFO]: Epoch 002 - training loss (MAE): 0.4951, validation MSE: 1.1347
2025-08-24 04:55:28 [INFO]: Epoch 003 - training loss (MAE): 0.4701, validation MSE: 1.1406
2025-08-24 04:55:28 [INFO]: Epoch 004 - training loss (MAE): 0.4741, validation MSE: 1.1188
2025-08-24 04:55:29 [INFO]: Epoch 005 - training loss (MAE): 0.4595, validation MSE: 1.0885
2025-08-24 04:55:29 [INFO]: Epoch 006 - training loss (MAE): 0.4675, validation MSE: 1.0522
2025-08-24 04:55:29 [INFO]: Epoch 007 - training loss (MAE): 0.4416, validation MSE: 1.0134
2025-08-24 04:55:29 [INFO]: Epoch 008 - training loss (MAE): 0.4329, validation MSE: 0.9812
2025-08-24 04:55:29 [INFO]: Epoch 009 - training loss (MAE): 0.4374, validation MSE: 0.9515
2025-08-24 04:55:29 [INFO]: Epoch 010 - training loss (MAE): 0.4232, validation MSE: 0.9363
2025-08-24 04:55:29 [INFO]: Epoch 011 - training loss (MAE): 0.4182, validation MSE: 0.9195
2025-08-24 04:55:29 [INFO]: Epoch 012 - training loss (MAE): 0.4133, validation MSE: 0.9002
2025-08-24 04:55:29 [INFO]: Epoch 013 - training loss (MAE): 0.4019, validation MSE: 0.8860
2025-08-24 04:55:29 [INFO]: Epoch 014 - training loss (MAE): 0.3966, validation MSE: 0.8691
2025-08-24 04:55:29 [INFO]: Epoch 015 - training loss (MAE): 0.3959, validation MSE: 0.8516
2025-08-24 04:55:29 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:55:31 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4631| MSE: 0.5882| RMSE: 0.7670| MRE: 0.5945| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:55:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:55:55 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:55:55 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:55:55 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:55:55 [INFO]: Loaded successfully!
2025-08-24 04:55:55 [INFO]: Total sample number: 192
2025-08-24 04:55:55 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:55:55 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:55:55 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:55:55 [INFO]: Number of steps: 48
2025-08-24 04:55:55 [INFO]: Number of features: 13
2025-08-24 04:55:55 [INFO]: Train set missing rate: 39.84%
2025-08-24 04:55:55 [INFO]: Validating set missing rate: 39.87%
2025-08-24 04:55:55 [INFO]: Test set missing rate: 40.52%
âœ… Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:55:55 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:55:55 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:55:55 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:55:55 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:55:55 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:55:55 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:56:04 [INFO]: Model placed on cuda:0
2025-08-24 04:56:04 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:56:05 [INFO]: Epoch 001 - training loss (MAE): 0.5125, validation MSE: 1.1646
2025-08-24 04:56:06 [INFO]: Epoch 002 - training loss (MAE): 0.4922, validation MSE: 1.1923
2025-08-24 04:56:06 [INFO]: Epoch 003 - training loss (MAE): 0.4708, validation MSE: 1.1516
2025-08-24 04:56:07 [INFO]: Epoch 004 - training loss (MAE): 0.4682, validation MSE: 1.1104
2025-08-24 04:56:08 [INFO]: Epoch 005 - training loss (MAE): 0.4530, validation MSE: 1.0784
2025-08-24 04:56:09 [INFO]: Epoch 006 - training loss (MAE): 0.4568, validation MSE: 1.0434
2025-08-24 04:56:09 [INFO]: Epoch 007 - training loss (MAE): 0.4348, validation MSE: 0.9995
2025-08-24 04:56:10 [INFO]: Epoch 008 - training loss (MAE): 0.4261, validation MSE: 0.9658
2025-08-24 04:56:11 [INFO]: Epoch 009 - training loss (MAE): 0.4278, validation MSE: 0.9393
2025-08-24 04:56:12 [INFO]: Epoch 010 - training loss (MAE): 0.4147, validation MSE: 0.9381
2025-08-24 04:56:13 [INFO]: Epoch 011 - training loss (MAE): 0.4121, validation MSE: 0.9283
2025-08-24 04:56:14 [INFO]: Epoch 012 - training loss (MAE): 0.4062, validation MSE: 0.9191
2025-08-24 04:56:14 [INFO]: Epoch 013 - training loss (MAE): 0.3925, validation MSE: 0.9151
2025-08-24 04:56:15 [INFO]: Epoch 014 - training loss (MAE): 0.3911, validation MSE: 0.8996
2025-08-24 04:56:16 [INFO]: Epoch 015 - training loss (MAE): 0.3864, validation MSE: 0.8804
2025-08-24 04:56:16 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:56:18 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4604| MSE: 0.5862| RMSE: 0.7656| MRE: 0.5911| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:56:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:56:43 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:56:43 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:56:43 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:56:43 [INFO]: Loaded successfully!
2025-08-24 04:56:43 [INFO]: Total sample number: 192
2025-08-24 04:56:43 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:56:43 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:56:43 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:56:43 [INFO]: Number of steps: 48
2025-08-24 04:56:43 [INFO]: Number of features: 13
2025-08-24 04:56:43 [INFO]: Train set missing rate: 39.84%
2025-08-24 04:56:43 [INFO]: Validating set missing rate: 39.87%
2025-08-24 04:56:43 [INFO]: Test set missing rate: 40.52%
âœ… Dataset 'italy_air_quality' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 04:56:43 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:56:43 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:56:44 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:56:44 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:56:44 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:56:44 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:56:52 [INFO]: Model placed on cuda:0
2025-08-24 04:56:52 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:56:52 [INFO]: Epoch 001 - training loss (MAE): 0.5108, validation MSE: 1.1362
2025-08-24 04:56:52 [INFO]: Epoch 002 - training loss (MAE): 0.4951, validation MSE: 1.1347
2025-08-24 04:56:52 [INFO]: Epoch 003 - training loss (MAE): 0.4701, validation MSE: 1.1406
2025-08-24 04:56:52 [INFO]: Epoch 004 - training loss (MAE): 0.4741, validation MSE: 1.1188
2025-08-24 04:56:52 [INFO]: Epoch 005 - training loss (MAE): 0.4595, validation MSE: 1.0885
2025-08-24 04:56:52 [INFO]: Epoch 006 - training loss (MAE): 0.4675, validation MSE: 1.0522
2025-08-24 04:56:52 [INFO]: Epoch 007 - training loss (MAE): 0.4416, validation MSE: 1.0134
2025-08-24 04:56:52 [INFO]: Epoch 008 - training loss (MAE): 0.4329, validation MSE: 0.9812
2025-08-24 04:56:52 [INFO]: Epoch 009 - training loss (MAE): 0.4374, validation MSE: 0.9515
2025-08-24 04:56:52 [INFO]: Epoch 010 - training loss (MAE): 0.4232, validation MSE: 0.9363
2025-08-24 04:56:52 [INFO]: Epoch 011 - training loss (MAE): 0.4182, validation MSE: 0.9195
2025-08-24 04:56:52 [INFO]: Epoch 012 - training loss (MAE): 0.4133, validation MSE: 0.9002
2025-08-24 04:56:52 [INFO]: Epoch 013 - training loss (MAE): 0.4019, validation MSE: 0.8860
2025-08-24 04:56:53 [INFO]: Epoch 014 - training loss (MAE): 0.3966, validation MSE: 0.8691
2025-08-24 04:56:53 [INFO]: Epoch 015 - training loss (MAE): 0.3959, validation MSE: 0.8516
2025-08-24 04:56:53 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:56:54 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4631| MSE: 0.5882| RMSE: 0.7670| MRE: 0.5945| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:57:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:57:24 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:57:24 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:57:24 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:57:24 [INFO]: Loaded successfully!
2025-08-24 04:57:24 [INFO]: Total sample number: 192
2025-08-24 04:57:24 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:57:24 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:57:24 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:57:24 [INFO]: Number of steps: 48
2025-08-24 04:57:24 [INFO]: Number of features: 13
2025-08-24 04:57:24 [INFO]: Train set missing rate: 50.11%
2025-08-24 04:57:24 [INFO]: Validating set missing rate: 50.03%
2025-08-24 04:57:24 [INFO]: Test set missing rate: 50.12%
âœ… Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:57:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:57:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:57:24 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 04:57:24 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 04:57:24 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:57:24 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:57:31 [INFO]: Model placed on cuda:0
2025-08-24 04:57:31 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:57:32 [INFO]: Epoch 001 - training loss (MAE): 0.5296, validation MSE: 1.3936
2025-08-24 04:57:33 [INFO]: Epoch 002 - training loss (MAE): 0.5153, validation MSE: 1.3925
2025-08-24 04:57:34 [INFO]: Epoch 003 - training loss (MAE): 0.4974, validation MSE: 1.3472
2025-08-24 04:57:35 [INFO]: Epoch 004 - training loss (MAE): 0.4940, validation MSE: 1.3003
2025-08-24 04:57:36 [INFO]: Epoch 005 - training loss (MAE): 0.4748, validation MSE: 1.2629
2025-08-24 04:57:37 [INFO]: Epoch 006 - training loss (MAE): 0.4746, validation MSE: 1.2145
2025-08-24 04:57:38 [INFO]: Epoch 007 - training loss (MAE): 0.4528, validation MSE: 1.1732
2025-08-24 04:57:38 [INFO]: Epoch 008 - training loss (MAE): 0.4393, validation MSE: 1.1295
2025-08-24 04:57:39 [INFO]: Epoch 009 - training loss (MAE): 0.4450, validation MSE: 1.0803
2025-08-24 04:57:40 [INFO]: Epoch 010 - training loss (MAE): 0.4275, validation MSE: 1.0853
2025-08-24 04:57:41 [INFO]: Epoch 011 - training loss (MAE): 0.4236, validation MSE: 1.0368
2025-08-24 04:57:42 [INFO]: Epoch 012 - training loss (MAE): 0.4173, validation MSE: 1.0410
2025-08-24 04:57:43 [INFO]: Epoch 013 - training loss (MAE): 0.4067, validation MSE: 1.1081
2025-08-24 04:57:44 [INFO]: Epoch 014 - training loss (MAE): 0.4068, validation MSE: 1.0871
2025-08-24 04:57:45 [INFO]: Epoch 015 - training loss (MAE): 0.4013, validation MSE: 1.0659
2025-08-24 04:57:45 [INFO]: Finished training. The best model is from epoch#11.
2025-08-24 04:57:48 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5175| MSE: 0.7169| RMSE: 0.8467| MRE: 0.6665| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:58:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:58:14 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:58:14 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:58:14 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:58:14 [INFO]: Loaded successfully!
2025-08-24 04:58:14 [INFO]: Total sample number: 192
2025-08-24 04:58:14 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:58:14 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:58:14 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:58:14 [INFO]: Number of steps: 48
2025-08-24 04:58:14 [INFO]: Number of features: 13
2025-08-24 04:58:14 [INFO]: Train set missing rate: 50.11%
2025-08-24 04:58:14 [INFO]: Validating set missing rate: 50.03%
2025-08-24 04:58:14 [INFO]: Test set missing rate: 50.12%
âœ… Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:58:14 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:58:14 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:58:15 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 04:58:15 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 04:58:15 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:58:15 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:58:22 [INFO]: Model placed on cuda:0
2025-08-24 04:58:22 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:58:22 [INFO]: Epoch 001 - training loss (MAE): 0.5271, validation MSE: 1.3394
2025-08-24 04:58:22 [INFO]: Epoch 002 - training loss (MAE): 0.5168, validation MSE: 1.3423
2025-08-24 04:58:22 [INFO]: Epoch 003 - training loss (MAE): 0.4960, validation MSE: 1.3526
2025-08-24 04:58:22 [INFO]: Epoch 004 - training loss (MAE): 0.5009, validation MSE: 1.3411
2025-08-24 04:58:22 [INFO]: Epoch 005 - training loss (MAE): 0.4842, validation MSE: 1.3218
2025-08-24 04:58:22 [INFO]: Epoch 006 - training loss (MAE): 0.4927, validation MSE: 1.2909
2025-08-24 04:58:22 [INFO]: Epoch 007 - training loss (MAE): 0.4719, validation MSE: 1.2523
2025-08-24 04:58:22 [INFO]: Epoch 008 - training loss (MAE): 0.4624, validation MSE: 1.2158
2025-08-24 04:58:22 [INFO]: Epoch 009 - training loss (MAE): 0.4762, validation MSE: 1.1801
2025-08-24 04:58:23 [INFO]: Epoch 010 - training loss (MAE): 0.4530, validation MSE: 1.1586
2025-08-24 04:58:23 [INFO]: Epoch 011 - training loss (MAE): 0.4524, validation MSE: 1.1457
2025-08-24 04:58:23 [INFO]: Epoch 012 - training loss (MAE): 0.4445, validation MSE: 1.1241
2025-08-24 04:58:23 [INFO]: Epoch 013 - training loss (MAE): 0.4357, validation MSE: 1.1121
2025-08-24 04:58:23 [INFO]: Epoch 014 - training loss (MAE): 0.4359, validation MSE: 1.0944
2025-08-24 04:58:23 [INFO]: Epoch 015 - training loss (MAE): 0.4304, validation MSE: 1.0742
2025-08-24 04:58:23 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:58:24 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5159| MSE: 0.7187| RMSE: 0.8477| MRE: 0.6644| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:58:47 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:58:47 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:58:47 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:58:47 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:58:47 [INFO]: Loaded successfully!
2025-08-24 04:58:47 [INFO]: Total sample number: 192
2025-08-24 04:58:47 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:58:47 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:58:47 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:58:47 [INFO]: Number of steps: 48
2025-08-24 04:58:47 [INFO]: Number of features: 13
2025-08-24 04:58:47 [INFO]: Train set missing rate: 50.11%
2025-08-24 04:58:47 [INFO]: Validating set missing rate: 50.03%
2025-08-24 04:58:47 [INFO]: Test set missing rate: 50.12%
âœ… Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:58:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:58:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:58:48 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 04:58:48 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 04:58:48 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:58:48 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:58:54 [INFO]: Model placed on cuda:0
2025-08-24 04:58:54 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:58:55 [INFO]: Epoch 001 - training loss (MAE): 0.5310, validation MSE: 1.3430
2025-08-24 04:58:56 [INFO]: Epoch 002 - training loss (MAE): 0.5144, validation MSE: 1.3812
2025-08-24 04:58:56 [INFO]: Epoch 003 - training loss (MAE): 0.4964, validation MSE: 1.3654
2025-08-24 04:58:57 [INFO]: Epoch 004 - training loss (MAE): 0.4957, validation MSE: 1.3319
2025-08-24 04:58:58 [INFO]: Epoch 005 - training loss (MAE): 0.4788, validation MSE: 1.2972
2025-08-24 04:58:59 [INFO]: Epoch 006 - training loss (MAE): 0.4847, validation MSE: 1.2551
2025-08-24 04:59:00 [INFO]: Epoch 007 - training loss (MAE): 0.4671, validation MSE: 1.2160
2025-08-24 04:59:01 [INFO]: Epoch 008 - training loss (MAE): 0.4555, validation MSE: 1.1850
2025-08-24 04:59:01 [INFO]: Epoch 009 - training loss (MAE): 0.4641, validation MSE: 1.1534
2025-08-24 04:59:02 [INFO]: Epoch 010 - training loss (MAE): 0.4440, validation MSE: 1.1343
2025-08-24 04:59:03 [INFO]: Epoch 011 - training loss (MAE): 0.4445, validation MSE: 1.1192
2025-08-24 04:59:04 [INFO]: Epoch 012 - training loss (MAE): 0.4368, validation MSE: 1.1075
2025-08-24 04:59:05 [INFO]: Epoch 013 - training loss (MAE): 0.4254, validation MSE: 1.1160
2025-08-24 04:59:05 [INFO]: Epoch 014 - training loss (MAE): 0.4264, validation MSE: 1.1011
2025-08-24 04:59:06 [INFO]: Epoch 015 - training loss (MAE): 0.4175, validation MSE: 1.0792
2025-08-24 04:59:06 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:59:08 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5112| MSE: 0.7114| RMSE: 0.8435| MRE: 0.6584| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=48, N_FEATURES=36)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 04:59:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 04:59:31 [INFO]: You're using dataset italy_air_quality, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/italy_air_quality
2025-08-24 04:59:31 [INFO]: Dataset italy_air_quality has already been downloaded. Processing directly...
2025-08-24 04:59:31 [INFO]: Dataset italy_air_quality has already been cached. Loading from cache directly...
2025-08-24 04:59:31 [INFO]: Loaded successfully!
2025-08-24 04:59:31 [INFO]: Total sample number: 192
2025-08-24 04:59:31 [INFO]: Training set size: 116 (60.42%)
2025-08-24 04:59:31 [INFO]: Validation set size: 38 (19.79%)
2025-08-24 04:59:31 [INFO]: Test set size: 38 (19.79%)
2025-08-24 04:59:31 [INFO]: Number of steps: 48
2025-08-24 04:59:31 [INFO]: Number of features: 13
2025-08-24 04:59:31 [INFO]: Train set missing rate: 50.11%
2025-08-24 04:59:31 [INFO]: Validating set missing rate: 50.03%
2025-08-24 04:59:31 [INFO]: Test set missing rate: 50.12%
âœ… Dataset 'italy_air_quality' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 04:59:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 04:59:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 04:59:31 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 04:59:31 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 04:59:31 [INFO]: Using customized MAE as the training loss function.
2025-08-24 04:59:31 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 04:59:38 [INFO]: Model placed on cuda:0
2025-08-24 04:59:38 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 04:59:38 [INFO]: Epoch 001 - training loss (MAE): 0.5271, validation MSE: 1.3394
2025-08-24 04:59:38 [INFO]: Epoch 002 - training loss (MAE): 0.5168, validation MSE: 1.3423
2025-08-24 04:59:38 [INFO]: Epoch 003 - training loss (MAE): 0.4960, validation MSE: 1.3526
2025-08-24 04:59:39 [INFO]: Epoch 004 - training loss (MAE): 0.5009, validation MSE: 1.3411
2025-08-24 04:59:39 [INFO]: Epoch 005 - training loss (MAE): 0.4842, validation MSE: 1.3218
2025-08-24 04:59:39 [INFO]: Epoch 006 - training loss (MAE): 0.4927, validation MSE: 1.2909
2025-08-24 04:59:39 [INFO]: Epoch 007 - training loss (MAE): 0.4719, validation MSE: 1.2523
2025-08-24 04:59:39 [INFO]: Epoch 008 - training loss (MAE): 0.4624, validation MSE: 1.2158
2025-08-24 04:59:39 [INFO]: Epoch 009 - training loss (MAE): 0.4762, validation MSE: 1.1801
2025-08-24 04:59:39 [INFO]: Epoch 010 - training loss (MAE): 0.4530, validation MSE: 1.1586
2025-08-24 04:59:39 [INFO]: Epoch 011 - training loss (MAE): 0.4524, validation MSE: 1.1457
2025-08-24 04:59:39 [INFO]: Epoch 012 - training loss (MAE): 0.4445, validation MSE: 1.1241
2025-08-24 04:59:39 [INFO]: Epoch 013 - training loss (MAE): 0.4357, validation MSE: 1.1121
2025-08-24 04:59:39 [INFO]: Epoch 014 - training loss (MAE): 0.4359, validation MSE: 1.0944
2025-08-24 04:59:39 [INFO]: Epoch 015 - training loss (MAE): 0.4304, validation MSE: 1.0742
2025-08-24 04:59:39 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 04:59:40 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.5159| MSE: 0.7187| RMSE: 0.8477| MRE: 0.6644| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/italy_air_quality/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: italy_air_quality | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:00:06 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:00:06 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:00:06 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:00:06 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:00:06 [INFO]: Loaded successfully!
2025-08-24 05:00:06 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:00:06 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:00:06 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:00:07 [INFO]: Total sample number: 182
2025-08-24 05:00:07 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:00:07 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:00:07 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:00:07 [INFO]: Number of steps: 96
2025-08-24 05:00:07 [INFO]: Number of features: 862
2025-08-24 05:00:07 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:00:07 [INFO]: Validating set missing rate: 10.03%
2025-08-24 05:00:07 [INFO]: Test set missing rate: 10.00%
âœ… Dataset 'pems_traffic' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:00:07 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:00:07 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:00:07 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:00:07 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:00:07 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:00:07 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:00:14 [INFO]: Model placed on cuda:0
2025-08-24 05:00:14 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:00:16 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:00:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:00:43 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:00:43 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:00:43 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:00:43 [INFO]: Loaded successfully!
2025-08-24 05:00:43 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:00:43 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:00:43 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:00:44 [INFO]: Total sample number: 182
2025-08-24 05:00:44 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:00:44 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:00:44 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:00:44 [INFO]: Number of steps: 96
2025-08-24 05:00:44 [INFO]: Number of features: 862
2025-08-24 05:00:44 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:00:44 [INFO]: Validating set missing rate: 10.03%
2025-08-24 05:00:44 [INFO]: Test set missing rate: 10.00%
âœ… Dataset 'pems_traffic' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:00:44 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:00:44 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:00:44 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:00:44 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:00:44 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:00:44 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:00:51 [INFO]: Model placed on cuda:0
2025-08-24 05:00:51 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:00:55 [INFO]: Epoch 001 - training loss (MAE): 0.7066, validation MSE: 0.9705
2025-08-24 05:00:58 [INFO]: Epoch 002 - training loss (MAE): 0.5878, validation MSE: 0.7816
2025-08-24 05:00:59 [INFO]: Epoch 003 - training loss (MAE): 0.5128, validation MSE: 0.7021
2025-08-24 05:01:02 [INFO]: Epoch 004 - training loss (MAE): 0.4828, validation MSE: 0.6847
2025-08-24 05:01:04 [INFO]: Epoch 005 - training loss (MAE): 0.4824, validation MSE: 0.6663
2025-08-24 05:01:06 [INFO]: Epoch 006 - training loss (MAE): 0.4766, validation MSE: 0.6327
2025-08-24 05:01:07 [INFO]: Epoch 007 - training loss (MAE): 0.4636, validation MSE: 0.5999
2025-08-24 05:01:09 [INFO]: Epoch 008 - training loss (MAE): 0.4502, validation MSE: 0.5748
2025-08-24 05:01:10 [INFO]: Epoch 009 - training loss (MAE): 0.4416, validation MSE: 0.5591
2025-08-24 05:01:11 [INFO]: Epoch 010 - training loss (MAE): 0.4354, validation MSE: 0.5499
2025-08-24 05:01:13 [INFO]: Epoch 011 - training loss (MAE): 0.4313, validation MSE: 0.5385
2025-08-24 05:01:14 [INFO]: Epoch 012 - training loss (MAE): 0.4221, validation MSE: 0.5206
2025-08-24 05:01:15 [INFO]: Epoch 013 - training loss (MAE): 0.4177, validation MSE: 0.5011
2025-08-24 05:01:17 [INFO]: Epoch 014 - training loss (MAE): 0.4113, validation MSE: 0.4842
2025-08-24 05:01:19 [INFO]: Epoch 015 - training loss (MAE): 0.4076, validation MSE: 0.4687
2025-08-24 05:01:19 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:01:21 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3909| MSE: 0.5883| RMSE: 0.7670| MRE: 0.4848| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:01:46 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:01:46 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:01:46 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:01:46 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:01:46 [INFO]: Loaded successfully!
2025-08-24 05:01:46 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:01:46 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:01:46 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:01:47 [INFO]: Total sample number: 182
2025-08-24 05:01:47 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:01:47 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:01:47 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:01:47 [INFO]: Number of steps: 96
2025-08-24 05:01:47 [INFO]: Number of features: 862
2025-08-24 05:01:47 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:01:47 [INFO]: Validating set missing rate: 10.03%
2025-08-24 05:01:47 [INFO]: Test set missing rate: 10.00%
âœ… Dataset 'pems_traffic' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:01:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:01:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:01:47 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:01:47 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:01:47 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:01:47 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:01:54 [INFO]: Model placed on cuda:0
2025-08-24 05:01:54 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:01:56 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:02:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:02:22 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:02:22 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:02:22 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:02:22 [INFO]: Loaded successfully!
2025-08-24 05:02:22 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:02:22 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:02:22 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:02:23 [INFO]: Total sample number: 182
2025-08-24 05:02:23 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:02:23 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:02:23 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:02:23 [INFO]: Number of steps: 96
2025-08-24 05:02:23 [INFO]: Number of features: 862
2025-08-24 05:02:23 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:02:23 [INFO]: Validating set missing rate: 10.03%
2025-08-24 05:02:23 [INFO]: Test set missing rate: 10.00%
âœ… Dataset 'pems_traffic' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:02:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:02:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:02:23 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:02:23 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:02:23 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:02:23 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:02:31 [INFO]: Model placed on cuda:0
2025-08-24 05:02:31 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:02:37 [INFO]: Epoch 001 - training loss (MAE): 0.7066, validation MSE: 0.9705
2025-08-24 05:02:42 [INFO]: Epoch 002 - training loss (MAE): 0.5878, validation MSE: 0.7816
2025-08-24 05:02:47 [INFO]: Epoch 003 - training loss (MAE): 0.5128, validation MSE: 0.7021
2025-08-24 05:02:51 [INFO]: Epoch 004 - training loss (MAE): 0.4828, validation MSE: 0.6847
2025-08-24 05:02:53 [INFO]: Epoch 005 - training loss (MAE): 0.4824, validation MSE: 0.6663
2025-08-24 05:03:01 [INFO]: Epoch 006 - training loss (MAE): 0.4766, validation MSE: 0.6327
2025-08-24 05:03:05 [INFO]: Epoch 007 - training loss (MAE): 0.4636, validation MSE: 0.5999
2025-08-24 05:03:11 [INFO]: Epoch 008 - training loss (MAE): 0.4502, validation MSE: 0.5748
2025-08-24 05:03:16 [INFO]: Epoch 009 - training loss (MAE): 0.4416, validation MSE: 0.5591
2025-08-24 05:03:21 [INFO]: Epoch 010 - training loss (MAE): 0.4354, validation MSE: 0.5499
2025-08-24 05:03:27 [INFO]: Epoch 011 - training loss (MAE): 0.4313, validation MSE: 0.5385
2025-08-24 05:03:32 [INFO]: Epoch 012 - training loss (MAE): 0.4221, validation MSE: 0.5206
2025-08-24 05:03:37 [INFO]: Epoch 013 - training loss (MAE): 0.4177, validation MSE: 0.5011
2025-08-24 05:03:41 [INFO]: Epoch 014 - training loss (MAE): 0.4113, validation MSE: 0.4842
2025-08-24 05:03:45 [INFO]: Epoch 015 - training loss (MAE): 0.4076, validation MSE: 0.4687
2025-08-24 05:03:45 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:03:46 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.3909| MSE: 0.5883| RMSE: 0.7670| MRE: 0.4848| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:04:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:04:16 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:04:16 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:04:16 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:04:16 [INFO]: Loaded successfully!
2025-08-24 05:04:16 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:04:16 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:04:16 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:04:16 [INFO]: Total sample number: 182
2025-08-24 05:04:16 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:04:16 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:04:16 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:04:16 [INFO]: Number of steps: 96
2025-08-24 05:04:16 [INFO]: Number of features: 862
2025-08-24 05:04:16 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:04:16 [INFO]: Validating set missing rate: 20.00%
2025-08-24 05:04:16 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'pems_traffic' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:04:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:04:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:04:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:04:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:04:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:04:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:04:24 [INFO]: Model placed on cuda:0
2025-08-24 05:04:24 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:04:25 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:04:49 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:04:49 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:04:49 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:04:49 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:04:49 [INFO]: Loaded successfully!
2025-08-24 05:04:49 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:04:49 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:04:49 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:04:50 [INFO]: Total sample number: 182
2025-08-24 05:04:50 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:04:50 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:04:50 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:04:50 [INFO]: Number of steps: 96
2025-08-24 05:04:50 [INFO]: Number of features: 862
2025-08-24 05:04:50 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:04:50 [INFO]: Validating set missing rate: 20.00%
2025-08-24 05:04:50 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'pems_traffic' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:04:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:04:50 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:04:50 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:04:50 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:04:50 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:04:50 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:04:57 [INFO]: Model placed on cuda:0
2025-08-24 05:04:57 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:05:05 [INFO]: Epoch 001 - training loss (MAE): 0.7074, validation MSE: 0.9872
2025-08-24 05:05:09 [INFO]: Epoch 002 - training loss (MAE): 0.5953, validation MSE: 0.8044
2025-08-24 05:05:13 [INFO]: Epoch 003 - training loss (MAE): 0.5215, validation MSE: 0.7206
2025-08-24 05:05:17 [INFO]: Epoch 004 - training loss (MAE): 0.4861, validation MSE: 0.7005
2025-08-24 05:05:23 [INFO]: Epoch 005 - training loss (MAE): 0.4833, validation MSE: 0.6880
2025-08-24 05:05:28 [INFO]: Epoch 006 - training loss (MAE): 0.4806, validation MSE: 0.6600
2025-08-24 05:05:35 [INFO]: Epoch 007 - training loss (MAE): 0.4701, validation MSE: 0.6281
2025-08-24 05:05:42 [INFO]: Epoch 008 - training loss (MAE): 0.4569, validation MSE: 0.6019
2025-08-24 05:05:47 [INFO]: Epoch 009 - training loss (MAE): 0.4485, validation MSE: 0.5855
2025-08-24 05:05:54 [INFO]: Epoch 010 - training loss (MAE): 0.4427, validation MSE: 0.5769
2025-08-24 05:06:00 [INFO]: Epoch 011 - training loss (MAE): 0.4388, validation MSE: 0.5693
2025-08-24 05:06:07 [INFO]: Epoch 012 - training loss (MAE): 0.4300, validation MSE: 0.5567
2025-08-24 05:06:14 [INFO]: Epoch 013 - training loss (MAE): 0.4260, validation MSE: 0.5400
2025-08-24 05:06:19 [INFO]: Epoch 014 - training loss (MAE): 0.4198, validation MSE: 0.5233
2025-08-24 05:06:22 [INFO]: Epoch 015 - training loss (MAE): 0.4160, validation MSE: 0.5072
2025-08-24 05:06:22 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:06:23 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4052| MSE: 0.6298| RMSE: 0.7936| MRE: 0.5032| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:06:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:06:54 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:06:54 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:06:54 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:06:54 [INFO]: Loaded successfully!
2025-08-24 05:06:54 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:06:54 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:06:54 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:06:54 [INFO]: Total sample number: 182
2025-08-24 05:06:54 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:06:54 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:06:54 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:06:54 [INFO]: Number of steps: 96
2025-08-24 05:06:54 [INFO]: Number of features: 862
2025-08-24 05:06:54 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:06:54 [INFO]: Validating set missing rate: 20.00%
2025-08-24 05:06:54 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'pems_traffic' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:06:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:06:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:06:55 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:06:55 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:06:55 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:06:55 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:07:03 [INFO]: Model placed on cuda:0
2025-08-24 05:07:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:07:04 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:07:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:07:34 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:07:34 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:07:34 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:07:34 [INFO]: Loaded successfully!
2025-08-24 05:07:34 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:07:34 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:07:34 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:07:35 [INFO]: Total sample number: 182
2025-08-24 05:07:35 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:07:35 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:07:35 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:07:35 [INFO]: Number of steps: 96
2025-08-24 05:07:35 [INFO]: Number of features: 862
2025-08-24 05:07:35 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:07:35 [INFO]: Validating set missing rate: 20.00%
2025-08-24 05:07:35 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'pems_traffic' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:07:35 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:07:35 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:07:35 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:07:35 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:07:35 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:07:35 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:07:43 [INFO]: Model placed on cuda:0
2025-08-24 05:07:43 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:07:46 [INFO]: Epoch 001 - training loss (MAE): 0.7074, validation MSE: 0.9872
2025-08-24 05:07:48 [INFO]: Epoch 002 - training loss (MAE): 0.5953, validation MSE: 0.8044
2025-08-24 05:07:51 [INFO]: Epoch 003 - training loss (MAE): 0.5215, validation MSE: 0.7206
2025-08-24 05:07:53 [INFO]: Epoch 004 - training loss (MAE): 0.4861, validation MSE: 0.7005
2025-08-24 05:07:55 [INFO]: Epoch 005 - training loss (MAE): 0.4833, validation MSE: 0.6880
2025-08-24 05:07:58 [INFO]: Epoch 006 - training loss (MAE): 0.4806, validation MSE: 0.6600
2025-08-24 05:07:59 [INFO]: Epoch 007 - training loss (MAE): 0.4701, validation MSE: 0.6281
2025-08-24 05:08:01 [INFO]: Epoch 008 - training loss (MAE): 0.4569, validation MSE: 0.6019
2025-08-24 05:08:04 [INFO]: Epoch 009 - training loss (MAE): 0.4485, validation MSE: 0.5855
2025-08-24 05:08:07 [INFO]: Epoch 010 - training loss (MAE): 0.4427, validation MSE: 0.5769
2025-08-24 05:08:10 [INFO]: Epoch 011 - training loss (MAE): 0.4388, validation MSE: 0.5693
2025-08-24 05:08:12 [INFO]: Epoch 012 - training loss (MAE): 0.4300, validation MSE: 0.5567
2025-08-24 05:08:15 [INFO]: Epoch 013 - training loss (MAE): 0.4260, validation MSE: 0.5400
2025-08-24 05:08:19 [INFO]: Epoch 014 - training loss (MAE): 0.4198, validation MSE: 0.5233
2025-08-24 05:08:22 [INFO]: Epoch 015 - training loss (MAE): 0.4160, validation MSE: 0.5072
2025-08-24 05:08:22 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:08:24 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4052| MSE: 0.6298| RMSE: 0.7936| MRE: 0.5032| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:08:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:08:54 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:08:54 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:08:54 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:08:54 [INFO]: Loaded successfully!
2025-08-24 05:08:54 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:08:54 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:08:54 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:08:54 [INFO]: Total sample number: 182
2025-08-24 05:08:54 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:08:54 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:08:54 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:08:54 [INFO]: Number of steps: 96
2025-08-24 05:08:54 [INFO]: Number of features: 862
2025-08-24 05:08:54 [INFO]: Train set missing rate: 30.01%
2025-08-24 05:08:54 [INFO]: Validating set missing rate: 29.97%
2025-08-24 05:08:54 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'pems_traffic' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:08:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:08:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:08:55 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:08:55 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:08:55 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:08:55 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:09:03 [INFO]: Model placed on cuda:0
2025-08-24 05:09:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:09:05 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:09:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:09:39 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:09:39 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:09:39 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:09:39 [INFO]: Loaded successfully!
2025-08-24 05:09:39 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:09:39 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:09:39 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:09:39 [INFO]: Total sample number: 182
2025-08-24 05:09:39 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:09:39 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:09:39 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:09:39 [INFO]: Number of steps: 96
2025-08-24 05:09:39 [INFO]: Number of features: 862
2025-08-24 05:09:39 [INFO]: Train set missing rate: 30.01%
2025-08-24 05:09:39 [INFO]: Validating set missing rate: 29.97%
2025-08-24 05:09:39 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'pems_traffic' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:09:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:09:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:09:40 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:09:40 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:09:40 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:09:40 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:09:47 [INFO]: Model placed on cuda:0
2025-08-24 05:09:47 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:09:51 [INFO]: Epoch 001 - training loss (MAE): 0.7086, validation MSE: 1.0063
2025-08-24 05:09:52 [INFO]: Epoch 002 - training loss (MAE): 0.6039, validation MSE: 0.8315
2025-08-24 05:09:55 [INFO]: Epoch 003 - training loss (MAE): 0.5321, validation MSE: 0.7437
2025-08-24 05:09:58 [INFO]: Epoch 004 - training loss (MAE): 0.4920, validation MSE: 0.7172
2025-08-24 05:10:00 [INFO]: Epoch 005 - training loss (MAE): 0.4852, validation MSE: 0.7073
2025-08-24 05:10:03 [INFO]: Epoch 006 - training loss (MAE): 0.4840, validation MSE: 0.6854
2025-08-24 05:10:05 [INFO]: Epoch 007 - training loss (MAE): 0.4771, validation MSE: 0.6561
2025-08-24 05:10:07 [INFO]: Epoch 008 - training loss (MAE): 0.4644, validation MSE: 0.6291
2025-08-24 05:10:09 [INFO]: Epoch 009 - training loss (MAE): 0.4562, validation MSE: 0.6107
2025-08-24 05:10:13 [INFO]: Epoch 010 - training loss (MAE): 0.4509, validation MSE: 0.6003
2025-08-24 05:10:15 [INFO]: Epoch 011 - training loss (MAE): 0.4471, validation MSE: 0.5939
2025-08-24 05:10:17 [INFO]: Epoch 012 - training loss (MAE): 0.4389, validation MSE: 0.5854
2025-08-24 05:10:21 [INFO]: Epoch 013 - training loss (MAE): 0.4357, validation MSE: 0.5728
2025-08-24 05:10:24 [INFO]: Epoch 014 - training loss (MAE): 0.4293, validation MSE: 0.5574
2025-08-24 05:10:28 [INFO]: Epoch 015 - training loss (MAE): 0.4259, validation MSE: 0.5408
2025-08-24 05:10:28 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:10:30 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4218| MSE: 0.6780| RMSE: 0.8234| MRE: 0.5234| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:10:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:10:54 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:10:54 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:10:54 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:10:54 [INFO]: Loaded successfully!
2025-08-24 05:10:54 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:10:54 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:10:54 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:10:54 [INFO]: Total sample number: 182
2025-08-24 05:10:54 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:10:54 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:10:54 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:10:54 [INFO]: Number of steps: 96
2025-08-24 05:10:54 [INFO]: Number of features: 862
2025-08-24 05:10:54 [INFO]: Train set missing rate: 30.01%
2025-08-24 05:10:54 [INFO]: Validating set missing rate: 29.97%
2025-08-24 05:10:54 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'pems_traffic' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:10:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:10:54 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:10:55 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:10:55 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:10:55 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:10:55 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:11:02 [INFO]: Model placed on cuda:0
2025-08-24 05:11:02 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:11:04 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:11:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:11:31 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:11:31 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:11:31 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:11:31 [INFO]: Loaded successfully!
2025-08-24 05:11:31 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:11:31 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:11:31 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:11:31 [INFO]: Total sample number: 182
2025-08-24 05:11:31 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:11:31 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:11:31 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:11:31 [INFO]: Number of steps: 96
2025-08-24 05:11:31 [INFO]: Number of features: 862
2025-08-24 05:11:31 [INFO]: Train set missing rate: 30.01%
2025-08-24 05:11:31 [INFO]: Validating set missing rate: 29.97%
2025-08-24 05:11:31 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'pems_traffic' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:11:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:11:31 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:11:32 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:11:32 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:11:32 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:11:32 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:11:38 [INFO]: Model placed on cuda:0
2025-08-24 05:11:38 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:11:43 [INFO]: Epoch 001 - training loss (MAE): 0.7086, validation MSE: 1.0063
2025-08-24 05:11:45 [INFO]: Epoch 002 - training loss (MAE): 0.6039, validation MSE: 0.8315
2025-08-24 05:11:48 [INFO]: Epoch 003 - training loss (MAE): 0.5321, validation MSE: 0.7437
2025-08-24 05:11:49 [INFO]: Epoch 004 - training loss (MAE): 0.4920, validation MSE: 0.7172
2025-08-24 05:11:51 [INFO]: Epoch 005 - training loss (MAE): 0.4852, validation MSE: 0.7073
2025-08-24 05:11:52 [INFO]: Epoch 006 - training loss (MAE): 0.4840, validation MSE: 0.6854
2025-08-24 05:11:54 [INFO]: Epoch 007 - training loss (MAE): 0.4771, validation MSE: 0.6561
2025-08-24 05:11:56 [INFO]: Epoch 008 - training loss (MAE): 0.4644, validation MSE: 0.6291
2025-08-24 05:11:58 [INFO]: Epoch 009 - training loss (MAE): 0.4562, validation MSE: 0.6107
2025-08-24 05:11:59 [INFO]: Epoch 010 - training loss (MAE): 0.4509, validation MSE: 0.6003
2025-08-24 05:12:01 [INFO]: Epoch 011 - training loss (MAE): 0.4471, validation MSE: 0.5939
2025-08-24 05:12:03 [INFO]: Epoch 012 - training loss (MAE): 0.4389, validation MSE: 0.5854
2025-08-24 05:12:05 [INFO]: Epoch 013 - training loss (MAE): 0.4357, validation MSE: 0.5728
2025-08-24 05:12:08 [INFO]: Epoch 014 - training loss (MAE): 0.4293, validation MSE: 0.5574
2025-08-24 05:12:10 [INFO]: Epoch 015 - training loss (MAE): 0.4259, validation MSE: 0.5408
2025-08-24 05:12:10 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:12:12 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4218| MSE: 0.6780| RMSE: 0.8234| MRE: 0.5234| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:12:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:12:41 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:12:41 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:12:41 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:12:41 [INFO]: Loaded successfully!
2025-08-24 05:12:41 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:12:41 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:12:41 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:12:41 [INFO]: Total sample number: 182
2025-08-24 05:12:41 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:12:41 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:12:41 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:12:41 [INFO]: Number of steps: 96
2025-08-24 05:12:41 [INFO]: Number of features: 862
2025-08-24 05:12:41 [INFO]: Train set missing rate: 40.01%
2025-08-24 05:12:41 [INFO]: Validating set missing rate: 39.94%
2025-08-24 05:12:41 [INFO]: Test set missing rate: 40.04%
âœ… Dataset 'pems_traffic' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:12:41 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:12:41 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:12:42 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:12:42 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:12:42 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:12:42 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:12:48 [INFO]: Model placed on cuda:0
2025-08-24 05:12:48 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:12:50 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:13:13 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:13:13 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:13:13 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:13:13 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:13:13 [INFO]: Loaded successfully!
2025-08-24 05:13:13 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:13:13 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:13:13 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:13:13 [INFO]: Total sample number: 182
2025-08-24 05:13:13 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:13:13 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:13:13 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:13:13 [INFO]: Number of steps: 96
2025-08-24 05:13:13 [INFO]: Number of features: 862
2025-08-24 05:13:13 [INFO]: Train set missing rate: 40.01%
2025-08-24 05:13:13 [INFO]: Validating set missing rate: 39.94%
2025-08-24 05:13:13 [INFO]: Test set missing rate: 40.04%
âœ… Dataset 'pems_traffic' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:13:13 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:13:13 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:13:14 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:13:14 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:13:14 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:13:14 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:13:22 [INFO]: Model placed on cuda:0
2025-08-24 05:13:22 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:13:25 [INFO]: Epoch 001 - training loss (MAE): 0.7100, validation MSE: 1.0190
2025-08-24 05:13:31 [INFO]: Epoch 002 - training loss (MAE): 0.6130, validation MSE: 0.8540
2025-08-24 05:13:35 [INFO]: Epoch 003 - training loss (MAE): 0.5444, validation MSE: 0.7638
2025-08-24 05:13:41 [INFO]: Epoch 004 - training loss (MAE): 0.5007, validation MSE: 0.7308
2025-08-24 05:13:47 [INFO]: Epoch 005 - training loss (MAE): 0.4872, validation MSE: 0.7217
2025-08-24 05:13:50 [INFO]: Epoch 006 - training loss (MAE): 0.4870, validation MSE: 0.7067
2025-08-24 05:13:52 [INFO]: Epoch 007 - training loss (MAE): 0.4833, validation MSE: 0.6819
2025-08-24 05:13:54 [INFO]: Epoch 008 - training loss (MAE): 0.4726, validation MSE: 0.6551
2025-08-24 05:13:57 [INFO]: Epoch 009 - training loss (MAE): 0.4648, validation MSE: 0.6346
2025-08-24 05:14:00 [INFO]: Epoch 010 - training loss (MAE): 0.4595, validation MSE: 0.6216
2025-08-24 05:14:05 [INFO]: Epoch 011 - training loss (MAE): 0.4556, validation MSE: 0.6143
2025-08-24 05:14:12 [INFO]: Epoch 012 - training loss (MAE): 0.4482, validation MSE: 0.6085
2025-08-24 05:14:19 [INFO]: Epoch 013 - training loss (MAE): 0.4456, validation MSE: 0.6008
2025-08-24 05:14:23 [INFO]: Epoch 014 - training loss (MAE): 0.4399, validation MSE: 0.5893
2025-08-24 05:14:25 [INFO]: Epoch 015 - training loss (MAE): 0.4368, validation MSE: 0.5737
2025-08-24 05:14:26 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:14:27 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4400| MSE: 0.7298| RMSE: 0.8543| MRE: 0.5461| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
2025-08-24 05:14:52 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:14:52 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:14:52 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:14:52 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:14:52 [INFO]: Loaded successfully!
2025-08-24 05:14:52 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:14:52 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:14:52 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:14:53 [INFO]: Total sample number: 182
2025-08-24 05:14:53 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:14:53 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:14:53 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:14:53 [INFO]: Number of steps: 96
2025-08-24 05:14:53 [INFO]: Number of features: 862
2025-08-24 05:14:53 [INFO]: Train set missing rate: 40.01%
2025-08-24 05:14:53 [INFO]: Validating set missing rate: 39.94%
2025-08-24 05:14:53 [INFO]: Test set missing rate: 40.04%
âœ… Dataset 'pems_traffic' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:14:53 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:14:53 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:14:54 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:14:54 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:14:54 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:14:54 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:15:00 [INFO]: Model placed on cuda:0
2025-08-24 05:15:00 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:15:02 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:15:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:15:35 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:15:35 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:15:35 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:15:35 [INFO]: Loaded successfully!
2025-08-24 05:15:35 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:15:35 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:15:35 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:15:36 [INFO]: Total sample number: 182
2025-08-24 05:15:36 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:15:36 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:15:36 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:15:36 [INFO]: Number of steps: 96
2025-08-24 05:15:36 [INFO]: Number of features: 862
2025-08-24 05:15:36 [INFO]: Train set missing rate: 40.01%
2025-08-24 05:15:36 [INFO]: Validating set missing rate: 39.94%
2025-08-24 05:15:36 [INFO]: Test set missing rate: 40.04%
âœ… Dataset 'pems_traffic' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:15:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:15:36 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:15:36 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:15:36 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:15:36 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:15:36 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:15:43 [INFO]: Model placed on cuda:0
2025-08-24 05:15:43 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:15:47 [INFO]: Epoch 001 - training loss (MAE): 0.7100, validation MSE: 1.0190
2025-08-24 05:15:55 [INFO]: Epoch 002 - training loss (MAE): 0.6130, validation MSE: 0.8540
2025-08-24 05:16:01 [INFO]: Epoch 003 - training loss (MAE): 0.5444, validation MSE: 0.7638
2025-08-24 05:16:08 [INFO]: Epoch 004 - training loss (MAE): 0.5007, validation MSE: 0.7308
2025-08-24 05:16:15 [INFO]: Epoch 005 - training loss (MAE): 0.4872, validation MSE: 0.7217
2025-08-24 05:16:18 [INFO]: Epoch 006 - training loss (MAE): 0.4870, validation MSE: 0.7067
2025-08-24 05:16:24 [INFO]: Epoch 007 - training loss (MAE): 0.4833, validation MSE: 0.6819
2025-08-24 05:16:29 [INFO]: Epoch 008 - training loss (MAE): 0.4726, validation MSE: 0.6551
2025-08-24 05:16:36 [INFO]: Epoch 009 - training loss (MAE): 0.4648, validation MSE: 0.6346
2025-08-24 05:16:41 [INFO]: Epoch 010 - training loss (MAE): 0.4595, validation MSE: 0.6216
2025-08-24 05:16:48 [INFO]: Epoch 011 - training loss (MAE): 0.4556, validation MSE: 0.6143
2025-08-24 05:16:54 [INFO]: Epoch 012 - training loss (MAE): 0.4482, validation MSE: 0.6085
2025-08-24 05:17:00 [INFO]: Epoch 013 - training loss (MAE): 0.4456, validation MSE: 0.6008
2025-08-24 05:17:05 [INFO]: Epoch 014 - training loss (MAE): 0.4399, validation MSE: 0.5893
2025-08-24 05:17:08 [INFO]: Epoch 015 - training loss (MAE): 0.4368, validation MSE: 0.5737
2025-08-24 05:17:08 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:17:10 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4400| MSE: 0.7298| RMSE: 0.8543| MRE: 0.5461| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:17:38 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:17:38 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:17:38 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:17:38 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:17:38 [INFO]: Loaded successfully!
2025-08-24 05:17:38 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:17:38 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:17:38 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:17:39 [INFO]: Total sample number: 182
2025-08-24 05:17:39 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:17:39 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:17:39 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:17:39 [INFO]: Number of steps: 96
2025-08-24 05:17:39 [INFO]: Number of features: 862
2025-08-24 05:17:39 [INFO]: Train set missing rate: 50.01%
2025-08-24 05:17:39 [INFO]: Validating set missing rate: 49.93%
2025-08-24 05:17:39 [INFO]: Test set missing rate: 50.04%
âœ… Dataset 'pems_traffic' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:17:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:17:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:17:39 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:17:39 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:17:39 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:17:39 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:17:46 [INFO]: Model placed on cuda:0
2025-08-24 05:17:46 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:17:48 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 15.02 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.24 GiB memory in use. Of the allocated memory 786.27 MiB is allocated by PyTorch, and 177.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:18:11 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:18:11 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:18:11 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:18:11 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:18:11 [INFO]: Loaded successfully!
2025-08-24 05:18:11 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:18:11 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:18:11 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:18:11 [INFO]: Total sample number: 182
2025-08-24 05:18:11 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:18:11 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:18:11 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:18:11 [INFO]: Number of steps: 96
2025-08-24 05:18:11 [INFO]: Number of features: 862
2025-08-24 05:18:11 [INFO]: Train set missing rate: 50.01%
2025-08-24 05:18:11 [INFO]: Validating set missing rate: 49.93%
2025-08-24 05:18:11 [INFO]: Test set missing rate: 50.04%
âœ… Dataset 'pems_traffic' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:18:11 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:18:11 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:18:12 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:18:12 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:18:12 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:18:12 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:18:19 [INFO]: Model placed on cuda:0
2025-08-24 05:18:19 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:18:22 [INFO]: Epoch 001 - training loss (MAE): 0.7121, validation MSE: 1.0378
2025-08-24 05:18:24 [INFO]: Epoch 002 - training loss (MAE): 0.6237, validation MSE: 0.8837
2025-08-24 05:18:27 [INFO]: Epoch 003 - training loss (MAE): 0.5586, validation MSE: 0.7923
2025-08-24 05:18:31 [INFO]: Epoch 004 - training loss (MAE): 0.5131, validation MSE: 0.7518
2025-08-24 05:18:34 [INFO]: Epoch 005 - training loss (MAE): 0.4924, validation MSE: 0.7402
2025-08-24 05:18:35 [INFO]: Epoch 006 - training loss (MAE): 0.4901, validation MSE: 0.7311
2025-08-24 05:18:38 [INFO]: Epoch 007 - training loss (MAE): 0.4887, validation MSE: 0.7127
2025-08-24 05:18:41 [INFO]: Epoch 008 - training loss (MAE): 0.4806, validation MSE: 0.6881
2025-08-24 05:18:43 [INFO]: Epoch 009 - training loss (MAE): 0.4741, validation MSE: 0.6666
2025-08-24 05:18:47 [INFO]: Epoch 010 - training loss (MAE): 0.4688, validation MSE: 0.6509
2025-08-24 05:18:49 [INFO]: Epoch 011 - training loss (MAE): 0.4657, validation MSE: 0.6413
2025-08-24 05:18:51 [INFO]: Epoch 012 - training loss (MAE): 0.4584, validation MSE: 0.6355
2025-08-24 05:18:53 [INFO]: Epoch 013 - training loss (MAE): 0.4562, validation MSE: 0.6312
2025-08-24 05:18:54 [INFO]: Epoch 014 - training loss (MAE): 0.4512, validation MSE: 0.6249
2025-08-24 05:18:57 [INFO]: Epoch 015 - training loss (MAE): 0.4483, validation MSE: 0.6137
2025-08-24 05:18:57 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:18:58 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4602| MSE: 0.7834| RMSE: 0.8851| MRE: 0.5716| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:19:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:19:24 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:19:24 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:19:24 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:19:24 [INFO]: Loaded successfully!
2025-08-24 05:19:24 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:19:24 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:19:24 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:19:24 [INFO]: Total sample number: 182
2025-08-24 05:19:24 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:19:24 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:19:24 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:19:24 [INFO]: Number of steps: 96
2025-08-24 05:19:24 [INFO]: Number of features: 862
2025-08-24 05:19:24 [INFO]: Train set missing rate: 50.01%
2025-08-24 05:19:24 [INFO]: Validating set missing rate: 49.93%
2025-08-24 05:19:24 [INFO]: Test set missing rate: 50.04%
âœ… Dataset 'pems_traffic' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:19:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:19:24 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:19:25 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:19:25 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:19:25 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:19:25 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:19:34 [INFO]: Model placed on cuda:0
2025-08-24 05:19:34 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:19:35 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 259, in _forward_internal
    kv = self.prompt_to_llm(llm_embeds.flatten(0, 1))  # [(B*D), Lp, d_llm]
                            ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 39.46 GiB. GPU 0 has a total capacity of 23.58 GiB of which 14.97 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 1.29 GiB memory in use. Of the allocated memory 833.15 MiB is allocated by PyTorch, and 178.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=228)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:20:00 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:20:00 [INFO]: You're using dataset pems_traffic, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/pems_traffic
2025-08-24 05:20:00 [INFO]: Dataset pems_traffic has already been downloaded. Processing directly...
2025-08-24 05:20:00 [INFO]: Dataset pems_traffic has already been cached. Loading from cache directly...
2025-08-24 05:20:00 [INFO]: Loaded successfully!
2025-08-24 05:20:00 [INFO]: months selected as train set are <PeriodArray>
['2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07',
 '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02',
 '2016-03']
Length: 15, dtype: period[M]
2025-08-24 05:20:00 [INFO]: months selected as val set are <PeriodArray>
['2016-04', '2016-05', '2016-06', '2016-07']
Length: 4, dtype: period[M]
2025-08-24 05:20:00 [INFO]: months selected as test set are <PeriodArray>
['2016-08', '2016-09', '2016-10', '2016-11', '2016-12']
Length: 5, dtype: period[M]
2025-08-24 05:20:00 [INFO]: Total sample number: 182
2025-08-24 05:20:00 [INFO]: Training set size: 114 (62.64%)
2025-08-24 05:20:00 [INFO]: Validation set size: 30 (16.48%)
2025-08-24 05:20:00 [INFO]: Test set size: 38 (20.88%)
2025-08-24 05:20:00 [INFO]: Number of steps: 96
2025-08-24 05:20:00 [INFO]: Number of features: 862
2025-08-24 05:20:00 [INFO]: Train set missing rate: 50.01%
2025-08-24 05:20:00 [INFO]: Validating set missing rate: 49.93%
2025-08-24 05:20:00 [INFO]: Test set missing rate: 50.04%
âœ… Dataset 'pems_traffic' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:20:00 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:20:00 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:20:00 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:20:00 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:20:00 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:20:00 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:20:08 [INFO]: Model placed on cuda:0
2025-08-24 05:20:08 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:20:10 [INFO]: Epoch 001 - training loss (MAE): 0.7121, validation MSE: 1.0378
2025-08-24 05:20:12 [INFO]: Epoch 002 - training loss (MAE): 0.6237, validation MSE: 0.8837
2025-08-24 05:20:15 [INFO]: Epoch 003 - training loss (MAE): 0.5586, validation MSE: 0.7923
2025-08-24 05:20:19 [INFO]: Epoch 004 - training loss (MAE): 0.5131, validation MSE: 0.7518
2025-08-24 05:20:21 [INFO]: Epoch 005 - training loss (MAE): 0.4924, validation MSE: 0.7402
2025-08-24 05:20:23 [INFO]: Epoch 006 - training loss (MAE): 0.4901, validation MSE: 0.7311
2025-08-24 05:20:25 [INFO]: Epoch 007 - training loss (MAE): 0.4887, validation MSE: 0.7127
2025-08-24 05:20:27 [INFO]: Epoch 008 - training loss (MAE): 0.4806, validation MSE: 0.6881
2025-08-24 05:20:29 [INFO]: Epoch 009 - training loss (MAE): 0.4741, validation MSE: 0.6666
2025-08-24 05:20:32 [INFO]: Epoch 010 - training loss (MAE): 0.4688, validation MSE: 0.6509
2025-08-24 05:20:35 [INFO]: Epoch 011 - training loss (MAE): 0.4657, validation MSE: 0.6413
2025-08-24 05:20:39 [INFO]: Epoch 012 - training loss (MAE): 0.4584, validation MSE: 0.6355
2025-08-24 05:20:42 [INFO]: Epoch 013 - training loss (MAE): 0.4562, validation MSE: 0.6312
2025-08-24 05:20:43 [INFO]: Epoch 014 - training loss (MAE): 0.4512, validation MSE: 0.6249
2025-08-24 05:20:46 [INFO]: Epoch 015 - training loss (MAE): 0.4483, validation MSE: 0.6137
2025-08-24 05:20:46 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:20:47 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.4602| MSE: 0.7834| RMSE: 0.8851| MRE: 0.5716| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/pems_traffic/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: pems_traffic | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:21:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:21:12 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:21:12 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:21:12 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:21:12 [INFO]: Loaded successfully!
2025-08-24 05:21:12 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:21:12 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:21:12 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:21:12 [INFO]: Total sample number: 547
2025-08-24 05:21:12 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:21:12 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:21:12 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:21:12 [INFO]: Number of steps: 96
2025-08-24 05:21:12 [INFO]: Number of features: 137
2025-08-24 05:21:12 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:21:12 [INFO]: Validating set missing rate: 9.94%
2025-08-24 05:21:12 [INFO]: Test set missing rate: 9.99%
âœ… Dataset 'solar_alabama' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:21:12 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:21:12 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:21:13 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:21:13 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:21:13 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:21:13 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:21:19 [INFO]: Model placed on cuda:0
2025-08-24 05:21:19 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:21:20 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:21:47 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:21:47 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:21:47 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:21:47 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:21:47 [INFO]: Loaded successfully!
2025-08-24 05:21:47 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:21:47 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:21:47 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:21:47 [INFO]: Total sample number: 547
2025-08-24 05:21:47 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:21:47 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:21:47 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:21:47 [INFO]: Number of steps: 96
2025-08-24 05:21:47 [INFO]: Number of features: 137
2025-08-24 05:21:47 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:21:47 [INFO]: Validating set missing rate: 9.94%
2025-08-24 05:21:47 [INFO]: Test set missing rate: 9.99%
âœ… Dataset 'solar_alabama' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:21:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:21:47 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:21:47 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:21:47 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:21:47 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:21:47 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:21:54 [INFO]: Model placed on cuda:0
2025-08-24 05:21:54 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:21:54 [INFO]: Epoch 001 - training loss (MAE): 0.6639, validation MSE: 0.5135
2025-08-24 05:21:55 [INFO]: Epoch 002 - training loss (MAE): 0.5216, validation MSE: 0.3196
2025-08-24 05:21:55 [INFO]: Epoch 003 - training loss (MAE): 0.4202, validation MSE: 0.1880
2025-08-24 05:21:55 [INFO]: Epoch 004 - training loss (MAE): 0.3605, validation MSE: 0.1314
2025-08-24 05:21:56 [INFO]: Epoch 005 - training loss (MAE): 0.3273, validation MSE: 0.1101
2025-08-24 05:21:56 [INFO]: Epoch 006 - training loss (MAE): 0.3130, validation MSE: 0.0995
2025-08-24 05:21:56 [INFO]: Epoch 007 - training loss (MAE): 0.3011, validation MSE: 0.0919
2025-08-24 05:21:56 [INFO]: Epoch 008 - training loss (MAE): 0.2914, validation MSE: 0.0891
2025-08-24 05:21:57 [INFO]: Epoch 009 - training loss (MAE): 0.2880, validation MSE: 0.0866
2025-08-24 05:21:57 [INFO]: Epoch 010 - training loss (MAE): 0.2823, validation MSE: 0.0835
2025-08-24 05:21:57 [INFO]: Epoch 011 - training loss (MAE): 0.2787, validation MSE: 0.0801
2025-08-24 05:21:58 [INFO]: Epoch 012 - training loss (MAE): 0.2751, validation MSE: 0.0787
2025-08-24 05:21:58 [INFO]: Epoch 013 - training loss (MAE): 0.2695, validation MSE: 0.0751
2025-08-24 05:21:58 [INFO]: Epoch 014 - training loss (MAE): 0.2674, validation MSE: 0.0740
2025-08-24 05:21:58 [INFO]: Epoch 015 - training loss (MAE): 0.2653, validation MSE: 0.0701
2025-08-24 05:21:58 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:22:00 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.1611| MSE: 0.0558| RMSE: 0.2362| MRE: 0.2079| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
ğŸš€ Running imputation pipeline for model: llm4imp
2025-08-24 05:22:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:22:23 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:22:23 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:22:23 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:22:23 [INFO]: Loaded successfully!
2025-08-24 05:22:23 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:22:23 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:22:23 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:22:23 [INFO]: Total sample number: 547
2025-08-24 05:22:23 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:22:23 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:22:23 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:22:23 [INFO]: Number of steps: 96
2025-08-24 05:22:23 [INFO]: Number of features: 137
2025-08-24 05:22:23 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:22:23 [INFO]: Validating set missing rate: 9.94%
2025-08-24 05:22:23 [INFO]: Test set missing rate: 9.99%
âœ… Dataset 'solar_alabama' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:22:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:22:23 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:22:24 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:22:24 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:22:24 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:22:24 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:22:30 [INFO]: Model placed on cuda:0
2025-08-24 05:22:30 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:22:31 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:22:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:22:56 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:22:56 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:22:56 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:22:56 [INFO]: Loaded successfully!
2025-08-24 05:22:56 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:22:56 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:22:56 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:22:56 [INFO]: Total sample number: 547
2025-08-24 05:22:56 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:22:56 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:22:56 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:22:56 [INFO]: Number of steps: 96
2025-08-24 05:22:56 [INFO]: Number of features: 137
2025-08-24 05:22:56 [INFO]: Train set missing rate: 9.98%
2025-08-24 05:22:56 [INFO]: Validating set missing rate: 9.94%
2025-08-24 05:22:56 [INFO]: Test set missing rate: 9.99%
âœ… Dataset 'solar_alabama' with missing rate 0.1 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.1
2025-08-24 05:22:56 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:22:56 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:22:56 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:22:56 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:22:56 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:22:56 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:23:03 [INFO]: Model placed on cuda:0
2025-08-24 05:23:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:23:03 [INFO]: Epoch 001 - training loss (MAE): 0.6639, validation MSE: 0.5135
2025-08-24 05:23:04 [INFO]: Epoch 002 - training loss (MAE): 0.5216, validation MSE: 0.3196
2025-08-24 05:23:04 [INFO]: Epoch 003 - training loss (MAE): 0.4202, validation MSE: 0.1880
2025-08-24 05:23:04 [INFO]: Epoch 004 - training loss (MAE): 0.3605, validation MSE: 0.1314
2025-08-24 05:23:04 [INFO]: Epoch 005 - training loss (MAE): 0.3273, validation MSE: 0.1101
2025-08-24 05:23:05 [INFO]: Epoch 006 - training loss (MAE): 0.3130, validation MSE: 0.0995
2025-08-24 05:23:05 [INFO]: Epoch 007 - training loss (MAE): 0.3011, validation MSE: 0.0919
2025-08-24 05:23:05 [INFO]: Epoch 008 - training loss (MAE): 0.2914, validation MSE: 0.0891
2025-08-24 05:23:06 [INFO]: Epoch 009 - training loss (MAE): 0.2880, validation MSE: 0.0866
2025-08-24 05:23:06 [INFO]: Epoch 010 - training loss (MAE): 0.2823, validation MSE: 0.0835
2025-08-24 05:23:06 [INFO]: Epoch 011 - training loss (MAE): 0.2787, validation MSE: 0.0801
2025-08-24 05:23:06 [INFO]: Epoch 012 - training loss (MAE): 0.2751, validation MSE: 0.0787
2025-08-24 05:23:07 [INFO]: Epoch 013 - training loss (MAE): 0.2695, validation MSE: 0.0751
2025-08-24 05:23:07 [INFO]: Epoch 014 - training loss (MAE): 0.2674, validation MSE: 0.0740
2025-08-24 05:23:07 [INFO]: Epoch 015 - training loss (MAE): 0.2653, validation MSE: 0.0701
2025-08-24 05:23:07 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:23:09 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.1611| MSE: 0.0558| RMSE: 0.2362| MRE: 0.2079| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.1_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:23:31 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:23:31 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:23:31 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:23:31 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:23:32 [INFO]: Loaded successfully!
2025-08-24 05:23:32 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:23:32 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:23:32 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:23:32 [INFO]: Total sample number: 547
2025-08-24 05:23:32 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:23:32 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:23:32 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:23:32 [INFO]: Number of steps: 96
2025-08-24 05:23:32 [INFO]: Number of features: 137
2025-08-24 05:23:32 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:23:32 [INFO]: Validating set missing rate: 19.95%
2025-08-24 05:23:32 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'solar_alabama' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:23:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:23:32 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:23:32 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:23:32 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:23:32 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:23:32 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:23:42 [INFO]: Model placed on cuda:0
2025-08-24 05:23:42 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:23:42 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:24:04 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:24:04 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:24:04 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:24:04 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:24:04 [INFO]: Loaded successfully!
2025-08-24 05:24:04 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:24:04 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:24:04 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:24:04 [INFO]: Total sample number: 547
2025-08-24 05:24:04 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:24:04 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:24:04 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:24:04 [INFO]: Number of steps: 96
2025-08-24 05:24:04 [INFO]: Number of features: 137
2025-08-24 05:24:04 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:24:04 [INFO]: Validating set missing rate: 19.95%
2025-08-24 05:24:04 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'solar_alabama' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:24:04 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:24:04 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:24:05 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:24:05 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:24:05 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:24:05 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:24:12 [INFO]: Model placed on cuda:0
2025-08-24 05:24:12 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:24:12 [INFO]: Epoch 001 - training loss (MAE): 0.6813, validation MSE: 0.5519
2025-08-24 05:24:12 [INFO]: Epoch 002 - training loss (MAE): 0.5507, validation MSE: 0.3772
2025-08-24 05:24:13 [INFO]: Epoch 003 - training loss (MAE): 0.4560, validation MSE: 0.2295
2025-08-24 05:24:13 [INFO]: Epoch 004 - training loss (MAE): 0.3934, validation MSE: 0.1512
2025-08-24 05:24:13 [INFO]: Epoch 005 - training loss (MAE): 0.3557, validation MSE: 0.1211
2025-08-24 05:24:14 [INFO]: Epoch 006 - training loss (MAE): 0.3373, validation MSE: 0.1092
2025-08-24 05:24:14 [INFO]: Epoch 007 - training loss (MAE): 0.3260, validation MSE: 0.0995
2025-08-24 05:24:14 [INFO]: Epoch 008 - training loss (MAE): 0.3151, validation MSE: 0.0947
2025-08-24 05:24:14 [INFO]: Epoch 009 - training loss (MAE): 0.3112, validation MSE: 0.0924
2025-08-24 05:24:15 [INFO]: Epoch 010 - training loss (MAE): 0.3046, validation MSE: 0.0893
2025-08-24 05:24:15 [INFO]: Epoch 011 - training loss (MAE): 0.3006, validation MSE: 0.0861
2025-08-24 05:24:15 [INFO]: Epoch 012 - training loss (MAE): 0.2969, validation MSE: 0.0849
2025-08-24 05:24:15 [INFO]: Epoch 013 - training loss (MAE): 0.2908, validation MSE: 0.0814
2025-08-24 05:24:16 [INFO]: Epoch 014 - training loss (MAE): 0.2888, validation MSE: 0.0802
2025-08-24 05:24:16 [INFO]: Epoch 015 - training loss (MAE): 0.2868, validation MSE: 0.0767
2025-08-24 05:24:16 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:24:17 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.1819| MSE: 0.0654| RMSE: 0.2558| MRE: 0.2349| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:24:45 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:24:45 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:24:45 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:24:45 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:24:45 [INFO]: Loaded successfully!
2025-08-24 05:24:45 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:24:45 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:24:45 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:24:45 [INFO]: Total sample number: 547
2025-08-24 05:24:45 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:24:45 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:24:45 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:24:45 [INFO]: Number of steps: 96
2025-08-24 05:24:45 [INFO]: Number of features: 137
2025-08-24 05:24:45 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:24:45 [INFO]: Validating set missing rate: 19.95%
2025-08-24 05:24:45 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'solar_alabama' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:24:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:24:45 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:24:46 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:24:46 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:24:46 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:24:46 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:24:52 [INFO]: Model placed on cuda:0
2025-08-24 05:24:52 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:24:53 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:25:15 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:25:15 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:25:15 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:25:15 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:25:15 [INFO]: Loaded successfully!
2025-08-24 05:25:15 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:25:15 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:25:15 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:25:15 [INFO]: Total sample number: 547
2025-08-24 05:25:15 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:25:15 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:25:15 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:25:15 [INFO]: Number of steps: 96
2025-08-24 05:25:15 [INFO]: Number of features: 137
2025-08-24 05:25:15 [INFO]: Train set missing rate: 20.00%
2025-08-24 05:25:15 [INFO]: Validating set missing rate: 19.95%
2025-08-24 05:25:15 [INFO]: Test set missing rate: 20.01%
âœ… Dataset 'solar_alabama' with missing rate 0.2 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.2
2025-08-24 05:25:15 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:25:15 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:25:16 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:25:16 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:25:16 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:25:16 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:25:23 [INFO]: Model placed on cuda:0
2025-08-24 05:25:23 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:25:23 [INFO]: Epoch 001 - training loss (MAE): 0.6813, validation MSE: 0.5519
2025-08-24 05:25:24 [INFO]: Epoch 002 - training loss (MAE): 0.5507, validation MSE: 0.3772
2025-08-24 05:25:24 [INFO]: Epoch 003 - training loss (MAE): 0.4560, validation MSE: 0.2295
2025-08-24 05:25:24 [INFO]: Epoch 004 - training loss (MAE): 0.3934, validation MSE: 0.1512
2025-08-24 05:25:24 [INFO]: Epoch 005 - training loss (MAE): 0.3557, validation MSE: 0.1211
2025-08-24 05:25:25 [INFO]: Epoch 006 - training loss (MAE): 0.3373, validation MSE: 0.1092
2025-08-24 05:25:25 [INFO]: Epoch 007 - training loss (MAE): 0.3260, validation MSE: 0.0995
2025-08-24 05:25:25 [INFO]: Epoch 008 - training loss (MAE): 0.3151, validation MSE: 0.0947
2025-08-24 05:25:26 [INFO]: Epoch 009 - training loss (MAE): 0.3112, validation MSE: 0.0924
2025-08-24 05:25:26 [INFO]: Epoch 010 - training loss (MAE): 0.3046, validation MSE: 0.0893
2025-08-24 05:25:26 [INFO]: Epoch 011 - training loss (MAE): 0.3006, validation MSE: 0.0861
2025-08-24 05:25:26 [INFO]: Epoch 012 - training loss (MAE): 0.2969, validation MSE: 0.0849
2025-08-24 05:25:27 [INFO]: Epoch 013 - training loss (MAE): 0.2908, validation MSE: 0.0814
2025-08-24 05:25:27 [INFO]: Epoch 014 - training loss (MAE): 0.2888, validation MSE: 0.0802
2025-08-24 05:25:27 [INFO]: Epoch 015 - training loss (MAE): 0.2868, validation MSE: 0.0767
2025-08-24 05:25:27 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:25:29 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.1819| MSE: 0.0654| RMSE: 0.2558| MRE: 0.2349| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.2_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:25:55 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:25:55 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:25:55 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:25:55 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:25:55 [INFO]: Loaded successfully!
2025-08-24 05:25:55 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:25:55 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:25:55 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:25:55 [INFO]: Total sample number: 547
2025-08-24 05:25:55 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:25:55 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:25:55 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:25:55 [INFO]: Number of steps: 96
2025-08-24 05:25:55 [INFO]: Number of features: 137
2025-08-24 05:25:55 [INFO]: Train set missing rate: 30.02%
2025-08-24 05:25:55 [INFO]: Validating set missing rate: 29.94%
2025-08-24 05:25:55 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'solar_alabama' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:25:55 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:25:55 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:25:56 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:25:56 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:25:56 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:25:56 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:26:03 [INFO]: Model placed on cuda:0
2025-08-24 05:26:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:26:03 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:26:27 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:26:27 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:26:27 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:26:27 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:26:27 [INFO]: Loaded successfully!
2025-08-24 05:26:27 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:26:27 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:26:27 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:26:27 [INFO]: Total sample number: 547
2025-08-24 05:26:27 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:26:27 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:26:27 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:26:27 [INFO]: Number of steps: 96
2025-08-24 05:26:27 [INFO]: Number of features: 137
2025-08-24 05:26:27 [INFO]: Train set missing rate: 30.02%
2025-08-24 05:26:27 [INFO]: Validating set missing rate: 29.94%
2025-08-24 05:26:27 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'solar_alabama' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:26:27 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:26:27 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:26:28 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:26:28 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:26:28 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:26:28 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:26:35 [INFO]: Model placed on cuda:0
2025-08-24 05:26:35 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:26:36 [INFO]: Epoch 001 - training loss (MAE): 0.6992, validation MSE: 0.5949
2025-08-24 05:26:36 [INFO]: Epoch 002 - training loss (MAE): 0.5811, validation MSE: 0.4453
2025-08-24 05:26:36 [INFO]: Epoch 003 - training loss (MAE): 0.4939, validation MSE: 0.2907
2025-08-24 05:26:36 [INFO]: Epoch 004 - training loss (MAE): 0.4312, validation MSE: 0.1852
2025-08-24 05:26:37 [INFO]: Epoch 005 - training loss (MAE): 0.3892, validation MSE: 0.1405
2025-08-24 05:26:37 [INFO]: Epoch 006 - training loss (MAE): 0.3660, validation MSE: 0.1231
2025-08-24 05:26:37 [INFO]: Epoch 007 - training loss (MAE): 0.3536, validation MSE: 0.1123
2025-08-24 05:26:38 [INFO]: Epoch 008 - training loss (MAE): 0.3425, validation MSE: 0.1050
2025-08-24 05:26:38 [INFO]: Epoch 009 - training loss (MAE): 0.3379, validation MSE: 0.1016
2025-08-24 05:26:38 [INFO]: Epoch 010 - training loss (MAE): 0.3297, validation MSE: 0.0979
2025-08-24 05:26:38 [INFO]: Epoch 011 - training loss (MAE): 0.3257, validation MSE: 0.0947
2025-08-24 05:26:39 [INFO]: Epoch 012 - training loss (MAE): 0.3215, validation MSE: 0.0933
2025-08-24 05:26:39 [INFO]: Epoch 013 - training loss (MAE): 0.3149, validation MSE: 0.0904
2025-08-24 05:26:39 [INFO]: Epoch 014 - training loss (MAE): 0.3130, validation MSE: 0.0888
2025-08-24 05:26:39 [INFO]: Epoch 015 - training loss (MAE): 0.3107, validation MSE: 0.0858
2025-08-24 05:26:39 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:26:41 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2100| MSE: 0.0797| RMSE: 0.2822| MRE: 0.2711| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:27:07 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:27:07 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:27:07 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:27:07 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:27:07 [INFO]: Loaded successfully!
2025-08-24 05:27:07 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:27:07 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:27:07 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:27:07 [INFO]: Total sample number: 547
2025-08-24 05:27:07 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:27:07 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:27:07 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:27:07 [INFO]: Number of steps: 96
2025-08-24 05:27:07 [INFO]: Number of features: 137
2025-08-24 05:27:07 [INFO]: Train set missing rate: 30.02%
2025-08-24 05:27:07 [INFO]: Validating set missing rate: 29.94%
2025-08-24 05:27:07 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'solar_alabama' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:27:07 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:27:07 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:27:08 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:27:08 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:27:08 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:27:08 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:27:15 [INFO]: Model placed on cuda:0
2025-08-24 05:27:15 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:27:15 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:27:38 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:27:38 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:27:38 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:27:38 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:27:38 [INFO]: Loaded successfully!
2025-08-24 05:27:38 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:27:38 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:27:38 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:27:38 [INFO]: Total sample number: 547
2025-08-24 05:27:38 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:27:38 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:27:38 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:27:38 [INFO]: Number of steps: 96
2025-08-24 05:27:38 [INFO]: Number of features: 137
2025-08-24 05:27:38 [INFO]: Train set missing rate: 30.02%
2025-08-24 05:27:38 [INFO]: Validating set missing rate: 29.94%
2025-08-24 05:27:38 [INFO]: Test set missing rate: 30.02%
âœ… Dataset 'solar_alabama' with missing rate 0.3 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.3
2025-08-24 05:27:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:27:39 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:27:39 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:27:39 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:27:39 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:27:39 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:27:48 [INFO]: Model placed on cuda:0
2025-08-24 05:27:48 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:27:49 [INFO]: Epoch 001 - training loss (MAE): 0.6992, validation MSE: 0.5949
2025-08-24 05:27:49 [INFO]: Epoch 002 - training loss (MAE): 0.5811, validation MSE: 0.4453
2025-08-24 05:27:49 [INFO]: Epoch 003 - training loss (MAE): 0.4939, validation MSE: 0.2907
2025-08-24 05:27:49 [INFO]: Epoch 004 - training loss (MAE): 0.4312, validation MSE: 0.1852
2025-08-24 05:27:50 [INFO]: Epoch 005 - training loss (MAE): 0.3892, validation MSE: 0.1405
2025-08-24 05:27:50 [INFO]: Epoch 006 - training loss (MAE): 0.3660, validation MSE: 0.1231
2025-08-24 05:27:50 [INFO]: Epoch 007 - training loss (MAE): 0.3536, validation MSE: 0.1123
2025-08-24 05:27:50 [INFO]: Epoch 008 - training loss (MAE): 0.3425, validation MSE: 0.1050
2025-08-24 05:27:51 [INFO]: Epoch 009 - training loss (MAE): 0.3379, validation MSE: 0.1016
2025-08-24 05:27:51 [INFO]: Epoch 010 - training loss (MAE): 0.3297, validation MSE: 0.0979
2025-08-24 05:27:51 [INFO]: Epoch 011 - training loss (MAE): 0.3257, validation MSE: 0.0947
2025-08-24 05:27:51 [INFO]: Epoch 012 - training loss (MAE): 0.3215, validation MSE: 0.0933
2025-08-24 05:27:52 [INFO]: Epoch 013 - training loss (MAE): 0.3149, validation MSE: 0.0904
2025-08-24 05:27:52 [INFO]: Epoch 014 - training loss (MAE): 0.3130, validation MSE: 0.0888
2025-08-24 05:27:52 [INFO]: Epoch 015 - training loss (MAE): 0.3107, validation MSE: 0.0858
2025-08-24 05:27:52 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:27:54 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2100| MSE: 0.0797| RMSE: 0.2822| MRE: 0.2711| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.3_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:28:17 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:28:17 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:28:17 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:28:17 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:28:17 [INFO]: Loaded successfully!
2025-08-24 05:28:17 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:28:17 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:28:17 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:28:18 [INFO]: Total sample number: 547
2025-08-24 05:28:18 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:28:18 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:28:18 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:28:18 [INFO]: Number of steps: 96
2025-08-24 05:28:18 [INFO]: Number of features: 137
2025-08-24 05:28:18 [INFO]: Train set missing rate: 40.00%
2025-08-24 05:28:18 [INFO]: Validating set missing rate: 39.97%
2025-08-24 05:28:18 [INFO]: Test set missing rate: 40.00%
âœ… Dataset 'solar_alabama' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:28:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:28:18 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:28:18 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:28:18 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:28:18 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:28:18 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:28:25 [INFO]: Model placed on cuda:0
2025-08-24 05:28:25 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:28:25 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:28:52 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:28:52 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:28:52 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:28:52 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:28:52 [INFO]: Loaded successfully!
2025-08-24 05:28:52 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:28:52 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:28:52 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:28:52 [INFO]: Total sample number: 547
2025-08-24 05:28:52 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:28:52 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:28:52 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:28:52 [INFO]: Number of steps: 96
2025-08-24 05:28:52 [INFO]: Number of features: 137
2025-08-24 05:28:52 [INFO]: Train set missing rate: 40.00%
2025-08-24 05:28:52 [INFO]: Validating set missing rate: 39.97%
2025-08-24 05:28:52 [INFO]: Test set missing rate: 40.00%
âœ… Dataset 'solar_alabama' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:28:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:28:52 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:28:52 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:28:52 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:28:52 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:28:52 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:28:59 [INFO]: Model placed on cuda:0
2025-08-24 05:28:59 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:29:00 [INFO]: Epoch 001 - training loss (MAE): 0.7178, validation MSE: 0.6407
2025-08-24 05:29:00 [INFO]: Epoch 002 - training loss (MAE): 0.6132, validation MSE: 0.5194
2025-08-24 05:29:00 [INFO]: Epoch 003 - training loss (MAE): 0.5331, validation MSE: 0.3690
2025-08-24 05:29:01 [INFO]: Epoch 004 - training loss (MAE): 0.4739, validation MSE: 0.2378
2025-08-24 05:29:01 [INFO]: Epoch 005 - training loss (MAE): 0.4279, validation MSE: 0.1716
2025-08-24 05:29:01 [INFO]: Epoch 006 - training loss (MAE): 0.3998, validation MSE: 0.1442
2025-08-24 05:29:01 [INFO]: Epoch 007 - training loss (MAE): 0.3843, validation MSE: 0.1303
2025-08-24 05:29:02 [INFO]: Epoch 008 - training loss (MAE): 0.3723, validation MSE: 0.1214
2025-08-24 05:29:02 [INFO]: Epoch 009 - training loss (MAE): 0.3682, validation MSE: 0.1157
2025-08-24 05:29:02 [INFO]: Epoch 010 - training loss (MAE): 0.3583, validation MSE: 0.1109
2025-08-24 05:29:03 [INFO]: Epoch 011 - training loss (MAE): 0.3544, validation MSE: 0.1076
2025-08-24 05:29:03 [INFO]: Epoch 012 - training loss (MAE): 0.3495, validation MSE: 0.1057
2025-08-24 05:29:03 [INFO]: Epoch 013 - training loss (MAE): 0.3428, validation MSE: 0.1027
2025-08-24 05:29:03 [INFO]: Epoch 014 - training loss (MAE): 0.3404, validation MSE: 0.1009
2025-08-24 05:29:04 [INFO]: Epoch 015 - training loss (MAE): 0.3377, validation MSE: 0.0980
2025-08-24 05:29:04 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:29:06 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2425| MSE: 0.0980| RMSE: 0.3131| MRE: 0.3131| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:29:29 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:29:29 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:29:29 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:29:29 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:29:29 [INFO]: Loaded successfully!
2025-08-24 05:29:29 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:29:29 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:29:29 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:29:29 [INFO]: Total sample number: 547
2025-08-24 05:29:29 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:29:29 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:29:29 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:29:29 [INFO]: Number of steps: 96
2025-08-24 05:29:29 [INFO]: Number of features: 137
2025-08-24 05:29:29 [INFO]: Train set missing rate: 40.00%
2025-08-24 05:29:29 [INFO]: Validating set missing rate: 39.97%
2025-08-24 05:29:29 [INFO]: Test set missing rate: 40.00%
âœ… Dataset 'solar_alabama' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:29:29 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:29:29 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:29:30 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:29:30 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:29:30 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:29:30 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:29:36 [INFO]: Model placed on cuda:0
2025-08-24 05:29:36 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:29:37 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:30:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:30:03 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:30:03 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:30:03 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:30:03 [INFO]: Loaded successfully!
2025-08-24 05:30:03 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:30:03 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:30:03 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:30:03 [INFO]: Total sample number: 547
2025-08-24 05:30:03 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:30:03 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:30:03 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:30:03 [INFO]: Number of steps: 96
2025-08-24 05:30:03 [INFO]: Number of features: 137
2025-08-24 05:30:03 [INFO]: Train set missing rate: 40.00%
2025-08-24 05:30:03 [INFO]: Validating set missing rate: 39.97%
2025-08-24 05:30:03 [INFO]: Test set missing rate: 40.00%
âœ… Dataset 'solar_alabama' with missing rate 0.4 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.4
2025-08-24 05:30:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:30:03 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:30:03 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:30:03 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:30:03 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:30:03 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:30:10 [INFO]: Model placed on cuda:0
2025-08-24 05:30:10 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:30:11 [INFO]: Epoch 001 - training loss (MAE): 0.7178, validation MSE: 0.6407
2025-08-24 05:30:11 [INFO]: Epoch 002 - training loss (MAE): 0.6132, validation MSE: 0.5194
2025-08-24 05:30:11 [INFO]: Epoch 003 - training loss (MAE): 0.5331, validation MSE: 0.3690
2025-08-24 05:30:12 [INFO]: Epoch 004 - training loss (MAE): 0.4739, validation MSE: 0.2378
2025-08-24 05:30:12 [INFO]: Epoch 005 - training loss (MAE): 0.4279, validation MSE: 0.1716
2025-08-24 05:30:12 [INFO]: Epoch 006 - training loss (MAE): 0.3998, validation MSE: 0.1442
2025-08-24 05:30:12 [INFO]: Epoch 007 - training loss (MAE): 0.3843, validation MSE: 0.1303
2025-08-24 05:30:13 [INFO]: Epoch 008 - training loss (MAE): 0.3723, validation MSE: 0.1214
2025-08-24 05:30:13 [INFO]: Epoch 009 - training loss (MAE): 0.3682, validation MSE: 0.1157
2025-08-24 05:30:13 [INFO]: Epoch 010 - training loss (MAE): 0.3583, validation MSE: 0.1109
2025-08-24 05:30:13 [INFO]: Epoch 011 - training loss (MAE): 0.3544, validation MSE: 0.1076
2025-08-24 05:30:14 [INFO]: Epoch 012 - training loss (MAE): 0.3495, validation MSE: 0.1057
2025-08-24 05:30:14 [INFO]: Epoch 013 - training loss (MAE): 0.3428, validation MSE: 0.1027
2025-08-24 05:30:14 [INFO]: Epoch 014 - training loss (MAE): 0.3404, validation MSE: 0.1009
2025-08-24 05:30:15 [INFO]: Epoch 015 - training loss (MAE): 0.3377, validation MSE: 0.0980
2025-08-24 05:30:15 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:30:16 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2425| MSE: 0.0980| RMSE: 0.3131| MRE: 0.3131| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.4_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:30:41 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:30:41 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:30:41 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:30:41 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:30:42 [INFO]: Loaded successfully!
2025-08-24 05:30:42 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:30:42 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:30:42 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:30:42 [INFO]: Total sample number: 547
2025-08-24 05:30:42 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:30:42 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:30:42 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:30:42 [INFO]: Number of steps: 96
2025-08-24 05:30:42 [INFO]: Number of features: 137
2025-08-24 05:30:42 [INFO]: Train set missing rate: 49.99%
2025-08-24 05:30:42 [INFO]: Validating set missing rate: 50.03%
2025-08-24 05:30:42 [INFO]: Test set missing rate: 50.02%
âœ… Dataset 'solar_alabama' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:30:42 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:30:42 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:30:42 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue
2025-08-24 05:30:42 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogtrue/tensorboard
2025-08-24 05:30:42 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:30:42 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:30:53 [INFO]: Model placed on cuda:0
2025-08-24 05:30:53 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:30:53 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.68 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.57 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 31.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:31:16 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:31:16 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:31:16 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:31:16 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:31:16 [INFO]: Loaded successfully!
2025-08-24 05:31:16 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:31:16 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:31:16 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:31:16 [INFO]: Total sample number: 547
2025-08-24 05:31:16 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:31:16 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:31:16 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:31:16 [INFO]: Number of steps: 96
2025-08-24 05:31:16 [INFO]: Number of features: 137
2025-08-24 05:31:16 [INFO]: Train set missing rate: 49.99%
2025-08-24 05:31:16 [INFO]: Validating set missing rate: 50.03%
2025-08-24 05:31:16 [INFO]: Test set missing rate: 50.02%
âœ… Dataset 'solar_alabama' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:31:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:31:16 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:31:17 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse
2025-08-24 05:31:17 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/tensorboard
2025-08-24 05:31:17 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:31:17 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:31:24 [INFO]: Model placed on cuda:0
2025-08-24 05:31:24 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:31:25 [INFO]: Epoch 001 - training loss (MAE): 0.7373, validation MSE: 0.6889
2025-08-24 05:31:25 [INFO]: Epoch 002 - training loss (MAE): 0.6453, validation MSE: 0.5988
2025-08-24 05:31:25 [INFO]: Epoch 003 - training loss (MAE): 0.5742, validation MSE: 0.4667
2025-08-24 05:31:25 [INFO]: Epoch 004 - training loss (MAE): 0.5207, validation MSE: 0.3182
2025-08-24 05:31:26 [INFO]: Epoch 005 - training loss (MAE): 0.4720, validation MSE: 0.2230
2025-08-24 05:31:26 [INFO]: Epoch 006 - training loss (MAE): 0.4402, validation MSE: 0.1796
2025-08-24 05:31:26 [INFO]: Epoch 007 - training loss (MAE): 0.4204, validation MSE: 0.1576
2025-08-24 05:31:26 [INFO]: Epoch 008 - training loss (MAE): 0.4066, validation MSE: 0.1461
2025-08-24 05:31:27 [INFO]: Epoch 009 - training loss (MAE): 0.4019, validation MSE: 0.1389
2025-08-24 05:31:27 [INFO]: Epoch 010 - training loss (MAE): 0.3915, validation MSE: 0.1317
2025-08-24 05:31:27 [INFO]: Epoch 011 - training loss (MAE): 0.3877, validation MSE: 0.1269
2025-08-24 05:31:28 [INFO]: Epoch 012 - training loss (MAE): 0.3819, validation MSE: 0.1248
2025-08-24 05:31:28 [INFO]: Epoch 013 - training loss (MAE): 0.3746, validation MSE: 0.1211
2025-08-24 05:31:28 [INFO]: Epoch 014 - training loss (MAE): 0.3720, validation MSE: 0.1187
2025-08-24 05:31:28 [INFO]: Epoch 015 - training loss (MAE): 0.3692, validation MSE: 0.1166
2025-08-24 05:31:28 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:31:30 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2803| MSE: 0.1230| RMSE: 0.3507| MRE: 0.3618| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/prompttrue_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_prompttrue_reprogfalse
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:31:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:31:56 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:31:56 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:31:56 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:31:56 [INFO]: Loaded successfully!
2025-08-24 05:31:56 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:31:56 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:31:56 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:31:56 [INFO]: Total sample number: 547
2025-08-24 05:31:56 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:31:56 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:31:56 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:31:56 [INFO]: Number of steps: 96
2025-08-24 05:31:56 [INFO]: Number of features: 137
2025-08-24 05:31:56 [INFO]: Train set missing rate: 49.99%
2025-08-24 05:31:56 [INFO]: Validating set missing rate: 50.03%
2025-08-24 05:31:56 [INFO]: Test set missing rate: 50.02%
âœ… Dataset 'solar_alabama' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:31:56 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:31:56 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:31:56 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue
2025-08-24 05:31:56 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogtrue/tensorboard
2025-08-24 05:31:56 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:31:56 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:32:03 [INFO]: Model placed on cuda:0
2025-08-24 05:32:03 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:32:04 [ERROR]: âŒ Exception: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 748, in _train_model
    results = self.model(inputs, calc_criterion=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/core.py", line 81, in forward
    reconstruction = self.backbone(X, missing_mask)  # [B, T, D]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 234, in forward
    return self._forward_internal(X, missing_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/backbone.py", line 261, in _forward_internal
    reprogrammed = self.reprogramming(q, kv, kv)       # [(B*D), P, d_llm]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/nn/modules/llm4imp/layers.py", line 76, in forward
    source = self.key_projection(source_embedding).view(B_, S, H, -1)    # [B*, S, H, d_k]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/anaconda3/envs/pypots/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.27 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.63 GiB is free. Process 1428406 has 7.31 GiB memory in use. Including non-PyTorch memory, this process has 14.62 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 33.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 80, in <module>
    main(args)
  File "/home/23r8105_messou/lab/iPyPOTS/main.py", line 59, in main
    mae, mse, rmse, mre = MODEL_PIPELINES[model_name](dataset, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/23r8105_messou/lab/iPyPOTS/pipeline/imputations/llm4imp.py", line 70, in train_and_evaluate_llm4imp
    model.fit(
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/imputation/llm4imp/model.py", line 305, in fit
    self._train_model(train_dataloader, val_dataloader)
  File "/home/23r8105_messou/lab/iPyPOTS/pypots/base.py", line 825, in _train_model
    raise RuntimeError(
RuntimeError: Training got interrupted. Model was not trained. Please investigate the error printed above.
srun: error: ai-gpgpu14: task 0: Exited with exit code 1
âŒ FAIL(1): solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogtrue
ğŸ”¥ RUN: llm4imp | solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse (DEVICE=cuda:0, N_STEPS=96, N_FEATURES=137)
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 160: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 161: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
/var/spool/slurm-llnl/slurmd/job04740/slurm_script: line 162: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
tee: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/logs/run_mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse.log: No such file or directory
[34m
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•
ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai [0m

Use Train GPT: False
Use Lora: False
Use Profiling: False
2025-08-24 05:32:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
ğŸš€ Running imputation pipeline for model: llm4imp
â„¹ï¸ TSDB migration not needed, directory exists: /home/23r8105_messou/lab/iPyPOTS/datasets
2025-08-24 05:32:28 [INFO]: You're using dataset solar_alabama, please cite it properly in your work. You can find its reference information at the below link: 
https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/solar_alabama
2025-08-24 05:32:28 [INFO]: Dataset solar_alabama has already been downloaded. Processing directly...
2025-08-24 05:32:28 [INFO]: Dataset solar_alabama has already been cached. Loading from cache directly...
2025-08-24 05:32:28 [INFO]: Loaded successfully!
2025-08-24 05:32:28 [INFO]: months selected as train set are <PeriodArray>
['2006-01', '2006-02', '2006-03', '2006-04', '2006-05', '2006-06']
Length: 6, dtype: period[M]
2025-08-24 05:32:28 [INFO]: months selected as val set are <PeriodArray>
['2006-07', '2006-08', '2006-09']
Length: 3, dtype: period[M]
2025-08-24 05:32:28 [INFO]: months selected as test set are <PeriodArray>
['2006-10', '2006-11', '2006-12']
Length: 3, dtype: period[M]
2025-08-24 05:32:28 [INFO]: Total sample number: 547
âœ… Dataset 'solar_alabama' with missing rate 0.5 loaded at: /home/23r8105_messou/lab/iPyPOTS/datasets/rate_0.5
2025-08-24 05:32:28 [INFO]: Training set size: 271 (49.54%)
2025-08-24 05:32:28 [INFO]: Validation set size: 138 (25.23%)
2025-08-24 05:32:28 [INFO]: Test set size: 138 (25.23%)
2025-08-24 05:32:28 [INFO]: Number of steps: 96
2025-08-24 05:32:28 [INFO]: Number of features: 137
2025-08-24 05:32:28 [INFO]: Train set missing rate: 49.99%
2025-08-24 05:32:28 [INFO]: Validating set missing rate: 50.03%
2025-08-24 05:32:28 [INFO]: Test set missing rate: 50.02%
2025-08-24 05:32:28 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using device list: ['cuda:0']
2025-08-24 05:32:28 [INFO]: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Using the given device: cuda:0
2025-08-24 05:32:29 [INFO]: Model files will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse
2025-08-24 05:32:29 [INFO]: Tensorboard file will be saved to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/tensorboard
2025-08-24 05:32:29 [INFO]: Using customized MAE as the training loss function.
2025-08-24 05:32:29 [INFO]: Using customized MSE as the validation metric function.
2025-08-24 05:32:36 [INFO]: Model placed on cuda:0
2025-08-24 05:32:36 [INFO]: LLM4IMP initialized with the given hyperparameters.
 â”œâ”€ Total parameters: 128,520,192
 â”œâ”€ Trainable parameters: 11,168,256
 â””â”€ Trainable ratio: 8.69%
2025-08-24 05:32:37 [INFO]: Epoch 001 - training loss (MAE): 0.7373, validation MSE: 0.6889
2025-08-24 05:32:37 [INFO]: Epoch 002 - training loss (MAE): 0.6453, validation MSE: 0.5988
2025-08-24 05:32:37 [INFO]: Epoch 003 - training loss (MAE): 0.5742, validation MSE: 0.4667
2025-08-24 05:32:38 [INFO]: Epoch 004 - training loss (MAE): 0.5207, validation MSE: 0.3182
2025-08-24 05:32:38 [INFO]: Epoch 005 - training loss (MAE): 0.4720, validation MSE: 0.2230
2025-08-24 05:32:38 [INFO]: Epoch 006 - training loss (MAE): 0.4402, validation MSE: 0.1796
2025-08-24 05:32:39 [INFO]: Epoch 007 - training loss (MAE): 0.4204, validation MSE: 0.1576
2025-08-24 05:32:39 [INFO]: Epoch 008 - training loss (MAE): 0.4066, validation MSE: 0.1461
2025-08-24 05:32:39 [INFO]: Epoch 009 - training loss (MAE): 0.4019, validation MSE: 0.1389
2025-08-24 05:32:39 [INFO]: Epoch 010 - training loss (MAE): 0.3915, validation MSE: 0.1317
2025-08-24 05:32:40 [INFO]: Epoch 011 - training loss (MAE): 0.3877, validation MSE: 0.1269
2025-08-24 05:32:40 [INFO]: Epoch 012 - training loss (MAE): 0.3819, validation MSE: 0.1248
2025-08-24 05:32:40 [INFO]: Epoch 013 - training loss (MAE): 0.3746, validation MSE: 0.1211
2025-08-24 05:32:40 [INFO]: Epoch 014 - training loss (MAE): 0.3720, validation MSE: 0.1187
2025-08-24 05:32:41 [INFO]: Epoch 015 - training loss (MAE): 0.3692, validation MSE: 0.1166
2025-08-24 05:32:41 [INFO]: Finished training. The best model is from epoch#15.
2025-08-24 05:32:42 [INFO]: Saved the model to output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/LLM4IMP.pypots
[LLM4IMP] Testing â€”â€” MAE: 0.2803| MSE: 0.1230| RMSE: 0.3507| MRE: 0.3618| 
ğŸ“Š Metrics saved to: output/imputation/cuda/llm4imp/gpt2/solar_alabama/epoch15/mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue/promptfalse_reprogfalse/metrics/llm4imp_metrics.json
âœ… DONE: solar_alabama | mr0.5_bs32_dm64_ffn128/h6_ly1/hanntrue_promptfalse_reprogfalse
âœ… All llm4imp runs completed at 2025-08-24T05:32:44+09:00.
